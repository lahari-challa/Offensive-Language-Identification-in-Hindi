{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpB3IEfZbyVf"
      },
      "source": [
        "**Multilingual BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSNlgx4sXMV",
        "outputId": "2768f766-9102-447b-f6b6-718b4c3933ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGxJpxGKYXOb",
        "outputId": "eec52b29-afea-43f6-8fb0-1cfdfc19619f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD-1mnduYUFd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtZRXoOYkxf",
        "outputId": "eb7bb5b8-2763-47e9-9819-6de0339a3745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU:  0\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    # torch.cuda.set_device(0)\n",
        "    device = torch.device('cuda')\n",
        "    print('Using GPU: ', torch.cuda.current_device())\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8yfMc-6jjmB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dezNA2_8DbGj",
        "outputId": "72cdb1c6-d604-4353-990c-21759fb56154"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-82dffa7c-16d6-45bc-8022-04c35d845838\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>UNT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82dffa7c-16d6-45bc-8022-04c35d845838')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82dffa7c-16d6-45bc-8022-04c35d845838 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82dffa7c-16d6-45bc-8022-04c35d845838');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            text_id                                               text task_1  \\\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
              "...             ...                                                ...    ...   \n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT   \n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
              "\n",
              "     task_2 task_3  \n",
              "0      NONE   NONE  \n",
              "1      PRFN    UNT  \n",
              "2      PRFN    TIN  \n",
              "3      NONE   NONE  \n",
              "4      NONE   NONE  \n",
              "...     ...    ...  \n",
              "4660   NONE   NONE  \n",
              "4661   PRFN    TIN  \n",
              "4662   NONE   NONE  \n",
              "4663   HATE    TIN  \n",
              "4664   HATE    TIN  \n",
              "\n",
              "[4665 rows x 5 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('drive/MyDrive/hindi_dataset.tsv',sep='\\t')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QJLMOQ8--AmC",
        "outputId": "7a3c292d-485b-482a-d002-25b8b644697b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8d7e0165-403f-4f76-9787-4a3ce38eebe6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d7e0165-403f-4f76-9787-4a3ce38eebe6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d7e0165-403f-4f76-9787-4a3ce38eebe6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d7e0165-403f-4f76-9787-4a3ce38eebe6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT\n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF\n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF\n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT\n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT\n",
              "...                                                 ...    ...\n",
              "4660  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT\n",
              "4661  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF\n",
              "4662      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT\n",
              "4663  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF\n",
              "4664  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF\n",
              "\n",
              "[4665 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(['text_id','task_2','task_3'],inplace=True,axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Ayba2w8XPIvG",
        "outputId": "8f3ddc87-5aef-4a58-e1d3-c79968338e3b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-81cc7da2-f243-4fac-b6fa-d9f0e1586387\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81cc7da2-f243-4fac-b6fa-d9f0e1586387')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81cc7da2-f243-4fac-b6fa-d9f0e1586387 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81cc7da2-f243-4fac-b6fa-d9f0e1586387');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0\n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          0\n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          0\n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0\n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0\n",
              "...                                                 ...    ...        ...\n",
              "4660  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT          0\n",
              "4661  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF          0\n",
              "4662      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT          0\n",
              "4663  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF          0\n",
              "4664  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF          0\n",
              "\n",
              "[4665 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1 = df.assign(Offensive=0)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qvtGLMsz6vOu",
        "outputId": "8c50bd06-e5dc-44a1-858c-eeac899a1a30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b749f250-f2b8-40ad-8ce4-c7151cfa6009\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b749f250-f2b8-40ad-8ce4-c7151cfa6009')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b749f250-f2b8-40ad-8ce4-c7151cfa6009 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b749f250-f2b8-40ad-8ce4-c7151cfa6009');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          0   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          0   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "4660  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT          0   \n",
              "4661  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF          0   \n",
              "4662      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT          0   \n",
              "4663  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF          0   \n",
              "4664  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  \n",
              "...            ...  \n",
              "4660             0  \n",
              "4661             0  \n",
              "4662             0  \n",
              "4663             0  \n",
              "4664             0  \n",
              "\n",
              "[4665 rows x 4 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = df1.assign(NotOffensive=0)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "AJZwBZYM6xYT",
        "outputId": "40129e85-d766-4d82-82a5-ef159015e15f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-4b42d279e6cb>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 0\n",
            "<ipython-input-13-4b42d279e6cb>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 1\n",
            "<ipython-input-13-4b42d279e6cb>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 1\n",
            "<ipython-input-13-4b42d279e6cb>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-82ecc361-0821-40ef-9d82-d552b8a6f58a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82ecc361-0821-40ef-9d82-d552b8a6f58a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82ecc361-0821-40ef-9d82-d552b8a6f58a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82ecc361-0821-40ef-9d82-d552b8a6f58a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          1   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          1   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "4660  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT          0   \n",
              "4661  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF          1   \n",
              "4662      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT          0   \n",
              "4663  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF          1   \n",
              "4664  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF          1   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "4660             1  \n",
              "4661             0  \n",
              "4662             1  \n",
              "4663             0  \n",
              "4664             0  \n",
              "\n",
              "[4665 rows x 4 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index in train_df.index:\n",
        "    k = train_df['task_1'][index]\n",
        "    if k == 'HOF':\n",
        "        train_df['Offensive'][index] = 1\n",
        "        train_df['NotOffensive'][index] = 0\n",
        "    else:\n",
        "        train_df['Offensive'][index] = 0\n",
        "        train_df['NotOffensive'][index] = 1\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr30G69g61ip",
        "outputId": "27ef737c-efa3-4eb4-dbc7-38283a7e0cbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['text', 'task_1', 'Offensive', 'NotOffensive'], dtype='object')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "C_ODsSFK63ot",
        "outputId": "64f5582f-7ad5-4a28-a9a7-0816ad85b096"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a97dfdf5-c48d-49ec-8187-4ff0ffed4e36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>number of comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>2469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotOffensive</td>\n",
              "      <td>2196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a97dfdf5-c48d-49ec-8187-4ff0ffed4e36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a97dfdf5-c48d-49ec-8187-4ff0ffed4e36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a97dfdf5-c48d-49ec-8187-4ff0ffed4e36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       category  number of comments\n",
              "0     Offensive                2469\n",
              "1  NotOffensive                2196"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories = train_df.columns[2:]\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, train_df[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOzQ1t6k65kv"
      },
      "outputs": [],
      "source": [
        "target_list = categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "249d296b76c440869969482834331cca",
            "e68f2c1379324e359ebc0dee22c6ea11",
            "193924d3bd4c47fa91a55d56ff7fab90",
            "47f371020d6448c589afb4845df104c3",
            "191ce1c7be50470086a23f92f46b73ce",
            "d16a92eb09924bf0b56bb67b5796fc56",
            "6f0caa4d8ef74835bd47dd6068126262",
            "3315773a47bb41c891c8b180ed9b8544",
            "9889eae758e64923b42d902d27a5c17c",
            "c632a6bc98a947e083b46c8a645691d1",
            "8e5b28dae67846e48f0b6e2e54c1706f",
            "6e47ef5247ea4bf0b3b7e3979cfe07bd",
            "da0a9fca9caf441db1ddc1726d647bdb",
            "80e769fc7268492bacac65973b6da2a1",
            "b4d8f146a73c44ac939ffbb7e03f376b",
            "03d81f93558945d4abc0ca93fc5930fb",
            "7d2013eb954041ddb37b8ab45c94258e",
            "00e9b37e652e4d9aacf5fb4b1ecfd6d6",
            "eec95e027b79484394e599b1e7b1f616",
            "d84be24aaf8d46f18c79983e24555758",
            "68f889a593dd4da896032fb04cbf2bda",
            "a0851ddc59674b3ba89922e7878bda67",
            "8c9808be8eca4e039a27de4667f8ede0",
            "5d5e885f84cb4bdcaf946a78ff301e44",
            "845034af10004c35aac4a0d37060d795",
            "caeb2a53a3df489ea750e557e9eda00c",
            "bc92bff70db4469ab6981f312ca88e88",
            "919b5aaaec7540a2847e8901ed1c11ea",
            "99c30588e33f470191d313fec5df002c",
            "fa8d7626893e480a922ae490f5906e59",
            "8e5dfbb6a5d447128851d7f27f7316c6",
            "a8608d08f44c468198d78f4711060c0c",
            "19b57232018f42e090dfe19ebb192d7c"
          ]
        },
        "id": "V3tm6cXy6_7P",
        "outputId": "29fa79c3-b4b2-4abb-d2a8-c201d34d74e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "249d296b76c440869969482834331cca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e47ef5247ea4bf0b3b7e3979cfe07bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c9808be8eca4e039a27de4667f8ede0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\",do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUdjDIuH7Ako"
      },
      "outputs": [],
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IZ7fh0a7CVQ"
      },
      "outputs": [],
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['text'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kitz7mVQ7EEz",
        "outputId": "b98f2112-728a-40b7-ea08-d79843359346"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Input:  torch.Size([4665, 300])\n",
            "Text Input Attention:  torch.Size([4665, 300])\n",
            "Labels:  torch.Size([4665, 2])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcPYdaoe7Gg7",
        "outputId": "0113d244-4703-4de8-c6c3-9fc7a99d2562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaHcsXKCco52"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bertmodel = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "        self.ffn1 = nn.Linear(768, hidden_dim)\n",
        "        self.dp1 = nn.Dropout()\n",
        "        self.ffn2 = nn.Linear(hidden_dim, num_labels)\n",
        "        \n",
        "    def forward(self, in_T, in_T_attn_masks):\n",
        "        outputs = self.bertmodel(in_T, in_T_attn_masks)\n",
        "        x = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "        x = F.relu(self.ffn1(x))\n",
        "        x = self.dp1(x)\n",
        "        x = torch.sigmoid(self.ffn2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS_EdGdpcp5s"
      },
      "outputs": [],
      "source": [
        "model = MultiClassClassifier(100, 2).to(device) # 100 hidden dimension, 2 lables\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8) # Adam with weight decay\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WX6Dr1E7IeM",
        "outputId": "b31f3e3e-c925-4c61-ae94-99f0332eb3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    467.    Elapsed: 0:00:02. Loss: 1.21615\n",
            "  Batch    10  of    467.    Elapsed: 0:00:05. Loss: 0.95545\n",
            "  Batch    15  of    467.    Elapsed: 0:00:07. Loss: 0.86350\n",
            "  Batch    20  of    467.    Elapsed: 0:00:09. Loss: 0.80869\n",
            "  Batch    25  of    467.    Elapsed: 0:00:11. Loss: 0.77111\n",
            "  Batch    30  of    467.    Elapsed: 0:00:14. Loss: 0.72568\n",
            "  Batch    35  of    467.    Elapsed: 0:00:16. Loss: 0.69815\n",
            "  Batch    40  of    467.    Elapsed: 0:00:18. Loss: 0.67549\n",
            "  Batch    45  of    467.    Elapsed: 0:00:21. Loss: 0.64542\n",
            "  Batch    50  of    467.    Elapsed: 0:00:23. Loss: 0.63418\n",
            "  Batch    55  of    467.    Elapsed: 0:00:25. Loss: 0.62850\n",
            "  Batch    60  of    467.    Elapsed: 0:00:28. Loss: 0.61370\n",
            "  Batch    65  of    467.    Elapsed: 0:00:30. Loss: 0.61986\n",
            "  Batch    70  of    467.    Elapsed: 0:00:32. Loss: 0.61298\n",
            "  Batch    75  of    467.    Elapsed: 0:00:35. Loss: 0.61912\n",
            "  Batch    80  of    467.    Elapsed: 0:00:37. Loss: 0.60529\n",
            "  Batch    85  of    467.    Elapsed: 0:00:39. Loss: 0.59399\n",
            "  Batch    90  of    467.    Elapsed: 0:00:42. Loss: 0.58618\n",
            "  Batch    95  of    467.    Elapsed: 0:00:44. Loss: 0.59162\n",
            "  Batch   100  of    467.    Elapsed: 0:00:46. Loss: 0.58665\n",
            "  Batch   105  of    467.    Elapsed: 0:00:49. Loss: 0.57409\n",
            "  Batch   110  of    467.    Elapsed: 0:00:51. Loss: 0.56220\n",
            "  Batch   115  of    467.    Elapsed: 0:00:53. Loss: 0.55935\n",
            "  Batch   120  of    467.    Elapsed: 0:00:56. Loss: 0.55794\n",
            "  Batch   125  of    467.    Elapsed: 0:00:58. Loss: 0.55739\n",
            "  Batch   130  of    467.    Elapsed: 0:01:00. Loss: 0.54939\n",
            "  Batch   135  of    467.    Elapsed: 0:01:03. Loss: 0.54733\n",
            "  Batch   140  of    467.    Elapsed: 0:01:05. Loss: 0.54696\n",
            "  Batch   145  of    467.    Elapsed: 0:01:07. Loss: 0.54196\n",
            "  Batch   150  of    467.    Elapsed: 0:01:10. Loss: 0.54276\n",
            "  Batch   155  of    467.    Elapsed: 0:01:12. Loss: 0.53842\n",
            "  Batch   160  of    467.    Elapsed: 0:01:14. Loss: 0.53722\n",
            "  Batch   165  of    467.    Elapsed: 0:01:17. Loss: 0.53079\n",
            "  Batch   170  of    467.    Elapsed: 0:01:19. Loss: 0.52354\n",
            "  Batch   175  of    467.    Elapsed: 0:01:21. Loss: 0.52753\n",
            "  Batch   180  of    467.    Elapsed: 0:01:24. Loss: 0.52824\n",
            "  Batch   185  of    467.    Elapsed: 0:01:26. Loss: 0.52910\n",
            "  Batch   190  of    467.    Elapsed: 0:01:29. Loss: 0.52342\n",
            "  Batch   195  of    467.    Elapsed: 0:01:31. Loss: 0.51736\n",
            "  Batch   200  of    467.    Elapsed: 0:01:33. Loss: 0.51970\n",
            "  Batch   205  of    467.    Elapsed: 0:01:36. Loss: 0.52463\n",
            "  Batch   210  of    467.    Elapsed: 0:01:38. Loss: 0.51912\n",
            "  Batch   215  of    467.    Elapsed: 0:01:41. Loss: 0.51869\n",
            "  Batch   220  of    467.    Elapsed: 0:01:43. Loss: 0.51681\n",
            "  Batch   225  of    467.    Elapsed: 0:01:45. Loss: 0.51383\n",
            "  Batch   230  of    467.    Elapsed: 0:01:48. Loss: 0.50972\n",
            "  Batch   235  of    467.    Elapsed: 0:01:50. Loss: 0.50856\n",
            "  Batch   240  of    467.    Elapsed: 0:01:53. Loss: 0.50713\n",
            "  Batch   245  of    467.    Elapsed: 0:01:55. Loss: 0.50712\n",
            "  Batch   250  of    467.    Elapsed: 0:01:58. Loss: 0.50586\n",
            "  Batch   255  of    467.    Elapsed: 0:02:00. Loss: 0.50736\n",
            "  Batch   260  of    467.    Elapsed: 0:02:02. Loss: 0.50638\n",
            "  Batch   265  of    467.    Elapsed: 0:02:05. Loss: 0.50605\n",
            "  Batch   270  of    467.    Elapsed: 0:02:07. Loss: 0.50429\n",
            "  Batch   275  of    467.    Elapsed: 0:02:10. Loss: 0.50482\n",
            "  Batch   280  of    467.    Elapsed: 0:02:12. Loss: 0.50140\n",
            "  Batch   285  of    467.    Elapsed: 0:02:15. Loss: 0.49731\n",
            "  Batch   290  of    467.    Elapsed: 0:02:17. Loss: 0.49533\n",
            "  Batch   295  of    467.    Elapsed: 0:02:19. Loss: 0.49453\n",
            "  Batch   300  of    467.    Elapsed: 0:02:22. Loss: 0.49433\n",
            "  Batch   305  of    467.    Elapsed: 0:02:24. Loss: 0.49113\n",
            "  Batch   310  of    467.    Elapsed: 0:02:27. Loss: 0.49223\n",
            "  Batch   315  of    467.    Elapsed: 0:02:29. Loss: 0.48911\n",
            "  Batch   320  of    467.    Elapsed: 0:02:32. Loss: 0.48595\n",
            "  Batch   325  of    467.    Elapsed: 0:02:34. Loss: 0.48625\n",
            "  Batch   330  of    467.    Elapsed: 0:02:37. Loss: 0.48518\n",
            "  Batch   335  of    467.    Elapsed: 0:02:39. Loss: 0.48396\n",
            "  Batch   340  of    467.    Elapsed: 0:02:41. Loss: 0.48373\n",
            "  Batch   345  of    467.    Elapsed: 0:02:44. Loss: 0.48395\n",
            "  Batch   350  of    467.    Elapsed: 0:02:46. Loss: 0.48275\n",
            "  Batch   355  of    467.    Elapsed: 0:02:49. Loss: 0.48096\n",
            "  Batch   360  of    467.    Elapsed: 0:02:51. Loss: 0.48075\n",
            "  Batch   365  of    467.    Elapsed: 0:02:54. Loss: 0.47885\n",
            "  Batch   370  of    467.    Elapsed: 0:02:56. Loss: 0.48037\n",
            "  Batch   375  of    467.    Elapsed: 0:02:59. Loss: 0.48276\n",
            "  Batch   380  of    467.    Elapsed: 0:03:01. Loss: 0.48321\n",
            "  Batch   385  of    467.    Elapsed: 0:03:04. Loss: 0.48377\n",
            "  Batch   390  of    467.    Elapsed: 0:03:06. Loss: 0.48328\n",
            "  Batch   395  of    467.    Elapsed: 0:03:08. Loss: 0.48130\n",
            "  Batch   400  of    467.    Elapsed: 0:03:11. Loss: 0.48314\n",
            "  Batch   405  of    467.    Elapsed: 0:03:13. Loss: 0.48168\n",
            "  Batch   410  of    467.    Elapsed: 0:03:16. Loss: 0.48313\n",
            "  Batch   415  of    467.    Elapsed: 0:03:18. Loss: 0.48163\n",
            "  Batch   420  of    467.    Elapsed: 0:03:21. Loss: 0.48107\n",
            "  Batch   425  of    467.    Elapsed: 0:03:23. Loss: 0.48200\n",
            "  Batch   430  of    467.    Elapsed: 0:03:26. Loss: 0.48205\n",
            "  Batch   435  of    467.    Elapsed: 0:03:28. Loss: 0.48145\n",
            "  Batch   440  of    467.    Elapsed: 0:03:31. Loss: 0.48111\n",
            "  Batch   445  of    467.    Elapsed: 0:03:33. Loss: 0.48162\n",
            "  Batch   450  of    467.    Elapsed: 0:03:36. Loss: 0.48102\n",
            "  Batch   455  of    467.    Elapsed: 0:03:38. Loss: 0.48009\n",
            "  Batch   460  of    467.    Elapsed: 0:03:41. Loss: 0.47978\n",
            "  Batch   465  of    467.    Elapsed: 0:03:43. Loss: 0.47793\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:03:44\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:18\n",
            "  Accuracy: 0.78\n",
            "  Accuracy: 0.79\n",
            "  Macro F1-score: 0.78\n",
            "  Macro F1-score: 0.79\n",
            "  Weighted F1-score: 0.78\n",
            "  Weighted F1-score: 0.79\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.69      0.75       444\n",
            "         1.0       0.76      0.86      0.81       489\n",
            "\n",
            "    accuracy                           0.78       933\n",
            "   macro avg       0.79      0.78      0.78       933\n",
            "weighted avg       0.79      0.78      0.78       933\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.67      0.77       489\n",
            "         1.0       0.72      0.93      0.81       444\n",
            "\n",
            "    accuracy                           0.79       933\n",
            "   macro avg       0.81      0.80      0.79       933\n",
            "weighted avg       0.82      0.79      0.79       933\n",
            "\n",
            "Confusion Matrix:\n",
            "[[308 136]\n",
            " [ 67 422]]\n",
            "[[327 162]\n",
            " [ 33 411]]\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    467.    Elapsed: 0:00:03. Loss: 0.30469\n",
            "  Batch    10  of    467.    Elapsed: 0:00:05. Loss: 0.39682\n",
            "  Batch    15  of    467.    Elapsed: 0:00:07. Loss: 0.36156\n",
            "  Batch    20  of    467.    Elapsed: 0:00:10. Loss: 0.31968\n",
            "  Batch    25  of    467.    Elapsed: 0:00:12. Loss: 0.34575\n",
            "  Batch    30  of    467.    Elapsed: 0:00:15. Loss: 0.34966\n",
            "  Batch    35  of    467.    Elapsed: 0:00:17. Loss: 0.34898\n",
            "  Batch    40  of    467.    Elapsed: 0:00:20. Loss: 0.34110\n",
            "  Batch    45  of    467.    Elapsed: 0:00:22. Loss: 0.35809\n",
            "  Batch    50  of    467.    Elapsed: 0:00:25. Loss: 0.34943\n",
            "  Batch    55  of    467.    Elapsed: 0:00:27. Loss: 0.33392\n",
            "  Batch    60  of    467.    Elapsed: 0:00:30. Loss: 0.34122\n",
            "  Batch    65  of    467.    Elapsed: 0:00:32. Loss: 0.32897\n",
            "  Batch    70  of    467.    Elapsed: 0:00:35. Loss: 0.32909\n",
            "  Batch    75  of    467.    Elapsed: 0:00:37. Loss: 0.33185\n",
            "  Batch    80  of    467.    Elapsed: 0:00:40. Loss: 0.33007\n",
            "  Batch    85  of    467.    Elapsed: 0:00:42. Loss: 0.33785\n",
            "  Batch    90  of    467.    Elapsed: 0:00:45. Loss: 0.34565\n",
            "  Batch    95  of    467.    Elapsed: 0:00:47. Loss: 0.34982\n",
            "  Batch   100  of    467.    Elapsed: 0:00:50. Loss: 0.35819\n",
            "  Batch   105  of    467.    Elapsed: 0:00:52. Loss: 0.38026\n",
            "  Batch   110  of    467.    Elapsed: 0:00:55. Loss: 0.37603\n",
            "  Batch   115  of    467.    Elapsed: 0:00:57. Loss: 0.37790\n",
            "  Batch   120  of    467.    Elapsed: 0:01:00. Loss: 0.37597\n",
            "  Batch   125  of    467.    Elapsed: 0:01:02. Loss: 0.37451\n",
            "  Batch   130  of    467.    Elapsed: 0:01:05. Loss: 0.37592\n",
            "  Batch   135  of    467.    Elapsed: 0:01:07. Loss: 0.37216\n",
            "  Batch   140  of    467.    Elapsed: 0:01:10. Loss: 0.36950\n",
            "  Batch   145  of    467.    Elapsed: 0:01:12. Loss: 0.36668\n",
            "  Batch   150  of    467.    Elapsed: 0:01:15. Loss: 0.36317\n",
            "  Batch   155  of    467.    Elapsed: 0:01:17. Loss: 0.35958\n",
            "  Batch   160  of    467.    Elapsed: 0:01:20. Loss: 0.35813\n",
            "  Batch   165  of    467.    Elapsed: 0:01:22. Loss: 0.35609\n",
            "  Batch   170  of    467.    Elapsed: 0:01:25. Loss: 0.35581\n",
            "  Batch   175  of    467.    Elapsed: 0:01:27. Loss: 0.35882\n",
            "  Batch   180  of    467.    Elapsed: 0:01:30. Loss: 0.35446\n",
            "  Batch   185  of    467.    Elapsed: 0:01:32. Loss: 0.35725\n",
            "  Batch   190  of    467.    Elapsed: 0:01:35. Loss: 0.36118\n",
            "  Batch   195  of    467.    Elapsed: 0:01:37. Loss: 0.36053\n",
            "  Batch   200  of    467.    Elapsed: 0:01:40. Loss: 0.36268\n",
            "  Batch   205  of    467.    Elapsed: 0:01:43. Loss: 0.36047\n",
            "  Batch   210  of    467.    Elapsed: 0:01:45. Loss: 0.36045\n",
            "  Batch   215  of    467.    Elapsed: 0:01:48. Loss: 0.36123\n",
            "  Batch   220  of    467.    Elapsed: 0:01:50. Loss: 0.35797\n",
            "  Batch   225  of    467.    Elapsed: 0:01:53. Loss: 0.35802\n",
            "  Batch   230  of    467.    Elapsed: 0:01:55. Loss: 0.35841\n",
            "  Batch   235  of    467.    Elapsed: 0:01:58. Loss: 0.35636\n",
            "  Batch   240  of    467.    Elapsed: 0:02:00. Loss: 0.35800\n",
            "  Batch   245  of    467.    Elapsed: 0:02:03. Loss: 0.35953\n",
            "  Batch   250  of    467.    Elapsed: 0:02:05. Loss: 0.35869\n",
            "  Batch   255  of    467.    Elapsed: 0:02:08. Loss: 0.36126\n",
            "  Batch   260  of    467.    Elapsed: 0:02:10. Loss: 0.35838\n",
            "  Batch   265  of    467.    Elapsed: 0:02:13. Loss: 0.36146\n",
            "  Batch   270  of    467.    Elapsed: 0:02:15. Loss: 0.35908\n",
            "  Batch   275  of    467.    Elapsed: 0:02:18. Loss: 0.36254\n",
            "  Batch   280  of    467.    Elapsed: 0:02:20. Loss: 0.36178\n",
            "  Batch   285  of    467.    Elapsed: 0:02:23. Loss: 0.36421\n",
            "  Batch   290  of    467.    Elapsed: 0:02:25. Loss: 0.36430\n",
            "  Batch   295  of    467.    Elapsed: 0:02:28. Loss: 0.36245\n",
            "  Batch   300  of    467.    Elapsed: 0:02:30. Loss: 0.36113\n",
            "  Batch   305  of    467.    Elapsed: 0:02:33. Loss: 0.36084\n",
            "  Batch   310  of    467.    Elapsed: 0:02:36. Loss: 0.35790\n",
            "  Batch   315  of    467.    Elapsed: 0:02:38. Loss: 0.35894\n",
            "  Batch   320  of    467.    Elapsed: 0:02:41. Loss: 0.35916\n",
            "  Batch   325  of    467.    Elapsed: 0:02:43. Loss: 0.36434\n",
            "  Batch   330  of    467.    Elapsed: 0:02:46. Loss: 0.36318\n",
            "  Batch   335  of    467.    Elapsed: 0:02:48. Loss: 0.36514\n",
            "  Batch   340  of    467.    Elapsed: 0:02:51. Loss: 0.36458\n",
            "  Batch   345  of    467.    Elapsed: 0:02:53. Loss: 0.36724\n",
            "  Batch   350  of    467.    Elapsed: 0:02:56. Loss: 0.36658\n",
            "  Batch   355  of    467.    Elapsed: 0:02:58. Loss: 0.36856\n",
            "  Batch   360  of    467.    Elapsed: 0:03:01. Loss: 0.36813\n",
            "  Batch   365  of    467.    Elapsed: 0:03:03. Loss: 0.36805\n",
            "  Batch   370  of    467.    Elapsed: 0:03:06. Loss: 0.36621\n",
            "  Batch   375  of    467.    Elapsed: 0:03:08. Loss: 0.36824\n",
            "  Batch   380  of    467.    Elapsed: 0:03:11. Loss: 0.36813\n",
            "  Batch   385  of    467.    Elapsed: 0:03:13. Loss: 0.36808\n",
            "  Batch   390  of    467.    Elapsed: 0:03:16. Loss: 0.36583\n",
            "  Batch   395  of    467.    Elapsed: 0:03:19. Loss: 0.36815\n",
            "  Batch   400  of    467.    Elapsed: 0:03:21. Loss: 0.36873\n",
            "  Batch   405  of    467.    Elapsed: 0:03:24. Loss: 0.36678\n",
            "  Batch   410  of    467.    Elapsed: 0:03:26. Loss: 0.36682\n",
            "  Batch   415  of    467.    Elapsed: 0:03:29. Loss: 0.36578\n",
            "  Batch   420  of    467.    Elapsed: 0:03:31. Loss: 0.36701\n",
            "  Batch   425  of    467.    Elapsed: 0:03:34. Loss: 0.36735\n",
            "  Batch   430  of    467.    Elapsed: 0:03:36. Loss: 0.36576\n",
            "  Batch   435  of    467.    Elapsed: 0:03:39. Loss: 0.36642\n",
            "  Batch   440  of    467.    Elapsed: 0:03:41. Loss: 0.36633\n",
            "  Batch   445  of    467.    Elapsed: 0:03:44. Loss: 0.36535\n",
            "  Batch   450  of    467.    Elapsed: 0:03:46. Loss: 0.36977\n",
            "  Batch   455  of    467.    Elapsed: 0:03:49. Loss: 0.36973\n",
            "  Batch   460  of    467.    Elapsed: 0:03:52. Loss: 0.36996\n",
            "  Batch   465  of    467.    Elapsed: 0:03:54. Loss: 0.36836\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:03:55\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:19\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.81\n",
            "  Macro F1-score: 0.80\n",
            "  Macro F1-score: 0.81\n",
            "  Weighted F1-score: 0.80\n",
            "  Weighted F1-score: 0.81\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.73      0.78       444\n",
            "         1.0       0.78      0.87      0.83       489\n",
            "\n",
            "    accuracy                           0.81       933\n",
            "   macro avg       0.81      0.80      0.80       933\n",
            "weighted avg       0.81      0.81      0.80       933\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.79      0.81       489\n",
            "         1.0       0.78      0.84      0.81       444\n",
            "\n",
            "    accuracy                           0.81       933\n",
            "   macro avg       0.81      0.81      0.81       933\n",
            "weighted avg       0.81      0.81      0.81       933\n",
            "\n",
            "Confusion Matrix:\n",
            "[[325 119]\n",
            " [ 62 427]]\n",
            "[[384 105]\n",
            " [ 72 372]]\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    467.    Elapsed: 0:00:03. Loss: 0.26469\n",
            "  Batch    10  of    467.    Elapsed: 0:00:05. Loss: 0.21313\n",
            "  Batch    15  of    467.    Elapsed: 0:00:08. Loss: 0.18418\n",
            "  Batch    20  of    467.    Elapsed: 0:00:10. Loss: 0.16453\n",
            "  Batch    25  of    467.    Elapsed: 0:00:13. Loss: 0.17913\n",
            "  Batch    30  of    467.    Elapsed: 0:00:15. Loss: 0.17664\n",
            "  Batch    35  of    467.    Elapsed: 0:00:18. Loss: 0.19947\n",
            "  Batch    40  of    467.    Elapsed: 0:00:20. Loss: 0.19214\n",
            "  Batch    45  of    467.    Elapsed: 0:00:23. Loss: 0.21622\n",
            "  Batch    50  of    467.    Elapsed: 0:00:25. Loss: 0.21927\n",
            "  Batch    55  of    467.    Elapsed: 0:00:28. Loss: 0.24569\n",
            "  Batch    60  of    467.    Elapsed: 0:00:30. Loss: 0.25309\n",
            "  Batch    65  of    467.    Elapsed: 0:00:33. Loss: 0.25482\n",
            "  Batch    70  of    467.    Elapsed: 0:00:35. Loss: 0.25880\n",
            "  Batch    75  of    467.    Elapsed: 0:00:38. Loss: 0.25951\n",
            "  Batch    80  of    467.    Elapsed: 0:00:41. Loss: 0.28061\n",
            "  Batch    85  of    467.    Elapsed: 0:00:43. Loss: 0.28076\n",
            "  Batch    90  of    467.    Elapsed: 0:00:46. Loss: 0.27426\n",
            "  Batch    95  of    467.    Elapsed: 0:00:48. Loss: 0.26890\n",
            "  Batch   100  of    467.    Elapsed: 0:00:51. Loss: 0.27619\n",
            "  Batch   105  of    467.    Elapsed: 0:00:53. Loss: 0.27544\n",
            "  Batch   110  of    467.    Elapsed: 0:00:56. Loss: 0.27670\n",
            "  Batch   115  of    467.    Elapsed: 0:00:58. Loss: 0.27523\n",
            "  Batch   120  of    467.    Elapsed: 0:01:01. Loss: 0.27408\n",
            "  Batch   125  of    467.    Elapsed: 0:01:03. Loss: 0.27380\n",
            "  Batch   130  of    467.    Elapsed: 0:01:06. Loss: 0.27138\n",
            "  Batch   135  of    467.    Elapsed: 0:01:08. Loss: 0.27364\n",
            "  Batch   140  of    467.    Elapsed: 0:01:11. Loss: 0.28498\n",
            "  Batch   145  of    467.    Elapsed: 0:01:14. Loss: 0.28493\n",
            "  Batch   150  of    467.    Elapsed: 0:01:16. Loss: 0.28277\n",
            "  Batch   155  of    467.    Elapsed: 0:01:19. Loss: 0.29310\n",
            "  Batch   160  of    467.    Elapsed: 0:01:21. Loss: 0.29129\n",
            "  Batch   165  of    467.    Elapsed: 0:01:24. Loss: 0.29373\n",
            "  Batch   170  of    467.    Elapsed: 0:01:26. Loss: 0.29180\n",
            "  Batch   175  of    467.    Elapsed: 0:01:29. Loss: 0.29356\n",
            "  Batch   180  of    467.    Elapsed: 0:01:31. Loss: 0.29167\n",
            "  Batch   185  of    467.    Elapsed: 0:01:34. Loss: 0.29358\n",
            "  Batch   190  of    467.    Elapsed: 0:01:36. Loss: 0.30059\n",
            "  Batch   195  of    467.    Elapsed: 0:01:39. Loss: 0.30065\n",
            "  Batch   200  of    467.    Elapsed: 0:01:41. Loss: 0.29594\n",
            "  Batch   205  of    467.    Elapsed: 0:01:44. Loss: 0.29382\n",
            "  Batch   210  of    467.    Elapsed: 0:01:47. Loss: 0.29641\n",
            "  Batch   215  of    467.    Elapsed: 0:01:49. Loss: 0.29955\n",
            "  Batch   220  of    467.    Elapsed: 0:01:52. Loss: 0.30310\n",
            "  Batch   225  of    467.    Elapsed: 0:01:54. Loss: 0.30082\n",
            "  Batch   230  of    467.    Elapsed: 0:01:57. Loss: 0.30211\n",
            "  Batch   235  of    467.    Elapsed: 0:01:59. Loss: 0.30610\n",
            "  Batch   240  of    467.    Elapsed: 0:02:02. Loss: 0.30201\n",
            "  Batch   245  of    467.    Elapsed: 0:02:04. Loss: 0.30088\n",
            "  Batch   250  of    467.    Elapsed: 0:02:07. Loss: 0.29921\n",
            "  Batch   255  of    467.    Elapsed: 0:02:09. Loss: 0.29813\n",
            "  Batch   260  of    467.    Elapsed: 0:02:12. Loss: 0.29964\n",
            "  Batch   265  of    467.    Elapsed: 0:02:14. Loss: 0.29857\n",
            "  Batch   270  of    467.    Elapsed: 0:02:17. Loss: 0.29933\n",
            "  Batch   275  of    467.    Elapsed: 0:02:20. Loss: 0.30061\n",
            "  Batch   280  of    467.    Elapsed: 0:02:22. Loss: 0.30351\n",
            "  Batch   285  of    467.    Elapsed: 0:02:25. Loss: 0.29961\n",
            "  Batch   290  of    467.    Elapsed: 0:02:27. Loss: 0.29987\n",
            "  Batch   295  of    467.    Elapsed: 0:02:30. Loss: 0.30007\n",
            "  Batch   300  of    467.    Elapsed: 0:02:32. Loss: 0.29728\n",
            "  Batch   305  of    467.    Elapsed: 0:02:35. Loss: 0.29818\n",
            "  Batch   310  of    467.    Elapsed: 0:02:37. Loss: 0.29684\n",
            "  Batch   315  of    467.    Elapsed: 0:02:40. Loss: 0.29614\n",
            "  Batch   320  of    467.    Elapsed: 0:02:42. Loss: 0.29761\n",
            "  Batch   325  of    467.    Elapsed: 0:02:45. Loss: 0.29590\n",
            "  Batch   330  of    467.    Elapsed: 0:02:48. Loss: 0.29643\n",
            "  Batch   335  of    467.    Elapsed: 0:02:50. Loss: 0.29649\n",
            "  Batch   340  of    467.    Elapsed: 0:02:53. Loss: 0.29454\n",
            "  Batch   345  of    467.    Elapsed: 0:02:55. Loss: 0.29687\n",
            "  Batch   350  of    467.    Elapsed: 0:02:58. Loss: 0.29621\n",
            "  Batch   355  of    467.    Elapsed: 0:03:00. Loss: 0.29661\n",
            "  Batch   360  of    467.    Elapsed: 0:03:03. Loss: 0.29616\n",
            "  Batch   365  of    467.    Elapsed: 0:03:05. Loss: 0.29311\n",
            "  Batch   370  of    467.    Elapsed: 0:03:08. Loss: 0.29503\n",
            "  Batch   375  of    467.    Elapsed: 0:03:10. Loss: 0.29357\n",
            "  Batch   380  of    467.    Elapsed: 0:03:13. Loss: 0.29450\n",
            "  Batch   385  of    467.    Elapsed: 0:03:15. Loss: 0.29563\n",
            "  Batch   390  of    467.    Elapsed: 0:03:18. Loss: 0.29274\n",
            "  Batch   395  of    467.    Elapsed: 0:03:20. Loss: 0.29277\n",
            "  Batch   400  of    467.    Elapsed: 0:03:23. Loss: 0.29138\n",
            "  Batch   405  of    467.    Elapsed: 0:03:26. Loss: 0.29344\n",
            "  Batch   410  of    467.    Elapsed: 0:03:28. Loss: 0.29187\n",
            "  Batch   415  of    467.    Elapsed: 0:03:31. Loss: 0.29075\n",
            "  Batch   420  of    467.    Elapsed: 0:03:33. Loss: 0.28911\n",
            "  Batch   425  of    467.    Elapsed: 0:03:36. Loss: 0.28735\n",
            "  Batch   430  of    467.    Elapsed: 0:03:38. Loss: 0.28729\n",
            "  Batch   435  of    467.    Elapsed: 0:03:41. Loss: 0.28692\n",
            "  Batch   440  of    467.    Elapsed: 0:03:43. Loss: 0.28888\n",
            "  Batch   445  of    467.    Elapsed: 0:03:46. Loss: 0.28856\n",
            "  Batch   450  of    467.    Elapsed: 0:03:48. Loss: 0.28789\n",
            "  Batch   455  of    467.    Elapsed: 0:03:51. Loss: 0.28868\n",
            "  Batch   460  of    467.    Elapsed: 0:03:53. Loss: 0.28806\n",
            "  Batch   465  of    467.    Elapsed: 0:03:56. Loss: 0.28760\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:03:57\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:19\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.80\n",
            "  Macro F1-score: 0.81\n",
            "  Macro F1-score: 0.80\n",
            "  Weighted F1-score: 0.81\n",
            "  Weighted F1-score: 0.81\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.80      0.80       444\n",
            "         1.0       0.82      0.82      0.82       489\n",
            "\n",
            "    accuracy                           0.81       933\n",
            "   macro avg       0.81      0.81      0.81       933\n",
            "weighted avg       0.81      0.81      0.81       933\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.78      0.81       489\n",
            "         1.0       0.77      0.83      0.80       444\n",
            "\n",
            "    accuracy                           0.80       933\n",
            "   macro avg       0.81      0.81      0.80       933\n",
            "weighted avg       0.81      0.80      0.81       933\n",
            "\n",
            "Confusion Matrix:\n",
            "[[354  90]\n",
            " [ 86 403]]\n",
            "[[381 108]\n",
            " [ 74 370]]\n"
          ]
        }
      ],
      "source": [
        "#TRAINING and VALIDATION\n",
        "epochs = 3  \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,2))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(2)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(2):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(2):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(2)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(2)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(2)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-9qbgDHEMW4"
      },
      "outputs": [],
      "source": [
        "model_path = 'model_state_dict.pt'\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLpfcvlRBQWv"
      },
      "outputs": [],
      "source": [
        "modelPathDrive = '/content/drive/MyDrive/mBert.pt'\n",
        "torch.save(model.state_dict(), modelPathDrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXMtsHdlBTRI",
        "outputId": "9a754677-9da3-4dc5-8b67-dfb1bf7624b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['bertmodel.embeddings.position_ids', 'bertmodel.embeddings.word_embeddings.weight', 'bertmodel.embeddings.position_embeddings.weight', 'bertmodel.embeddings.token_type_embeddings.weight', 'bertmodel.embeddings.LayerNorm.weight', 'bertmodel.embeddings.LayerNorm.bias', 'bertmodel.encoder.layer.0.attention.self.query.weight', 'bertmodel.encoder.layer.0.attention.self.query.bias', 'bertmodel.encoder.layer.0.attention.self.key.weight', 'bertmodel.encoder.layer.0.attention.self.key.bias', 'bertmodel.encoder.layer.0.attention.self.value.weight', 'bertmodel.encoder.layer.0.attention.self.value.bias', 'bertmodel.encoder.layer.0.attention.output.dense.weight', 'bertmodel.encoder.layer.0.attention.output.dense.bias', 'bertmodel.encoder.layer.0.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.0.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.0.intermediate.dense.weight', 'bertmodel.encoder.layer.0.intermediate.dense.bias', 'bertmodel.encoder.layer.0.output.dense.weight', 'bertmodel.encoder.layer.0.output.dense.bias', 'bertmodel.encoder.layer.0.output.LayerNorm.weight', 'bertmodel.encoder.layer.0.output.LayerNorm.bias', 'bertmodel.encoder.layer.1.attention.self.query.weight', 'bertmodel.encoder.layer.1.attention.self.query.bias', 'bertmodel.encoder.layer.1.attention.self.key.weight', 'bertmodel.encoder.layer.1.attention.self.key.bias', 'bertmodel.encoder.layer.1.attention.self.value.weight', 'bertmodel.encoder.layer.1.attention.self.value.bias', 'bertmodel.encoder.layer.1.attention.output.dense.weight', 'bertmodel.encoder.layer.1.attention.output.dense.bias', 'bertmodel.encoder.layer.1.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.1.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.1.intermediate.dense.weight', 'bertmodel.encoder.layer.1.intermediate.dense.bias', 'bertmodel.encoder.layer.1.output.dense.weight', 'bertmodel.encoder.layer.1.output.dense.bias', 'bertmodel.encoder.layer.1.output.LayerNorm.weight', 'bertmodel.encoder.layer.1.output.LayerNorm.bias', 'bertmodel.encoder.layer.2.attention.self.query.weight', 'bertmodel.encoder.layer.2.attention.self.query.bias', 'bertmodel.encoder.layer.2.attention.self.key.weight', 'bertmodel.encoder.layer.2.attention.self.key.bias', 'bertmodel.encoder.layer.2.attention.self.value.weight', 'bertmodel.encoder.layer.2.attention.self.value.bias', 'bertmodel.encoder.layer.2.attention.output.dense.weight', 'bertmodel.encoder.layer.2.attention.output.dense.bias', 'bertmodel.encoder.layer.2.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.2.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.2.intermediate.dense.weight', 'bertmodel.encoder.layer.2.intermediate.dense.bias', 'bertmodel.encoder.layer.2.output.dense.weight', 'bertmodel.encoder.layer.2.output.dense.bias', 'bertmodel.encoder.layer.2.output.LayerNorm.weight', 'bertmodel.encoder.layer.2.output.LayerNorm.bias', 'bertmodel.encoder.layer.3.attention.self.query.weight', 'bertmodel.encoder.layer.3.attention.self.query.bias', 'bertmodel.encoder.layer.3.attention.self.key.weight', 'bertmodel.encoder.layer.3.attention.self.key.bias', 'bertmodel.encoder.layer.3.attention.self.value.weight', 'bertmodel.encoder.layer.3.attention.self.value.bias', 'bertmodel.encoder.layer.3.attention.output.dense.weight', 'bertmodel.encoder.layer.3.attention.output.dense.bias', 'bertmodel.encoder.layer.3.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.3.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.3.intermediate.dense.weight', 'bertmodel.encoder.layer.3.intermediate.dense.bias', 'bertmodel.encoder.layer.3.output.dense.weight', 'bertmodel.encoder.layer.3.output.dense.bias', 'bertmodel.encoder.layer.3.output.LayerNorm.weight', 'bertmodel.encoder.layer.3.output.LayerNorm.bias', 'bertmodel.encoder.layer.4.attention.self.query.weight', 'bertmodel.encoder.layer.4.attention.self.query.bias', 'bertmodel.encoder.layer.4.attention.self.key.weight', 'bertmodel.encoder.layer.4.attention.self.key.bias', 'bertmodel.encoder.layer.4.attention.self.value.weight', 'bertmodel.encoder.layer.4.attention.self.value.bias', 'bertmodel.encoder.layer.4.attention.output.dense.weight', 'bertmodel.encoder.layer.4.attention.output.dense.bias', 'bertmodel.encoder.layer.4.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.4.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.4.intermediate.dense.weight', 'bertmodel.encoder.layer.4.intermediate.dense.bias', 'bertmodel.encoder.layer.4.output.dense.weight', 'bertmodel.encoder.layer.4.output.dense.bias', 'bertmodel.encoder.layer.4.output.LayerNorm.weight', 'bertmodel.encoder.layer.4.output.LayerNorm.bias', 'bertmodel.encoder.layer.5.attention.self.query.weight', 'bertmodel.encoder.layer.5.attention.self.query.bias', 'bertmodel.encoder.layer.5.attention.self.key.weight', 'bertmodel.encoder.layer.5.attention.self.key.bias', 'bertmodel.encoder.layer.5.attention.self.value.weight', 'bertmodel.encoder.layer.5.attention.self.value.bias', 'bertmodel.encoder.layer.5.attention.output.dense.weight', 'bertmodel.encoder.layer.5.attention.output.dense.bias', 'bertmodel.encoder.layer.5.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.5.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.5.intermediate.dense.weight', 'bertmodel.encoder.layer.5.intermediate.dense.bias', 'bertmodel.encoder.layer.5.output.dense.weight', 'bertmodel.encoder.layer.5.output.dense.bias', 'bertmodel.encoder.layer.5.output.LayerNorm.weight', 'bertmodel.encoder.layer.5.output.LayerNorm.bias', 'bertmodel.encoder.layer.6.attention.self.query.weight', 'bertmodel.encoder.layer.6.attention.self.query.bias', 'bertmodel.encoder.layer.6.attention.self.key.weight', 'bertmodel.encoder.layer.6.attention.self.key.bias', 'bertmodel.encoder.layer.6.attention.self.value.weight', 'bertmodel.encoder.layer.6.attention.self.value.bias', 'bertmodel.encoder.layer.6.attention.output.dense.weight', 'bertmodel.encoder.layer.6.attention.output.dense.bias', 'bertmodel.encoder.layer.6.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.6.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.6.intermediate.dense.weight', 'bertmodel.encoder.layer.6.intermediate.dense.bias', 'bertmodel.encoder.layer.6.output.dense.weight', 'bertmodel.encoder.layer.6.output.dense.bias', 'bertmodel.encoder.layer.6.output.LayerNorm.weight', 'bertmodel.encoder.layer.6.output.LayerNorm.bias', 'bertmodel.encoder.layer.7.attention.self.query.weight', 'bertmodel.encoder.layer.7.attention.self.query.bias', 'bertmodel.encoder.layer.7.attention.self.key.weight', 'bertmodel.encoder.layer.7.attention.self.key.bias', 'bertmodel.encoder.layer.7.attention.self.value.weight', 'bertmodel.encoder.layer.7.attention.self.value.bias', 'bertmodel.encoder.layer.7.attention.output.dense.weight', 'bertmodel.encoder.layer.7.attention.output.dense.bias', 'bertmodel.encoder.layer.7.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.7.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.7.intermediate.dense.weight', 'bertmodel.encoder.layer.7.intermediate.dense.bias', 'bertmodel.encoder.layer.7.output.dense.weight', 'bertmodel.encoder.layer.7.output.dense.bias', 'bertmodel.encoder.layer.7.output.LayerNorm.weight', 'bertmodel.encoder.layer.7.output.LayerNorm.bias', 'bertmodel.encoder.layer.8.attention.self.query.weight', 'bertmodel.encoder.layer.8.attention.self.query.bias', 'bertmodel.encoder.layer.8.attention.self.key.weight', 'bertmodel.encoder.layer.8.attention.self.key.bias', 'bertmodel.encoder.layer.8.attention.self.value.weight', 'bertmodel.encoder.layer.8.attention.self.value.bias', 'bertmodel.encoder.layer.8.attention.output.dense.weight', 'bertmodel.encoder.layer.8.attention.output.dense.bias', 'bertmodel.encoder.layer.8.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.8.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.8.intermediate.dense.weight', 'bertmodel.encoder.layer.8.intermediate.dense.bias', 'bertmodel.encoder.layer.8.output.dense.weight', 'bertmodel.encoder.layer.8.output.dense.bias', 'bertmodel.encoder.layer.8.output.LayerNorm.weight', 'bertmodel.encoder.layer.8.output.LayerNorm.bias', 'bertmodel.encoder.layer.9.attention.self.query.weight', 'bertmodel.encoder.layer.9.attention.self.query.bias', 'bertmodel.encoder.layer.9.attention.self.key.weight', 'bertmodel.encoder.layer.9.attention.self.key.bias', 'bertmodel.encoder.layer.9.attention.self.value.weight', 'bertmodel.encoder.layer.9.attention.self.value.bias', 'bertmodel.encoder.layer.9.attention.output.dense.weight', 'bertmodel.encoder.layer.9.attention.output.dense.bias', 'bertmodel.encoder.layer.9.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.9.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.9.intermediate.dense.weight', 'bertmodel.encoder.layer.9.intermediate.dense.bias', 'bertmodel.encoder.layer.9.output.dense.weight', 'bertmodel.encoder.layer.9.output.dense.bias', 'bertmodel.encoder.layer.9.output.LayerNorm.weight', 'bertmodel.encoder.layer.9.output.LayerNorm.bias', 'bertmodel.encoder.layer.10.attention.self.query.weight', 'bertmodel.encoder.layer.10.attention.self.query.bias', 'bertmodel.encoder.layer.10.attention.self.key.weight', 'bertmodel.encoder.layer.10.attention.self.key.bias', 'bertmodel.encoder.layer.10.attention.self.value.weight', 'bertmodel.encoder.layer.10.attention.self.value.bias', 'bertmodel.encoder.layer.10.attention.output.dense.weight', 'bertmodel.encoder.layer.10.attention.output.dense.bias', 'bertmodel.encoder.layer.10.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.10.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.10.intermediate.dense.weight', 'bertmodel.encoder.layer.10.intermediate.dense.bias', 'bertmodel.encoder.layer.10.output.dense.weight', 'bertmodel.encoder.layer.10.output.dense.bias', 'bertmodel.encoder.layer.10.output.LayerNorm.weight', 'bertmodel.encoder.layer.10.output.LayerNorm.bias', 'bertmodel.encoder.layer.11.attention.self.query.weight', 'bertmodel.encoder.layer.11.attention.self.query.bias', 'bertmodel.encoder.layer.11.attention.self.key.weight', 'bertmodel.encoder.layer.11.attention.self.key.bias', 'bertmodel.encoder.layer.11.attention.self.value.weight', 'bertmodel.encoder.layer.11.attention.self.value.bias', 'bertmodel.encoder.layer.11.attention.output.dense.weight', 'bertmodel.encoder.layer.11.attention.output.dense.bias', 'bertmodel.encoder.layer.11.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.11.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.11.intermediate.dense.weight', 'bertmodel.encoder.layer.11.intermediate.dense.bias', 'bertmodel.encoder.layer.11.output.dense.weight', 'bertmodel.encoder.layer.11.output.dense.bias', 'bertmodel.encoder.layer.11.output.LayerNorm.weight', 'bertmodel.encoder.layer.11.output.LayerNorm.bias', 'bertmodel.pooler.dense.weight', 'bertmodel.pooler.dense.bias', 'ffn1.weight', 'ffn1.bias', 'ffn2.weight', 'ffn2.bias'])\n"
          ]
        }
      ],
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/mBert.pt')\n",
        "print(state_dict.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ceYSfOMH--q"
      },
      "source": [
        "**Indic BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoPyQsSDBYJc",
        "outputId": "36e17389-f11a-43ae-dd2d-43c9429a87d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiEtMH-_I4ku"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wrFYCG-mB80A",
        "outputId": "5a182124-b717-4b4f-912a-dc9401e176b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4789b770-ff59-4e29-bee5-6ae1d3aef894\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6060</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6062</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6063</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6064</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6065 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4789b770-ff59-4e29-bee5-6ae1d3aef894')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4789b770-ff59-4e29-bee5-6ae1d3aef894 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4789b770-ff59-4e29-bee5-6ae1d3aef894');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          1   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          1   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "6060                                 कश्मीर भगवान से है    HOF          1   \n",
              "6061  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "6062     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "6063  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          1   \n",
              "6064  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "6060             0  \n",
              "6061             1  \n",
              "6062             1  \n",
              "6063             0  \n",
              "6064             1  \n",
              "\n",
              "[6065 rows x 4 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv('drive/MyDrive/final_hindi_backtranslated.csv')\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "499986f2c57e469b98b1e73721788436",
            "29584c9137fa42878952b7411e9732b1",
            "12e97d68ae3f47a09eb9a34923cb22e1",
            "553e465cacd740c08c911d19ae879f71",
            "831f4a268fb74033a00f2db882352b02",
            "5e6f4cd8e3134486b3dd8adbec1e10e7",
            "2ea96408e7204b408378f56f9235e66f",
            "cbc3e836518f4006a8870091b1526632",
            "a034885c1eeb47b58553185437ca6b53",
            "4b90dbf447834a9a85098874fa977f96",
            "875e8610c00b4f14bf55bff3a2e1e2cb",
            "8cef96f98efe473398ef2f9a8ce88b9a",
            "e4a2240694a64b9dbe18ba4572e202f3",
            "cac3c81be3034b34bdec137646037d0b",
            "fb8584c4f6ea4880b0ff57c5f7dee08d",
            "6c936be896b042e7a8429aaeec145819",
            "c38211566c994e7e951008ab4b7dba59",
            "0f0a9ee121384fb3878112f616d8962c",
            "11357a9c05cb4783924f5985c92eadb6",
            "a61d3df12ba0499cbc560f8ccbd3fc8a",
            "368c321ad2ba4afdab7e13add5f3fdb1",
            "91e36e79d5154d72b7eb7f0b76ff2929"
          ]
        },
        "id": "2eqByFfNIVt2",
        "outputId": "f2907139-4f23-4955-cd09-16267be195b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499986f2c57e469b98b1e73721788436",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cef96f98efe473398ef2f9a8ce88b9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXeT2mHIa5z"
      },
      "outputs": [],
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3V06ZTgIdJ7"
      },
      "outputs": [],
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['text'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap5k6zxbIg6C",
        "outputId": "ed6e59e2-d23a-4ef5-cde1-2f351c1dbb72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Input:  torch.Size([6065, 300])\n",
            "Text Input Attention:  torch.Size([6065, 300])\n",
            "Labels:  torch.Size([6065, 2])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6CaIoOmIi_o",
        "outputId": "37b08d6f-4e62-42eb-f06e-465aa35f36c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmjtrarvImW-"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bertmodel = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
        "        self.ffn1 = nn.Linear(768, hidden_dim)\n",
        "        self.dp1 = nn.Dropout()\n",
        "        self.ffn2 = nn.Linear(hidden_dim, num_labels)\n",
        "        \n",
        "    def forward(self, in_T, in_T_attn_masks):\n",
        "        outputs = self.bertmodel(in_T, in_T_attn_masks)\n",
        "        x = torch.mean(outputs.last_hidden_state, dim=1) \n",
        "        x = F.relu(self.ffn1(x))\n",
        "        x = self.dp1(x)\n",
        "        x = torch.sigmoid(self.ffn2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "36ce295db8e74c3cacf75f6d73468996",
            "83a809bc7c49487bbab7ede3e168ad58",
            "d1cf6edcbe424bf687af12333526101b",
            "068d5fcee1da4a688347a130c571f367",
            "25fe45cfbdf54deca03643cac961c608",
            "1282f503b52746ee908fbd7845d6aa17",
            "e00d35df68f04c5a912f7d08579eb0a9",
            "969e6276a8194f1997fd555ecdb4a680",
            "74dcf26698754e3491ce2e0ffa654a01",
            "077f3e2778f0424e9c3012d7baac43c1",
            "2bba2e0d5a004b3d986cda12454516d6"
          ]
        },
        "id": "rImJLM7wIpHa",
        "outputId": "c7364494-da8e-486b-9875-931ae2e3b546"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36ce295db8e74c3cacf75f6d73468996",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'sop_classifier.classifier.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'predictions.dense.bias', 'predictions.dense.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = MultiClassClassifier(100, 2).to(device) # 100 hidden dimension, 2 lables\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8) # Adam with weight decay\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3-enlBZIrvz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AU6Z51LIub-",
        "outputId": "e50a72fb-9262-4194-906f-a98c966edfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.69297\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.69223\n",
            "  Batch    15  of    607.    Elapsed: 0:00:06. Loss: 0.68966\n",
            "  Batch    20  of    607.    Elapsed: 0:00:08. Loss: 0.68973\n",
            "  Batch    25  of    607.    Elapsed: 0:00:10. Loss: 0.69051\n",
            "  Batch    30  of    607.    Elapsed: 0:00:12. Loss: 0.68815\n",
            "  Batch    35  of    607.    Elapsed: 0:00:14. Loss: 0.68645\n",
            "  Batch    40  of    607.    Elapsed: 0:00:16. Loss: 0.68501\n",
            "  Batch    45  of    607.    Elapsed: 0:00:17. Loss: 0.68584\n",
            "  Batch    50  of    607.    Elapsed: 0:00:19. Loss: 0.68466\n",
            "  Batch    55  of    607.    Elapsed: 0:00:21. Loss: 0.68348\n",
            "  Batch    60  of    607.    Elapsed: 0:00:23. Loss: 0.68295\n",
            "  Batch    65  of    607.    Elapsed: 0:00:25. Loss: 0.68126\n",
            "  Batch    70  of    607.    Elapsed: 0:00:27. Loss: 0.68186\n",
            "  Batch    75  of    607.    Elapsed: 0:00:29. Loss: 0.67958\n",
            "  Batch    80  of    607.    Elapsed: 0:00:31. Loss: 0.67817\n",
            "  Batch    85  of    607.    Elapsed: 0:00:33. Loss: 0.67735\n",
            "  Batch    90  of    607.    Elapsed: 0:00:35. Loss: 0.67439\n",
            "  Batch    95  of    607.    Elapsed: 0:00:37. Loss: 0.67531\n",
            "  Batch   100  of    607.    Elapsed: 0:00:39. Loss: 0.67274\n",
            "  Batch   105  of    607.    Elapsed: 0:00:42. Loss: 0.67079\n",
            "  Batch   110  of    607.    Elapsed: 0:00:44. Loss: 0.66894\n",
            "  Batch   115  of    607.    Elapsed: 0:00:46. Loss: 0.66808\n",
            "  Batch   120  of    607.    Elapsed: 0:00:48. Loss: 0.66850\n",
            "  Batch   125  of    607.    Elapsed: 0:00:50. Loss: 0.66639\n",
            "  Batch   130  of    607.    Elapsed: 0:00:52. Loss: 0.66559\n",
            "  Batch   135  of    607.    Elapsed: 0:00:54. Loss: 0.66576\n",
            "  Batch   140  of    607.    Elapsed: 0:00:56. Loss: 0.66583\n",
            "  Batch   145  of    607.    Elapsed: 0:00:58. Loss: 0.66366\n",
            "  Batch   150  of    607.    Elapsed: 0:01:00. Loss: 0.66371\n",
            "  Batch   155  of    607.    Elapsed: 0:01:02. Loss: 0.66382\n",
            "  Batch   160  of    607.    Elapsed: 0:01:04. Loss: 0.66192\n",
            "  Batch   165  of    607.    Elapsed: 0:01:06. Loss: 0.66047\n",
            "  Batch   170  of    607.    Elapsed: 0:01:08. Loss: 0.65914\n",
            "  Batch   175  of    607.    Elapsed: 0:01:10. Loss: 0.65998\n",
            "  Batch   180  of    607.    Elapsed: 0:01:12. Loss: 0.66240\n",
            "  Batch   185  of    607.    Elapsed: 0:01:14. Loss: 0.66165\n",
            "  Batch   190  of    607.    Elapsed: 0:01:16. Loss: 0.66161\n",
            "  Batch   195  of    607.    Elapsed: 0:01:18. Loss: 0.66210\n",
            "  Batch   200  of    607.    Elapsed: 0:01:20. Loss: 0.66204\n",
            "  Batch   205  of    607.    Elapsed: 0:01:22. Loss: 0.66107\n",
            "  Batch   210  of    607.    Elapsed: 0:01:24. Loss: 0.66001\n",
            "  Batch   215  of    607.    Elapsed: 0:01:26. Loss: 0.66004\n",
            "  Batch   220  of    607.    Elapsed: 0:01:28. Loss: 0.66024\n",
            "  Batch   225  of    607.    Elapsed: 0:01:30. Loss: 0.65921\n",
            "  Batch   230  of    607.    Elapsed: 0:01:32. Loss: 0.65900\n",
            "  Batch   235  of    607.    Elapsed: 0:01:34. Loss: 0.65856\n",
            "  Batch   240  of    607.    Elapsed: 0:01:37. Loss: 0.65752\n",
            "  Batch   245  of    607.    Elapsed: 0:01:39. Loss: 0.65650\n",
            "  Batch   250  of    607.    Elapsed: 0:01:41. Loss: 0.65721\n",
            "  Batch   255  of    607.    Elapsed: 0:01:43. Loss: 0.65663\n",
            "  Batch   260  of    607.    Elapsed: 0:01:45. Loss: 0.65603\n",
            "  Batch   265  of    607.    Elapsed: 0:01:47. Loss: 0.65515\n",
            "  Batch   270  of    607.    Elapsed: 0:01:49. Loss: 0.65310\n",
            "  Batch   275  of    607.    Elapsed: 0:01:51. Loss: 0.65365\n",
            "  Batch   280  of    607.    Elapsed: 0:01:53. Loss: 0.65303\n",
            "  Batch   285  of    607.    Elapsed: 0:01:55. Loss: 0.65254\n",
            "  Batch   290  of    607.    Elapsed: 0:01:58. Loss: 0.65189\n",
            "  Batch   295  of    607.    Elapsed: 0:02:00. Loss: 0.65056\n",
            "  Batch   300  of    607.    Elapsed: 0:02:02. Loss: 0.65140\n",
            "  Batch   305  of    607.    Elapsed: 0:02:04. Loss: 0.65136\n",
            "  Batch   310  of    607.    Elapsed: 0:02:06. Loss: 0.64985\n",
            "  Batch   315  of    607.    Elapsed: 0:02:08. Loss: 0.64987\n",
            "  Batch   320  of    607.    Elapsed: 0:02:10. Loss: 0.65034\n",
            "  Batch   325  of    607.    Elapsed: 0:02:12. Loss: 0.64929\n",
            "  Batch   330  of    607.    Elapsed: 0:02:15. Loss: 0.64980\n",
            "  Batch   335  of    607.    Elapsed: 0:02:17. Loss: 0.64860\n",
            "  Batch   340  of    607.    Elapsed: 0:02:19. Loss: 0.64848\n",
            "  Batch   345  of    607.    Elapsed: 0:02:21. Loss: 0.64842\n",
            "  Batch   350  of    607.    Elapsed: 0:02:23. Loss: 0.64682\n",
            "  Batch   355  of    607.    Elapsed: 0:02:25. Loss: 0.64757\n",
            "  Batch   360  of    607.    Elapsed: 0:02:27. Loss: 0.64849\n",
            "  Batch   365  of    607.    Elapsed: 0:02:30. Loss: 0.64832\n",
            "  Batch   370  of    607.    Elapsed: 0:02:32. Loss: 0.64817\n",
            "  Batch   375  of    607.    Elapsed: 0:02:34. Loss: 0.64723\n",
            "  Batch   380  of    607.    Elapsed: 0:02:36. Loss: 0.64679\n",
            "  Batch   385  of    607.    Elapsed: 0:02:38. Loss: 0.64811\n",
            "  Batch   390  of    607.    Elapsed: 0:02:40. Loss: 0.64734\n",
            "  Batch   395  of    607.    Elapsed: 0:02:43. Loss: 0.64696\n",
            "  Batch   400  of    607.    Elapsed: 0:02:45. Loss: 0.64602\n",
            "  Batch   405  of    607.    Elapsed: 0:02:47. Loss: 0.64571\n",
            "  Batch   410  of    607.    Elapsed: 0:02:49. Loss: 0.64677\n",
            "  Batch   415  of    607.    Elapsed: 0:02:51. Loss: 0.64601\n",
            "  Batch   420  of    607.    Elapsed: 0:02:53. Loss: 0.64505\n",
            "  Batch   425  of    607.    Elapsed: 0:02:55. Loss: 0.64481\n",
            "  Batch   430  of    607.    Elapsed: 0:02:57. Loss: 0.64525\n",
            "  Batch   435  of    607.    Elapsed: 0:03:00. Loss: 0.64626\n",
            "  Batch   440  of    607.    Elapsed: 0:03:02. Loss: 0.64555\n",
            "  Batch   445  of    607.    Elapsed: 0:03:04. Loss: 0.64545\n",
            "  Batch   450  of    607.    Elapsed: 0:03:06. Loss: 0.64525\n",
            "  Batch   455  of    607.    Elapsed: 0:03:08. Loss: 0.64471\n",
            "  Batch   460  of    607.    Elapsed: 0:03:10. Loss: 0.64400\n",
            "  Batch   465  of    607.    Elapsed: 0:03:13. Loss: 0.64363\n",
            "  Batch   470  of    607.    Elapsed: 0:03:15. Loss: 0.64274\n",
            "  Batch   475  of    607.    Elapsed: 0:03:17. Loss: 0.64233\n",
            "  Batch   480  of    607.    Elapsed: 0:03:19. Loss: 0.64172\n",
            "  Batch   485  of    607.    Elapsed: 0:03:21. Loss: 0.64102\n",
            "  Batch   490  of    607.    Elapsed: 0:03:23. Loss: 0.64083\n",
            "  Batch   495  of    607.    Elapsed: 0:03:25. Loss: 0.64053\n",
            "  Batch   500  of    607.    Elapsed: 0:03:28. Loss: 0.64000\n",
            "  Batch   505  of    607.    Elapsed: 0:03:30. Loss: 0.64020\n",
            "  Batch   510  of    607.    Elapsed: 0:03:32. Loss: 0.63914\n",
            "  Batch   515  of    607.    Elapsed: 0:03:34. Loss: 0.63860\n",
            "  Batch   520  of    607.    Elapsed: 0:03:36. Loss: 0.63879\n",
            "  Batch   525  of    607.    Elapsed: 0:03:38. Loss: 0.63943\n",
            "  Batch   530  of    607.    Elapsed: 0:03:41. Loss: 0.63943\n",
            "  Batch   535  of    607.    Elapsed: 0:03:43. Loss: 0.63870\n",
            "  Batch   540  of    607.    Elapsed: 0:03:45. Loss: 0.63884\n",
            "  Batch   545  of    607.    Elapsed: 0:03:47. Loss: 0.63890\n",
            "  Batch   550  of    607.    Elapsed: 0:03:49. Loss: 0.63898\n",
            "  Batch   555  of    607.    Elapsed: 0:03:51. Loss: 0.63815\n",
            "  Batch   560  of    607.    Elapsed: 0:03:54. Loss: 0.63796\n",
            "  Batch   565  of    607.    Elapsed: 0:03:56. Loss: 0.63710\n",
            "  Batch   570  of    607.    Elapsed: 0:03:58. Loss: 0.63698\n",
            "  Batch   575  of    607.    Elapsed: 0:04:00. Loss: 0.63650\n",
            "  Batch   580  of    607.    Elapsed: 0:04:02. Loss: 0.63571\n",
            "  Batch   585  of    607.    Elapsed: 0:04:05. Loss: 0.63576\n",
            "  Batch   590  of    607.    Elapsed: 0:04:07. Loss: 0.63491\n",
            "  Batch   595  of    607.    Elapsed: 0:04:09. Loss: 0.63456\n",
            "  Batch   600  of    607.    Elapsed: 0:04:11. Loss: 0.63413\n",
            "  Batch   605  of    607.    Elapsed: 0:04:13. Loss: 0.63417\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:04:14\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:25\n",
            "  Accuracy: 0.52\n",
            "  Accuracy: 0.52\n",
            "  Macro F1-score: 0.42\n",
            "  Macro F1-score: 0.34\n",
            "  Weighted F1-score: 0.41\n",
            "  Weighted F1-score: 0.36\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.10      0.18       634\n",
            "         1.0       0.50      0.99      0.66       579\n",
            "\n",
            "    accuracy                           0.52      1213\n",
            "   macro avg       0.70      0.54      0.42      1213\n",
            "weighted avg       0.71      0.52      0.41      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00       579\n",
            "         1.0       0.52      1.00      0.69       634\n",
            "\n",
            "    accuracy                           0.52      1213\n",
            "   macro avg       0.26      0.50      0.34      1213\n",
            "weighted avg       0.27      0.52      0.36      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 62 572]\n",
            " [  7 572]]\n",
            "[[  0 579]\n",
            " [  0 634]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.54910\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.55483\n",
            "  Batch    15  of    607.    Elapsed: 0:00:07. Loss: 0.55794\n",
            "  Batch    20  of    607.    Elapsed: 0:00:09. Loss: 0.54812\n",
            "  Batch    25  of    607.    Elapsed: 0:00:11. Loss: 0.56313\n",
            "  Batch    30  of    607.    Elapsed: 0:00:13. Loss: 0.56686\n",
            "  Batch    35  of    607.    Elapsed: 0:00:15. Loss: 0.56733\n",
            "  Batch    40  of    607.    Elapsed: 0:00:18. Loss: 0.55898\n",
            "  Batch    45  of    607.    Elapsed: 0:00:20. Loss: 0.55664\n",
            "  Batch    50  of    607.    Elapsed: 0:00:22. Loss: 0.55361\n",
            "  Batch    55  of    607.    Elapsed: 0:00:24. Loss: 0.55887\n",
            "  Batch    60  of    607.    Elapsed: 0:00:26. Loss: 0.56403\n",
            "  Batch    65  of    607.    Elapsed: 0:00:29. Loss: 0.56281\n",
            "  Batch    70  of    607.    Elapsed: 0:00:31. Loss: 0.56631\n",
            "  Batch    75  of    607.    Elapsed: 0:00:33. Loss: 0.56677\n",
            "  Batch    80  of    607.    Elapsed: 0:00:35. Loss: 0.56434\n",
            "  Batch    85  of    607.    Elapsed: 0:00:37. Loss: 0.56195\n",
            "  Batch    90  of    607.    Elapsed: 0:00:39. Loss: 0.55804\n",
            "  Batch    95  of    607.    Elapsed: 0:00:42. Loss: 0.55684\n",
            "  Batch   100  of    607.    Elapsed: 0:00:44. Loss: 0.55769\n",
            "  Batch   105  of    607.    Elapsed: 0:00:46. Loss: 0.55956\n",
            "  Batch   110  of    607.    Elapsed: 0:00:48. Loss: 0.55897\n",
            "  Batch   115  of    607.    Elapsed: 0:00:51. Loss: 0.55953\n",
            "  Batch   120  of    607.    Elapsed: 0:00:53. Loss: 0.55954\n",
            "  Batch   125  of    607.    Elapsed: 0:00:55. Loss: 0.56149\n",
            "  Batch   130  of    607.    Elapsed: 0:00:57. Loss: 0.56283\n",
            "  Batch   135  of    607.    Elapsed: 0:00:59. Loss: 0.56598\n",
            "  Batch   140  of    607.    Elapsed: 0:01:02. Loss: 0.56390\n",
            "  Batch   145  of    607.    Elapsed: 0:01:04. Loss: 0.56423\n",
            "  Batch   150  of    607.    Elapsed: 0:01:06. Loss: 0.56258\n",
            "  Batch   155  of    607.    Elapsed: 0:01:08. Loss: 0.56348\n",
            "  Batch   160  of    607.    Elapsed: 0:01:10. Loss: 0.56350\n",
            "  Batch   165  of    607.    Elapsed: 0:01:13. Loss: 0.56832\n",
            "  Batch   170  of    607.    Elapsed: 0:01:15. Loss: 0.56904\n",
            "  Batch   175  of    607.    Elapsed: 0:01:17. Loss: 0.56673\n",
            "  Batch   180  of    607.    Elapsed: 0:01:19. Loss: 0.56539\n",
            "  Batch   185  of    607.    Elapsed: 0:01:22. Loss: 0.56557\n",
            "  Batch   190  of    607.    Elapsed: 0:01:24. Loss: 0.56444\n",
            "  Batch   195  of    607.    Elapsed: 0:01:26. Loss: 0.56333\n",
            "  Batch   200  of    607.    Elapsed: 0:01:28. Loss: 0.56383\n",
            "  Batch   205  of    607.    Elapsed: 0:01:30. Loss: 0.56642\n",
            "  Batch   210  of    607.    Elapsed: 0:01:33. Loss: 0.56837\n",
            "  Batch   215  of    607.    Elapsed: 0:01:35. Loss: 0.56704\n",
            "  Batch   220  of    607.    Elapsed: 0:01:37. Loss: 0.56641\n",
            "  Batch   225  of    607.    Elapsed: 0:01:39. Loss: 0.56517\n",
            "  Batch   230  of    607.    Elapsed: 0:01:42. Loss: 0.56383\n",
            "  Batch   235  of    607.    Elapsed: 0:01:44. Loss: 0.56273\n",
            "  Batch   240  of    607.    Elapsed: 0:01:46. Loss: 0.56270\n",
            "  Batch   245  of    607.    Elapsed: 0:01:48. Loss: 0.56423\n",
            "  Batch   250  of    607.    Elapsed: 0:01:50. Loss: 0.56549\n",
            "  Batch   255  of    607.    Elapsed: 0:01:53. Loss: 0.56507\n",
            "  Batch   260  of    607.    Elapsed: 0:01:55. Loss: 0.56482\n",
            "  Batch   265  of    607.    Elapsed: 0:01:57. Loss: 0.56345\n",
            "  Batch   270  of    607.    Elapsed: 0:01:59. Loss: 0.56262\n",
            "  Batch   275  of    607.    Elapsed: 0:02:02. Loss: 0.56443\n",
            "  Batch   280  of    607.    Elapsed: 0:02:04. Loss: 0.56324\n",
            "  Batch   285  of    607.    Elapsed: 0:02:06. Loss: 0.56313\n",
            "  Batch   290  of    607.    Elapsed: 0:02:08. Loss: 0.56290\n",
            "  Batch   295  of    607.    Elapsed: 0:02:11. Loss: 0.56212\n",
            "  Batch   300  of    607.    Elapsed: 0:02:13. Loss: 0.56045\n",
            "  Batch   305  of    607.    Elapsed: 0:02:15. Loss: 0.56258\n",
            "  Batch   310  of    607.    Elapsed: 0:02:17. Loss: 0.56339\n",
            "  Batch   315  of    607.    Elapsed: 0:02:19. Loss: 0.56439\n",
            "  Batch   320  of    607.    Elapsed: 0:02:22. Loss: 0.56477\n",
            "  Batch   325  of    607.    Elapsed: 0:02:24. Loss: 0.56540\n",
            "  Batch   330  of    607.    Elapsed: 0:02:26. Loss: 0.56417\n",
            "  Batch   335  of    607.    Elapsed: 0:02:28. Loss: 0.56343\n",
            "  Batch   340  of    607.    Elapsed: 0:02:31. Loss: 0.56491\n",
            "  Batch   345  of    607.    Elapsed: 0:02:33. Loss: 0.56438\n",
            "  Batch   350  of    607.    Elapsed: 0:02:35. Loss: 0.56373\n",
            "  Batch   355  of    607.    Elapsed: 0:02:37. Loss: 0.56304\n",
            "  Batch   360  of    607.    Elapsed: 0:02:40. Loss: 0.56226\n",
            "  Batch   365  of    607.    Elapsed: 0:02:42. Loss: 0.56126\n",
            "  Batch   370  of    607.    Elapsed: 0:02:44. Loss: 0.55926\n",
            "  Batch   375  of    607.    Elapsed: 0:02:46. Loss: 0.55879\n",
            "  Batch   380  of    607.    Elapsed: 0:02:49. Loss: 0.55794\n",
            "  Batch   385  of    607.    Elapsed: 0:02:51. Loss: 0.55851\n",
            "  Batch   390  of    607.    Elapsed: 0:02:53. Loss: 0.55737\n",
            "  Batch   395  of    607.    Elapsed: 0:02:55. Loss: 0.55824\n",
            "  Batch   400  of    607.    Elapsed: 0:02:58. Loss: 0.55925\n",
            "  Batch   405  of    607.    Elapsed: 0:03:00. Loss: 0.55776\n",
            "  Batch   410  of    607.    Elapsed: 0:03:02. Loss: 0.55754\n",
            "  Batch   415  of    607.    Elapsed: 0:03:04. Loss: 0.55750\n",
            "  Batch   420  of    607.    Elapsed: 0:03:06. Loss: 0.55855\n",
            "  Batch   425  of    607.    Elapsed: 0:03:09. Loss: 0.55863\n",
            "  Batch   430  of    607.    Elapsed: 0:03:11. Loss: 0.55728\n",
            "  Batch   435  of    607.    Elapsed: 0:03:13. Loss: 0.55764\n",
            "  Batch   440  of    607.    Elapsed: 0:03:15. Loss: 0.55834\n",
            "  Batch   445  of    607.    Elapsed: 0:03:18. Loss: 0.55827\n",
            "  Batch   450  of    607.    Elapsed: 0:03:20. Loss: 0.55857\n",
            "  Batch   455  of    607.    Elapsed: 0:03:22. Loss: 0.55729\n",
            "  Batch   460  of    607.    Elapsed: 0:03:24. Loss: 0.55798\n",
            "  Batch   465  of    607.    Elapsed: 0:03:27. Loss: 0.55787\n",
            "  Batch   470  of    607.    Elapsed: 0:03:29. Loss: 0.55776\n",
            "  Batch   475  of    607.    Elapsed: 0:03:31. Loss: 0.55739\n",
            "  Batch   480  of    607.    Elapsed: 0:03:33. Loss: 0.55716\n",
            "  Batch   485  of    607.    Elapsed: 0:03:36. Loss: 0.55610\n",
            "  Batch   490  of    607.    Elapsed: 0:03:38. Loss: 0.55809\n",
            "  Batch   495  of    607.    Elapsed: 0:03:40. Loss: 0.55757\n",
            "  Batch   500  of    607.    Elapsed: 0:03:42. Loss: 0.55686\n",
            "  Batch   505  of    607.    Elapsed: 0:03:44. Loss: 0.55583\n",
            "  Batch   510  of    607.    Elapsed: 0:03:47. Loss: 0.55489\n",
            "  Batch   515  of    607.    Elapsed: 0:03:49. Loss: 0.55554\n",
            "  Batch   520  of    607.    Elapsed: 0:03:51. Loss: 0.55549\n",
            "  Batch   525  of    607.    Elapsed: 0:03:53. Loss: 0.55506\n",
            "  Batch   530  of    607.    Elapsed: 0:03:55. Loss: 0.55505\n",
            "  Batch   535  of    607.    Elapsed: 0:03:58. Loss: 0.55568\n",
            "  Batch   540  of    607.    Elapsed: 0:04:00. Loss: 0.55461\n",
            "  Batch   545  of    607.    Elapsed: 0:04:02. Loss: 0.55566\n",
            "  Batch   550  of    607.    Elapsed: 0:04:04. Loss: 0.55537\n",
            "  Batch   555  of    607.    Elapsed: 0:04:07. Loss: 0.55507\n",
            "  Batch   560  of    607.    Elapsed: 0:04:09. Loss: 0.55377\n",
            "  Batch   565  of    607.    Elapsed: 0:04:11. Loss: 0.55463\n",
            "  Batch   570  of    607.    Elapsed: 0:04:13. Loss: 0.55475\n",
            "  Batch   575  of    607.    Elapsed: 0:04:16. Loss: 0.55570\n",
            "  Batch   580  of    607.    Elapsed: 0:04:18. Loss: 0.55486\n",
            "  Batch   585  of    607.    Elapsed: 0:04:20. Loss: 0.55450\n",
            "  Batch   590  of    607.    Elapsed: 0:04:22. Loss: 0.55389\n",
            "  Batch   595  of    607.    Elapsed: 0:04:25. Loss: 0.55355\n",
            "  Batch   600  of    607.    Elapsed: 0:04:27. Loss: 0.55225\n",
            "  Batch   605  of    607.    Elapsed: 0:04:29. Loss: 0.55269\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:04:30\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.60\n",
            "  Validation took: 0:00:25\n",
            "  Accuracy: 0.74\n",
            "  Accuracy: 0.61\n",
            "  Macro F1-score: 0.74\n",
            "  Macro F1-score: 0.52\n",
            "  Weighted F1-score: 0.74\n",
            "  Weighted F1-score: 0.53\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.84      0.77       634\n",
            "         1.0       0.78      0.63      0.70       579\n",
            "\n",
            "    accuracy                           0.74      1213\n",
            "   macro avg       0.75      0.74      0.74      1213\n",
            "weighted avg       0.75      0.74      0.74      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.19      0.31       579\n",
            "         1.0       0.57      0.99      0.72       634\n",
            "\n",
            "    accuracy                           0.61      1213\n",
            "   macro avg       0.75      0.59      0.52      1213\n",
            "weighted avg       0.74      0.61      0.53      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[533 101]\n",
            " [214 365]]\n",
            "[[109 470]\n",
            " [  8 626]]\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.58167\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.63393\n",
            "  Batch    15  of    607.    Elapsed: 0:00:07. Loss: 0.61453\n",
            "  Batch    20  of    607.    Elapsed: 0:00:09. Loss: 0.59827\n",
            "  Batch    25  of    607.    Elapsed: 0:00:11. Loss: 0.56975\n",
            "  Batch    30  of    607.    Elapsed: 0:00:13. Loss: 0.55583\n",
            "  Batch    35  of    607.    Elapsed: 0:00:16. Loss: 0.53848\n",
            "  Batch    40  of    607.    Elapsed: 0:00:18. Loss: 0.52741\n",
            "  Batch    45  of    607.    Elapsed: 0:00:20. Loss: 0.53518\n",
            "  Batch    50  of    607.    Elapsed: 0:00:22. Loss: 0.53991\n",
            "  Batch    55  of    607.    Elapsed: 0:00:25. Loss: 0.53497\n",
            "  Batch    60  of    607.    Elapsed: 0:00:27. Loss: 0.53880\n",
            "  Batch    65  of    607.    Elapsed: 0:00:29. Loss: 0.53222\n",
            "  Batch    70  of    607.    Elapsed: 0:00:31. Loss: 0.52365\n",
            "  Batch    75  of    607.    Elapsed: 0:00:34. Loss: 0.51781\n",
            "  Batch    80  of    607.    Elapsed: 0:00:36. Loss: 0.51927\n",
            "  Batch    85  of    607.    Elapsed: 0:00:38. Loss: 0.51131\n",
            "  Batch    90  of    607.    Elapsed: 0:00:40. Loss: 0.51482\n",
            "  Batch    95  of    607.    Elapsed: 0:00:43. Loss: 0.51243\n",
            "  Batch   100  of    607.    Elapsed: 0:00:45. Loss: 0.51328\n",
            "  Batch   105  of    607.    Elapsed: 0:00:47. Loss: 0.50887\n",
            "  Batch   110  of    607.    Elapsed: 0:00:49. Loss: 0.50412\n",
            "  Batch   115  of    607.    Elapsed: 0:00:51. Loss: 0.50240\n",
            "  Batch   120  of    607.    Elapsed: 0:00:54. Loss: 0.50225\n",
            "  Batch   125  of    607.    Elapsed: 0:00:56. Loss: 0.50631\n",
            "  Batch   130  of    607.    Elapsed: 0:00:58. Loss: 0.51160\n",
            "  Batch   135  of    607.    Elapsed: 0:01:00. Loss: 0.50552\n",
            "  Batch   140  of    607.    Elapsed: 0:01:03. Loss: 0.50419\n",
            "  Batch   145  of    607.    Elapsed: 0:01:05. Loss: 0.50411\n",
            "  Batch   150  of    607.    Elapsed: 0:01:07. Loss: 0.50364\n",
            "  Batch   155  of    607.    Elapsed: 0:01:09. Loss: 0.50339\n",
            "  Batch   160  of    607.    Elapsed: 0:01:12. Loss: 0.50354\n",
            "  Batch   165  of    607.    Elapsed: 0:01:14. Loss: 0.50164\n",
            "  Batch   170  of    607.    Elapsed: 0:01:16. Loss: 0.49797\n",
            "  Batch   175  of    607.    Elapsed: 0:01:18. Loss: 0.49984\n",
            "  Batch   180  of    607.    Elapsed: 0:01:21. Loss: 0.50122\n",
            "  Batch   185  of    607.    Elapsed: 0:01:23. Loss: 0.49988\n",
            "  Batch   190  of    607.    Elapsed: 0:01:25. Loss: 0.49706\n",
            "  Batch   195  of    607.    Elapsed: 0:01:27. Loss: 0.49711\n",
            "  Batch   200  of    607.    Elapsed: 0:01:30. Loss: 0.49658\n",
            "  Batch   205  of    607.    Elapsed: 0:01:32. Loss: 0.49450\n",
            "  Batch   210  of    607.    Elapsed: 0:01:34. Loss: 0.49395\n",
            "  Batch   215  of    607.    Elapsed: 0:01:36. Loss: 0.49150\n",
            "  Batch   220  of    607.    Elapsed: 0:01:39. Loss: 0.48745\n",
            "  Batch   225  of    607.    Elapsed: 0:01:41. Loss: 0.48534\n",
            "  Batch   230  of    607.    Elapsed: 0:01:43. Loss: 0.48492\n",
            "  Batch   235  of    607.    Elapsed: 0:01:45. Loss: 0.48262\n",
            "  Batch   240  of    607.    Elapsed: 0:01:48. Loss: 0.48094\n",
            "  Batch   245  of    607.    Elapsed: 0:01:50. Loss: 0.48063\n",
            "  Batch   250  of    607.    Elapsed: 0:01:52. Loss: 0.48238\n",
            "  Batch   255  of    607.    Elapsed: 0:01:54. Loss: 0.48363\n",
            "  Batch   260  of    607.    Elapsed: 0:01:57. Loss: 0.48404\n",
            "  Batch   265  of    607.    Elapsed: 0:01:59. Loss: 0.48438\n",
            "  Batch   270  of    607.    Elapsed: 0:02:01. Loss: 0.48499\n",
            "  Batch   275  of    607.    Elapsed: 0:02:03. Loss: 0.48317\n",
            "  Batch   280  of    607.    Elapsed: 0:02:06. Loss: 0.48442\n",
            "  Batch   285  of    607.    Elapsed: 0:02:08. Loss: 0.48605\n",
            "  Batch   290  of    607.    Elapsed: 0:02:10. Loss: 0.48716\n",
            "  Batch   295  of    607.    Elapsed: 0:02:12. Loss: 0.48634\n",
            "  Batch   300  of    607.    Elapsed: 0:02:15. Loss: 0.48474\n",
            "  Batch   305  of    607.    Elapsed: 0:02:17. Loss: 0.48651\n",
            "  Batch   310  of    607.    Elapsed: 0:02:19. Loss: 0.48572\n",
            "  Batch   315  of    607.    Elapsed: 0:02:21. Loss: 0.48619\n",
            "  Batch   320  of    607.    Elapsed: 0:02:24. Loss: 0.48502\n",
            "  Batch   325  of    607.    Elapsed: 0:02:26. Loss: 0.48572\n",
            "  Batch   330  of    607.    Elapsed: 0:02:28. Loss: 0.48688\n",
            "  Batch   335  of    607.    Elapsed: 0:02:30. Loss: 0.48667\n",
            "  Batch   340  of    607.    Elapsed: 0:02:32. Loss: 0.48585\n",
            "  Batch   345  of    607.    Elapsed: 0:02:35. Loss: 0.48512\n",
            "  Batch   350  of    607.    Elapsed: 0:02:37. Loss: 0.48467\n",
            "  Batch   355  of    607.    Elapsed: 0:02:39. Loss: 0.48445\n",
            "  Batch   360  of    607.    Elapsed: 0:02:41. Loss: 0.48382\n",
            "  Batch   365  of    607.    Elapsed: 0:02:44. Loss: 0.48652\n",
            "  Batch   370  of    607.    Elapsed: 0:02:46. Loss: 0.48802\n",
            "  Batch   375  of    607.    Elapsed: 0:02:48. Loss: 0.48697\n",
            "  Batch   380  of    607.    Elapsed: 0:02:50. Loss: 0.48936\n",
            "  Batch   385  of    607.    Elapsed: 0:02:53. Loss: 0.48761\n",
            "  Batch   390  of    607.    Elapsed: 0:02:55. Loss: 0.48891\n",
            "  Batch   395  of    607.    Elapsed: 0:02:57. Loss: 0.49107\n",
            "  Batch   400  of    607.    Elapsed: 0:02:59. Loss: 0.49248\n",
            "  Batch   405  of    607.    Elapsed: 0:03:01. Loss: 0.49272\n",
            "  Batch   410  of    607.    Elapsed: 0:03:04. Loss: 0.49326\n",
            "  Batch   415  of    607.    Elapsed: 0:03:06. Loss: 0.49250\n",
            "  Batch   420  of    607.    Elapsed: 0:03:08. Loss: 0.49160\n",
            "  Batch   425  of    607.    Elapsed: 0:03:10. Loss: 0.49270\n",
            "  Batch   430  of    607.    Elapsed: 0:03:13. Loss: 0.49205\n",
            "  Batch   435  of    607.    Elapsed: 0:03:15. Loss: 0.49040\n",
            "  Batch   440  of    607.    Elapsed: 0:03:17. Loss: 0.49095\n",
            "  Batch   445  of    607.    Elapsed: 0:03:19. Loss: 0.49154\n",
            "  Batch   450  of    607.    Elapsed: 0:03:22. Loss: 0.49029\n",
            "  Batch   455  of    607.    Elapsed: 0:03:24. Loss: 0.48956\n",
            "  Batch   460  of    607.    Elapsed: 0:03:26. Loss: 0.48872\n",
            "  Batch   465  of    607.    Elapsed: 0:03:28. Loss: 0.48771\n",
            "  Batch   470  of    607.    Elapsed: 0:03:30. Loss: 0.48747\n",
            "  Batch   475  of    607.    Elapsed: 0:03:33. Loss: 0.48793\n",
            "  Batch   480  of    607.    Elapsed: 0:03:35. Loss: 0.48828\n",
            "  Batch   485  of    607.    Elapsed: 0:03:37. Loss: 0.48629\n",
            "  Batch   490  of    607.    Elapsed: 0:03:39. Loss: 0.48677\n",
            "  Batch   495  of    607.    Elapsed: 0:03:42. Loss: 0.48761\n",
            "  Batch   500  of    607.    Elapsed: 0:03:44. Loss: 0.48643\n",
            "  Batch   505  of    607.    Elapsed: 0:03:46. Loss: 0.48660\n",
            "  Batch   510  of    607.    Elapsed: 0:03:48. Loss: 0.48663\n",
            "  Batch   515  of    607.    Elapsed: 0:03:51. Loss: 0.48761\n",
            "  Batch   520  of    607.    Elapsed: 0:03:53. Loss: 0.48860\n",
            "  Batch   525  of    607.    Elapsed: 0:03:55. Loss: 0.49024\n",
            "  Batch   530  of    607.    Elapsed: 0:03:57. Loss: 0.48986\n",
            "  Batch   535  of    607.    Elapsed: 0:04:00. Loss: 0.48950\n",
            "  Batch   540  of    607.    Elapsed: 0:04:02. Loss: 0.48866\n",
            "  Batch   545  of    607.    Elapsed: 0:04:04. Loss: 0.48882\n",
            "  Batch   550  of    607.    Elapsed: 0:04:06. Loss: 0.48827\n",
            "  Batch   555  of    607.    Elapsed: 0:04:09. Loss: 0.48743\n",
            "  Batch   560  of    607.    Elapsed: 0:04:11. Loss: 0.48706\n",
            "  Batch   565  of    607.    Elapsed: 0:04:13. Loss: 0.48625\n",
            "  Batch   570  of    607.    Elapsed: 0:04:15. Loss: 0.48569\n",
            "  Batch   575  of    607.    Elapsed: 0:04:18. Loss: 0.48687\n",
            "  Batch   580  of    607.    Elapsed: 0:04:20. Loss: 0.48635\n",
            "  Batch   585  of    607.    Elapsed: 0:04:22. Loss: 0.48598\n",
            "  Batch   590  of    607.    Elapsed: 0:04:24. Loss: 0.48587\n",
            "  Batch   595  of    607.    Elapsed: 0:04:27. Loss: 0.48676\n",
            "  Batch   600  of    607.    Elapsed: 0:04:29. Loss: 0.48685\n",
            "  Batch   605  of    607.    Elapsed: 0:04:31. Loss: 0.48640\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:04:32\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.52\n",
            "  Validation took: 0:00:25\n",
            "  Accuracy: 0.74\n",
            "  Accuracy: 0.76\n",
            "  Macro F1-score: 0.74\n",
            "  Macro F1-score: 0.76\n",
            "  Weighted F1-score: 0.74\n",
            "  Weighted F1-score: 0.76\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.66      0.73       634\n",
            "         1.0       0.69      0.83      0.75       579\n",
            "\n",
            "    accuracy                           0.74      1213\n",
            "   macro avg       0.75      0.74      0.74      1213\n",
            "weighted avg       0.75      0.74      0.74      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.68      0.73       579\n",
            "         1.0       0.74      0.83      0.78       634\n",
            "\n",
            "    accuracy                           0.76      1213\n",
            "   macro avg       0.77      0.76      0.76      1213\n",
            "weighted avg       0.76      0.76      0.76      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[416 218]\n",
            " [ 97 482]]\n",
            "[[394 185]\n",
            " [105 529]]\n"
          ]
        }
      ],
      "source": [
        "epochs = 3   \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,2))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(2)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(2):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(2):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(2)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(2)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(2)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVnwdyZ5Mz1j"
      },
      "source": [
        "**Back translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "WF1jhrK1OEdY",
        "outputId": "297d4794-3721-4869-fd7b-cdcd14cee0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.*\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.12.7)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17413 sha256=89bda664662eee05accc582057629f6b9ce0bac8bd993006be8275e19da0f69a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh6izqhzNt2R"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "import pandas as pd\n",
        "import httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kft3oUYNJIu"
      },
      "outputs": [],
      "source": [
        "batch_1 = train_df['text'][:100]\n",
        "timeout = httpx.Timeout(15)\n",
        "translator = Translator(service_urls=['translate.google.com'],timeout=timeout)\n",
        "\n",
        "# Back translate the Hindi tweets to English and then back to Hindi\n",
        "back_translated = []\n",
        "for text in batch_1:\n",
        "    hindi_tweet = text\n",
        "    english_translation = translator.translate(hindi_tweet, src='hi', dest='en').text\n",
        "    back_translation = translator.translate(english_translation, src='en', dest='hi').text\n",
        "    back_translated.append(back_translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA9r78faDnbJ",
        "outputId": "7eeab103-3bc1-45ab-8c73-6156d3a3925c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(back_translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZfwXdjrIMta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6M6pFhIPzUf"
      },
      "outputs": [],
      "source": [
        "batch_2 = train_df['text'][100:300]\n",
        "timeout = httpx.Timeout(15)\n",
        "translator = Translator(service_urls=['translate.google.com'],timeout=timeout)\n",
        "\n",
        "# Back translate the Hindi tweets to English and then back to Hindi\n",
        "for text in batch_2:\n",
        "    hindi_tweet = text\n",
        "    english_translation = translator.translate(hindi_tweet, src='hi', dest='en').text\n",
        "    back_translation = translator.translate(english_translation, src='en', dest='hi').text\n",
        "    back_translated.append(back_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3GZ_0CTaE95",
        "outputId": "2a42e5f4-6f3c-4794-e855-738e2830dbf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(back_translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5SNxC9Gk1BT"
      },
      "outputs": [],
      "source": [
        "batch_3 = train_df['text'][300:500]\n",
        "timeout = httpx.Timeout(15)\n",
        "translator = Translator(service_urls=['translate.google.com'],timeout=timeout)\n",
        "\n",
        "# Back translate the Hindi tweets to English and then back to Hindi\n",
        "for text in batch_3:\n",
        "    hindi_tweet = text\n",
        "    english_translation = translator.translate(hindi_tweet, src='hi', dest='en').text\n",
        "    back_translation = translator.translate(english_translation, src='en', dest='hi').text\n",
        "    back_translated.append(back_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSDo_3hLH6GX"
      },
      "outputs": [],
      "source": [
        "batch_8 = train_df['text'][1400:2000]\n",
        "timeout = httpx.Timeout(15)\n",
        "translator = Translator(service_urls=['translate.google.com'],timeout=timeout)\n",
        "back_translated = []\n",
        "# Back translate the Hindi tweets to English and then back to Hindi\n",
        "for text in batch_8:\n",
        "    hindi_tweet = text\n",
        "    english_translation = translator.translate(hindi_tweet, src='hi', dest='en').text\n",
        "    back_translation = translator.translate(english_translation, src='en', dest='hi').text\n",
        "    back_translated.append(back_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cqydmxjnnBO",
        "outputId": "3854b620-e997-44ad-bc42-d197fe06b73a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(back_translated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xouaVwCbKdOk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO2uJH1cKtVz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-qlKYykK6hf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoeJpZKraM50",
        "outputId": "a3391ab4-172d-4d68-9cec-36bd8611b54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       NOT\n",
              "1       HOF\n",
              "2       HOF\n",
              "3       NOT\n",
              "4       NOT\n",
              "       ... \n",
              "1395    HOF\n",
              "1396    NOT\n",
              "1397    NOT\n",
              "1398    HOF\n",
              "1399    NOT\n",
              "Name: task_1, Length: 1400, dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_L1 = train_df['task_1'][:1400]\n",
        "batch_L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIxjuh3tf-Bs",
        "outputId": "d9825a0d-2a04-4804-83ef-426e8fadc74c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-065077a30f1f>:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  batch_L2 = batch_L1.append(train_df['task_1'][:1400])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0       NOT\n",
              "1       HOF\n",
              "2       HOF\n",
              "3       NOT\n",
              "4       NOT\n",
              "       ... \n",
              "1395    HOF\n",
              "1396    NOT\n",
              "1397    NOT\n",
              "1398    HOF\n",
              "1399    NOT\n",
              "Name: task_1, Length: 2800, dtype: object"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_L2 = batch_L1.append(train_df['task_1'][:1400])\n",
        "batch_L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "vDSkTb-H3nZb",
        "outputId": "16983351-2b3c-4616-87e7-5b0f3b9be110"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-231e38d6-790a-4a5b-bdc4-148e6c7152bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-231e38d6-790a-4a5b-bdc4-148e6c7152bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-231e38d6-790a-4a5b-bdc4-148e6c7152bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-231e38d6-790a-4a5b-bdc4-148e6c7152bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0 task_1\n",
              "0     बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...    NOT\n",
              "1     जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...    HOF\n",
              "2     आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...    HOF\n",
              "3     भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...    NOT\n",
              "4     शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...    NOT\n",
              "...                                                 ...    ...\n",
              "1395                                 कश्मीर भगवान से है    HOF\n",
              "1396  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT\n",
              "1397     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT\n",
              "1398  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF\n",
              "1399  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT\n",
              "\n",
              "[1400 rows x 2 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "back_translated = pd.read_csv('drive/MyDrive/backoutput.csv',sep='\\t')\n",
        "back_translated['task_1'] = batch_L1\n",
        "back_translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "LFAcJVbM3cZV",
        "outputId": "1bf19a1d-0ad7-431c-a99c-27106dbad371"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ed7098fd-be7f-46a2-bfea-70adbaa93ff6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed7098fd-be7f-46a2-bfea-70adbaa93ff6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed7098fd-be7f-46a2-bfea-70adbaa93ff6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed7098fd-be7f-46a2-bfea-70adbaa93ff6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0 task_1  Offensive\n",
              "0     बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...    NOT          0\n",
              "1     जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...    HOF          0\n",
              "2     आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...    HOF          0\n",
              "3     भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...    NOT          0\n",
              "4     शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...    NOT          0\n",
              "...                                                 ...    ...        ...\n",
              "1395                                 कश्मीर भगवान से है    HOF          0\n",
              "1396  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0\n",
              "1397     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0\n",
              "1398  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          0\n",
              "1399  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0\n",
              "\n",
              "[1400 rows x 3 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfb1 = back_translated.assign(Offensive=0)\n",
        "dfb1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YhlsrpvC3cZW",
        "outputId": "1b1d2681-c254-4f69-dcce-a350567e4e41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bf8f8f25-c2d0-446f-bc1e-72de5ebf1dd6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf8f8f25-c2d0-446f-bc1e-72de5ebf1dd6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf8f8f25-c2d0-446f-bc1e-72de5ebf1dd6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf8f8f25-c2d0-446f-bc1e-72de5ebf1dd6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0 task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...    NOT          0   \n",
              "1     जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...    HOF          0   \n",
              "2     आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...    HOF          0   \n",
              "3     भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...    NOT          0   \n",
              "4     शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "1395                                 कश्मीर भगवान से है    HOF          0   \n",
              "1396  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "1397     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "1398  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          0   \n",
              "1399  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                0  \n",
              "1                0  \n",
              "2                0  \n",
              "3                0  \n",
              "4                0  \n",
              "...            ...  \n",
              "1395             0  \n",
              "1396             0  \n",
              "1397             0  \n",
              "1398             0  \n",
              "1399             0  \n",
              "\n",
              "[1400 rows x 4 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dfb = dfb1.assign(NotOffensive=0)\n",
        "train_dfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "YfaDH74O3cZW",
        "outputId": "45fd1b72-2457-4174-8ddd-ed5b1248d9ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-4ca1f7411d7d>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_dfb['Offensive'][index] = 0\n",
            "<ipython-input-30-4ca1f7411d7d>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_dfb['NotOffensive'][index] = 1\n",
            "<ipython-input-30-4ca1f7411d7d>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_dfb['Offensive'][index] = 1\n",
            "<ipython-input-30-4ca1f7411d7d>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_dfb['NotOffensive'][index] = 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-850ab66e-a116-48b5-a1a6-7dfaaf5c56ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-850ab66e-a116-48b5-a1a6-7dfaaf5c56ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-850ab66e-a116-48b5-a1a6-7dfaaf5c56ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-850ab66e-a116-48b5-a1a6-7dfaaf5c56ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0 task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...    NOT          0   \n",
              "1     जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...    HOF          1   \n",
              "2     आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...    HOF          1   \n",
              "3     भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...    NOT          0   \n",
              "4     शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "1395                                 कश्मीर भगवान से है    HOF          1   \n",
              "1396  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "1397     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "1398  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          1   \n",
              "1399  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "1395             0  \n",
              "1396             1  \n",
              "1397             1  \n",
              "1398             0  \n",
              "1399             1  \n",
              "\n",
              "[1400 rows x 4 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for index in train_dfb.index:\n",
        "    k = train_dfb['task_1'][index]\n",
        "    if k == 'HOF':\n",
        "        train_dfb['Offensive'][index] = 1\n",
        "        train_dfb['NotOffensive'][index] = 0\n",
        "    else:\n",
        "        train_dfb['Offensive'][index] = 0\n",
        "        train_dfb['NotOffensive'][index] = 1\n",
        "train_dfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "cbCpiDOIrH9Y",
        "outputId": "8f64f0d5-40d0-4589-bce2-07ef87034fc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-196ae80d-de9b-420a-a10b-b66374049af9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-196ae80d-de9b-420a-a10b-b66374049af9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-196ae80d-de9b-420a-a10b-b66374049af9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-196ae80d-de9b-420a-a10b-b66374049af9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत 314 रन के लिए...    NOT          0   \n",
              "1     जैसे ही कोई वेश्या के नृत्य को देखने में व्यस्...    HOF          1   \n",
              "2     आप जैसे हरविस के लिए जूतों की कमी है, धन्यवाद,...    HOF          1   \n",
              "3     भाजपा के विधायक आकाश विजयवर्गिया, जेल से रिहा,...    NOT          0   \n",
              "4     शाइनिंग बुखार: विधानसभा परिसर में आरजेडी प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "1395                                 कश्मीर भगवान से है    HOF          1   \n",
              "1396  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "1397     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "1398  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          1   \n",
              "1399  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "1395             0  \n",
              "1396             1  \n",
              "1397             1  \n",
              "1398             0  \n",
              "1399             1  \n",
              "\n",
              "[1400 rows x 4 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dfb.columns = ['text','task_1','Offensive','NotOffensive']\n",
        "train_dfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U689jqww5s8g"
      },
      "outputs": [],
      "source": [
        "frames = [train_df, train_dfb]\n",
        "final_train_df = pd.concat(frames)\n",
        "final_train_df\n",
        "final_train_df.to_csv('drive/MyDrive/final_hindi_backtranslated.csv',index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_oL6dS_VrDa5",
        "outputId": "5362270f-a521-4570-e790-7189ad6837d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9639390a-5dfc-4141-8da7-a3b59598a2b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6060</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6062</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6063</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6064</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6065 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9639390a-5dfc-4141-8da7-a3b59598a2b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9639390a-5dfc-4141-8da7-a3b59598a2b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9639390a-5dfc-4141-8da7-a3b59598a2b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          1   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          1   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "6060                                 कश्मीर भगवान से है    HOF          1   \n",
              "6061  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "6062     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "6063  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          1   \n",
              "6064  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "6060             0  \n",
              "6061             1  \n",
              "6062             1  \n",
              "6063             0  \n",
              "6064             1  \n",
              "\n",
              "[6065 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_train_df = pd.read_csv('drive/MyDrive/final_hindi_backtranslated.csv')\n",
        "final_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "f12HJ3GXhfwq",
        "outputId": "8e4a68a7-4d70-459b-831a-f5bc5b8a50cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a0a4ff01-dc3d-4063-9d2c-854d0b76d7dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>number of comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>2888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotOffensive</td>\n",
              "      <td>3177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0a4ff01-dc3d-4063-9d2c-854d0b76d7dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0a4ff01-dc3d-4063-9d2c-854d0b76d7dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0a4ff01-dc3d-4063-9d2c-854d0b76d7dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       category  number of comments\n",
              "0     Offensive                2888\n",
              "1  NotOffensive                3177"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories = final_train_df.columns[2:]\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, final_train_df[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80VPYkCPhfwr"
      },
      "outputs": [],
      "source": [
        "target_list = categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql1ZODfqecgy"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\",do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI8PC-ahez6a"
      },
      "outputs": [],
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT90U25-ez6b"
      },
      "outputs": [],
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['text'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCmDFPuuez6c",
        "outputId": "248b35e8-8252-486f-9c8a-a09dc98e0601"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Input:  torch.Size([6065, 300])\n",
            "Text Input Attention:  torch.Size([6065, 300])\n",
            "Labels:  torch.Size([6065, 2])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    final_train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGxmAfkDez6d",
        "outputId": "6c257c98-d925-4c98-e894-86321e8f4a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOtVeN44ez6e"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bertmodel = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "        self.ffn1 = nn.Linear(768, hidden_dim)\n",
        "        self.dp1 = nn.Dropout()\n",
        "        self.ffn2 = nn.Linear(hidden_dim, num_labels)\n",
        "        \n",
        "    def forward(self, in_T, in_T_attn_masks):\n",
        "        outputs = self.bertmodel(in_T, in_T_attn_masks)\n",
        "        x = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "        x = F.relu(self.ffn1(x))\n",
        "        x = self.dp1(x)\n",
        "        x = torch.sigmoid(self.ffn2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5vF5i-wez6f",
        "outputId": "9b16700c-ebcd-4a58-c41c-f9ea2aa60528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = MultiClassClassifier(100, 2).to(device) # 100 hidden dimension, 2 lables\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8) # Adam with weight decay\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA_3cDNSiB-y"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56VxP3xMez6g",
        "outputId": "c8d10bd9-d79d-4a48-e413-6d8a9431c7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.69452\n",
            "  Batch    10  of    607.    Elapsed: 0:00:05. Loss: 0.69682\n",
            "  Batch    15  of    607.    Elapsed: 0:00:07. Loss: 0.69629\n",
            "  Batch    20  of    607.    Elapsed: 0:00:09. Loss: 0.69504\n",
            "  Batch    25  of    607.    Elapsed: 0:00:12. Loss: 0.69238\n",
            "  Batch    30  of    607.    Elapsed: 0:00:14. Loss: 0.69056\n",
            "  Batch    35  of    607.    Elapsed: 0:00:17. Loss: 0.68681\n",
            "  Batch    40  of    607.    Elapsed: 0:00:19. Loss: 0.68260\n",
            "  Batch    45  of    607.    Elapsed: 0:00:21. Loss: 0.67987\n",
            "  Batch    50  of    607.    Elapsed: 0:00:24. Loss: 0.67449\n",
            "  Batch    55  of    607.    Elapsed: 0:00:26. Loss: 0.66914\n",
            "  Batch    60  of    607.    Elapsed: 0:00:29. Loss: 0.66273\n",
            "  Batch    65  of    607.    Elapsed: 0:00:31. Loss: 0.66474\n",
            "  Batch    70  of    607.    Elapsed: 0:00:34. Loss: 0.66113\n",
            "  Batch    75  of    607.    Elapsed: 0:00:36. Loss: 0.65576\n",
            "  Batch    80  of    607.    Elapsed: 0:00:39. Loss: 0.64697\n",
            "  Batch    85  of    607.    Elapsed: 0:00:41. Loss: 0.64614\n",
            "  Batch    90  of    607.    Elapsed: 0:00:44. Loss: 0.64231\n",
            "  Batch    95  of    607.    Elapsed: 0:00:46. Loss: 0.63781\n",
            "  Batch   100  of    607.    Elapsed: 0:00:49. Loss: 0.63376\n",
            "  Batch   105  of    607.    Elapsed: 0:00:51. Loss: 0.63377\n",
            "  Batch   110  of    607.    Elapsed: 0:00:54. Loss: 0.62884\n",
            "  Batch   115  of    607.    Elapsed: 0:00:57. Loss: 0.62425\n",
            "  Batch   120  of    607.    Elapsed: 0:00:59. Loss: 0.62447\n",
            "  Batch   125  of    607.    Elapsed: 0:01:02. Loss: 0.62221\n",
            "  Batch   130  of    607.    Elapsed: 0:01:05. Loss: 0.61847\n",
            "  Batch   135  of    607.    Elapsed: 0:01:07. Loss: 0.61563\n",
            "  Batch   140  of    607.    Elapsed: 0:01:10. Loss: 0.61302\n",
            "  Batch   145  of    607.    Elapsed: 0:01:13. Loss: 0.60922\n",
            "  Batch   150  of    607.    Elapsed: 0:01:15. Loss: 0.61347\n",
            "  Batch   155  of    607.    Elapsed: 0:01:18. Loss: 0.61140\n",
            "  Batch   160  of    607.    Elapsed: 0:01:21. Loss: 0.60752\n",
            "  Batch   165  of    607.    Elapsed: 0:01:23. Loss: 0.60439\n",
            "  Batch   170  of    607.    Elapsed: 0:01:26. Loss: 0.61124\n",
            "  Batch   175  of    607.    Elapsed: 0:01:28. Loss: 0.60428\n",
            "  Batch   180  of    607.    Elapsed: 0:01:31. Loss: 0.60706\n",
            "  Batch   185  of    607.    Elapsed: 0:01:34. Loss: 0.60700\n",
            "  Batch   190  of    607.    Elapsed: 0:01:36. Loss: 0.60324\n",
            "  Batch   195  of    607.    Elapsed: 0:01:39. Loss: 0.59812\n",
            "  Batch   200  of    607.    Elapsed: 0:01:41. Loss: 0.59693\n",
            "  Batch   205  of    607.    Elapsed: 0:01:44. Loss: 0.59531\n",
            "  Batch   210  of    607.    Elapsed: 0:01:46. Loss: 0.59693\n",
            "  Batch   215  of    607.    Elapsed: 0:01:49. Loss: 0.59996\n",
            "  Batch   220  of    607.    Elapsed: 0:01:51. Loss: 0.59691\n",
            "  Batch   225  of    607.    Elapsed: 0:01:54. Loss: 0.59215\n",
            "  Batch   230  of    607.    Elapsed: 0:01:56. Loss: 0.59047\n",
            "  Batch   235  of    607.    Elapsed: 0:01:59. Loss: 0.58614\n",
            "  Batch   240  of    607.    Elapsed: 0:02:01. Loss: 0.58312\n",
            "  Batch   245  of    607.    Elapsed: 0:02:04. Loss: 0.58786\n",
            "  Batch   250  of    607.    Elapsed: 0:02:06. Loss: 0.58682\n",
            "  Batch   255  of    607.    Elapsed: 0:02:09. Loss: 0.58897\n",
            "  Batch   260  of    607.    Elapsed: 0:02:11. Loss: 0.59057\n",
            "  Batch   265  of    607.    Elapsed: 0:02:14. Loss: 0.59155\n",
            "  Batch   270  of    607.    Elapsed: 0:02:16. Loss: 0.58928\n",
            "  Batch   275  of    607.    Elapsed: 0:02:19. Loss: 0.58894\n",
            "  Batch   280  of    607.    Elapsed: 0:02:21. Loss: 0.58675\n",
            "  Batch   285  of    607.    Elapsed: 0:02:24. Loss: 0.58333\n",
            "  Batch   290  of    607.    Elapsed: 0:02:26. Loss: 0.58326\n",
            "  Batch   295  of    607.    Elapsed: 0:02:29. Loss: 0.58096\n",
            "  Batch   300  of    607.    Elapsed: 0:02:31. Loss: 0.57896\n",
            "  Batch   305  of    607.    Elapsed: 0:02:34. Loss: 0.57666\n",
            "  Batch   310  of    607.    Elapsed: 0:02:36. Loss: 0.57731\n",
            "  Batch   315  of    607.    Elapsed: 0:02:39. Loss: 0.57772\n",
            "  Batch   320  of    607.    Elapsed: 0:02:41. Loss: 0.58114\n",
            "  Batch   325  of    607.    Elapsed: 0:02:44. Loss: 0.57881\n",
            "  Batch   330  of    607.    Elapsed: 0:02:47. Loss: 0.57753\n",
            "  Batch   335  of    607.    Elapsed: 0:02:49. Loss: 0.57709\n",
            "  Batch   340  of    607.    Elapsed: 0:02:52. Loss: 0.57526\n",
            "  Batch   345  of    607.    Elapsed: 0:02:54. Loss: 0.57421\n",
            "  Batch   350  of    607.    Elapsed: 0:02:57. Loss: 0.57200\n",
            "  Batch   355  of    607.    Elapsed: 0:02:59. Loss: 0.57192\n",
            "  Batch   360  of    607.    Elapsed: 0:03:02. Loss: 0.57364\n",
            "  Batch   365  of    607.    Elapsed: 0:03:05. Loss: 0.57695\n",
            "  Batch   370  of    607.    Elapsed: 0:03:07. Loss: 0.57525\n",
            "  Batch   375  of    607.    Elapsed: 0:03:10. Loss: 0.57244\n",
            "  Batch   380  of    607.    Elapsed: 0:03:12. Loss: 0.57258\n",
            "  Batch   385  of    607.    Elapsed: 0:03:15. Loss: 0.57157\n",
            "  Batch   390  of    607.    Elapsed: 0:03:18. Loss: 0.57179\n",
            "  Batch   395  of    607.    Elapsed: 0:03:20. Loss: 0.57228\n",
            "  Batch   400  of    607.    Elapsed: 0:03:23. Loss: 0.57144\n",
            "  Batch   405  of    607.    Elapsed: 0:03:25. Loss: 0.56784\n",
            "  Batch   410  of    607.    Elapsed: 0:03:28. Loss: 0.56718\n",
            "  Batch   415  of    607.    Elapsed: 0:03:31. Loss: 0.56563\n",
            "  Batch   420  of    607.    Elapsed: 0:03:33. Loss: 0.56451\n",
            "  Batch   425  of    607.    Elapsed: 0:03:36. Loss: 0.56340\n",
            "  Batch   430  of    607.    Elapsed: 0:03:38. Loss: 0.56152\n",
            "  Batch   435  of    607.    Elapsed: 0:03:41. Loss: 0.56079\n",
            "  Batch   440  of    607.    Elapsed: 0:03:44. Loss: 0.55834\n",
            "  Batch   445  of    607.    Elapsed: 0:03:46. Loss: 0.55789\n",
            "  Batch   450  of    607.    Elapsed: 0:03:49. Loss: 0.55645\n",
            "  Batch   455  of    607.    Elapsed: 0:03:51. Loss: 0.55771\n",
            "  Batch   460  of    607.    Elapsed: 0:03:54. Loss: 0.55548\n",
            "  Batch   465  of    607.    Elapsed: 0:03:56. Loss: 0.55431\n",
            "  Batch   470  of    607.    Elapsed: 0:03:59. Loss: 0.55397\n",
            "  Batch   475  of    607.    Elapsed: 0:04:02. Loss: 0.55261\n",
            "  Batch   480  of    607.    Elapsed: 0:04:04. Loss: 0.55054\n",
            "  Batch   485  of    607.    Elapsed: 0:04:07. Loss: 0.54936\n",
            "  Batch   490  of    607.    Elapsed: 0:04:09. Loss: 0.54815\n",
            "  Batch   495  of    607.    Elapsed: 0:04:12. Loss: 0.54919\n",
            "  Batch   500  of    607.    Elapsed: 0:04:14. Loss: 0.54791\n",
            "  Batch   505  of    607.    Elapsed: 0:04:17. Loss: 0.54742\n",
            "  Batch   510  of    607.    Elapsed: 0:04:20. Loss: 0.54723\n",
            "  Batch   515  of    607.    Elapsed: 0:04:22. Loss: 0.54519\n",
            "  Batch   520  of    607.    Elapsed: 0:04:25. Loss: 0.54320\n",
            "  Batch   525  of    607.    Elapsed: 0:04:27. Loss: 0.54341\n",
            "  Batch   530  of    607.    Elapsed: 0:04:30. Loss: 0.54335\n",
            "  Batch   535  of    607.    Elapsed: 0:04:32. Loss: 0.54480\n",
            "  Batch   540  of    607.    Elapsed: 0:04:35. Loss: 0.54285\n",
            "  Batch   545  of    607.    Elapsed: 0:04:38. Loss: 0.54149\n",
            "  Batch   550  of    607.    Elapsed: 0:04:40. Loss: 0.54082\n",
            "  Batch   555  of    607.    Elapsed: 0:04:43. Loss: 0.53934\n",
            "  Batch   560  of    607.    Elapsed: 0:04:45. Loss: 0.53834\n",
            "  Batch   565  of    607.    Elapsed: 0:04:48. Loss: 0.53652\n",
            "  Batch   570  of    607.    Elapsed: 0:04:50. Loss: 0.53623\n",
            "  Batch   575  of    607.    Elapsed: 0:04:53. Loss: 0.53457\n",
            "  Batch   580  of    607.    Elapsed: 0:04:56. Loss: 0.53351\n",
            "  Batch   585  of    607.    Elapsed: 0:04:58. Loss: 0.53260\n",
            "  Batch   590  of    607.    Elapsed: 0:05:01. Loss: 0.53217\n",
            "  Batch   595  of    607.    Elapsed: 0:05:03. Loss: 0.53228\n",
            "  Batch   600  of    607.    Elapsed: 0:05:06. Loss: 0.53009\n",
            "  Batch   605  of    607.    Elapsed: 0:05:08. Loss: 0.53029\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:05:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:26\n",
            "  Accuracy: 0.78\n",
            "  Accuracy: 0.80\n",
            "  Macro F1-score: 0.78\n",
            "  Macro F1-score: 0.79\n",
            "  Weighted F1-score: 0.78\n",
            "  Weighted F1-score: 0.79\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.82      0.79       619\n",
            "         1.0       0.80      0.75      0.77       594\n",
            "\n",
            "    accuracy                           0.78      1213\n",
            "   macro avg       0.78      0.78      0.78      1213\n",
            "weighted avg       0.78      0.78      0.78      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.67      0.76       594\n",
            "         1.0       0.74      0.92      0.82       619\n",
            "\n",
            "    accuracy                           0.80      1213\n",
            "   macro avg       0.82      0.79      0.79      1213\n",
            "weighted avg       0.81      0.80      0.79      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[505 114]\n",
            " [148 446]]\n",
            "[[399 195]\n",
            " [ 51 568]]\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:03. Loss: 0.50100\n",
            "  Batch    10  of    607.    Elapsed: 0:00:05. Loss: 0.49021\n",
            "  Batch    15  of    607.    Elapsed: 0:00:08. Loss: 0.45416\n",
            "  Batch    20  of    607.    Elapsed: 0:00:10. Loss: 0.45091\n",
            "  Batch    25  of    607.    Elapsed: 0:00:13. Loss: 0.46045\n",
            "  Batch    30  of    607.    Elapsed: 0:00:15. Loss: 0.43161\n",
            "  Batch    35  of    607.    Elapsed: 0:00:18. Loss: 0.41462\n",
            "  Batch    40  of    607.    Elapsed: 0:00:21. Loss: 0.42832\n",
            "  Batch    45  of    607.    Elapsed: 0:00:23. Loss: 0.43959\n",
            "  Batch    50  of    607.    Elapsed: 0:00:26. Loss: 0.44146\n",
            "  Batch    55  of    607.    Elapsed: 0:00:28. Loss: 0.44977\n",
            "  Batch    60  of    607.    Elapsed: 0:00:31. Loss: 0.43626\n",
            "  Batch    65  of    607.    Elapsed: 0:00:34. Loss: 0.44235\n",
            "  Batch    70  of    607.    Elapsed: 0:00:36. Loss: 0.43280\n",
            "  Batch    75  of    607.    Elapsed: 0:00:39. Loss: 0.44633\n",
            "  Batch    80  of    607.    Elapsed: 0:00:41. Loss: 0.44242\n",
            "  Batch    85  of    607.    Elapsed: 0:00:44. Loss: 0.43352\n",
            "  Batch    90  of    607.    Elapsed: 0:00:47. Loss: 0.42665\n",
            "  Batch    95  of    607.    Elapsed: 0:00:49. Loss: 0.41891\n",
            "  Batch   100  of    607.    Elapsed: 0:00:52. Loss: 0.41387\n",
            "  Batch   105  of    607.    Elapsed: 0:00:54. Loss: 0.42072\n",
            "  Batch   110  of    607.    Elapsed: 0:00:57. Loss: 0.41941\n",
            "  Batch   115  of    607.    Elapsed: 0:00:59. Loss: 0.41349\n",
            "  Batch   120  of    607.    Elapsed: 0:01:02. Loss: 0.41591\n",
            "  Batch   125  of    607.    Elapsed: 0:01:05. Loss: 0.40989\n",
            "  Batch   130  of    607.    Elapsed: 0:01:07. Loss: 0.41168\n",
            "  Batch   135  of    607.    Elapsed: 0:01:10. Loss: 0.41126\n",
            "  Batch   140  of    607.    Elapsed: 0:01:12. Loss: 0.41283\n",
            "  Batch   145  of    607.    Elapsed: 0:01:15. Loss: 0.41085\n",
            "  Batch   150  of    607.    Elapsed: 0:01:17. Loss: 0.41321\n",
            "  Batch   155  of    607.    Elapsed: 0:01:20. Loss: 0.41017\n",
            "  Batch   160  of    607.    Elapsed: 0:01:23. Loss: 0.40654\n",
            "  Batch   165  of    607.    Elapsed: 0:01:25. Loss: 0.40335\n",
            "  Batch   170  of    607.    Elapsed: 0:01:28. Loss: 0.40550\n",
            "  Batch   175  of    607.    Elapsed: 0:01:30. Loss: 0.41114\n",
            "  Batch   180  of    607.    Elapsed: 0:01:33. Loss: 0.40784\n",
            "  Batch   185  of    607.    Elapsed: 0:01:35. Loss: 0.40385\n",
            "  Batch   190  of    607.    Elapsed: 0:01:38. Loss: 0.40566\n",
            "  Batch   195  of    607.    Elapsed: 0:01:41. Loss: 0.40200\n",
            "  Batch   200  of    607.    Elapsed: 0:01:43. Loss: 0.39667\n",
            "  Batch   205  of    607.    Elapsed: 0:01:46. Loss: 0.39859\n",
            "  Batch   210  of    607.    Elapsed: 0:01:48. Loss: 0.40075\n",
            "  Batch   215  of    607.    Elapsed: 0:01:51. Loss: 0.40395\n",
            "  Batch   220  of    607.    Elapsed: 0:01:53. Loss: 0.40353\n",
            "  Batch   225  of    607.    Elapsed: 0:01:56. Loss: 0.40035\n",
            "  Batch   230  of    607.    Elapsed: 0:01:59. Loss: 0.40677\n",
            "  Batch   235  of    607.    Elapsed: 0:02:01. Loss: 0.41561\n",
            "  Batch   240  of    607.    Elapsed: 0:02:04. Loss: 0.41193\n",
            "  Batch   245  of    607.    Elapsed: 0:02:06. Loss: 0.40957\n",
            "  Batch   250  of    607.    Elapsed: 0:02:09. Loss: 0.41096\n",
            "  Batch   255  of    607.    Elapsed: 0:02:11. Loss: 0.40984\n",
            "  Batch   260  of    607.    Elapsed: 0:02:14. Loss: 0.41022\n",
            "  Batch   265  of    607.    Elapsed: 0:02:17. Loss: 0.41185\n",
            "  Batch   270  of    607.    Elapsed: 0:02:19. Loss: 0.41282\n",
            "  Batch   275  of    607.    Elapsed: 0:02:22. Loss: 0.41488\n",
            "  Batch   280  of    607.    Elapsed: 0:02:24. Loss: 0.41344\n",
            "  Batch   285  of    607.    Elapsed: 0:02:27. Loss: 0.41235\n",
            "  Batch   290  of    607.    Elapsed: 0:02:29. Loss: 0.41539\n",
            "  Batch   295  of    607.    Elapsed: 0:02:32. Loss: 0.41414\n",
            "  Batch   300  of    607.    Elapsed: 0:02:35. Loss: 0.41206\n",
            "  Batch   305  of    607.    Elapsed: 0:02:37. Loss: 0.41170\n",
            "  Batch   310  of    607.    Elapsed: 0:02:40. Loss: 0.40988\n",
            "  Batch   315  of    607.    Elapsed: 0:02:42. Loss: 0.40891\n",
            "  Batch   320  of    607.    Elapsed: 0:02:45. Loss: 0.40785\n",
            "  Batch   325  of    607.    Elapsed: 0:02:47. Loss: 0.40670\n",
            "  Batch   330  of    607.    Elapsed: 0:02:50. Loss: 0.40846\n",
            "  Batch   335  of    607.    Elapsed: 0:02:53. Loss: 0.41018\n",
            "  Batch   340  of    607.    Elapsed: 0:02:55. Loss: 0.40849\n",
            "  Batch   345  of    607.    Elapsed: 0:02:58. Loss: 0.40656\n",
            "  Batch   350  of    607.    Elapsed: 0:03:00. Loss: 0.40826\n",
            "  Batch   355  of    607.    Elapsed: 0:03:03. Loss: 0.40985\n",
            "  Batch   360  of    607.    Elapsed: 0:03:06. Loss: 0.41220\n",
            "  Batch   365  of    607.    Elapsed: 0:03:08. Loss: 0.41354\n",
            "  Batch   370  of    607.    Elapsed: 0:03:11. Loss: 0.41265\n",
            "  Batch   375  of    607.    Elapsed: 0:03:13. Loss: 0.41268\n",
            "  Batch   380  of    607.    Elapsed: 0:03:16. Loss: 0.41130\n",
            "  Batch   385  of    607.    Elapsed: 0:03:19. Loss: 0.41310\n",
            "  Batch   390  of    607.    Elapsed: 0:03:21. Loss: 0.41050\n",
            "  Batch   395  of    607.    Elapsed: 0:03:24. Loss: 0.41073\n",
            "  Batch   400  of    607.    Elapsed: 0:03:26. Loss: 0.41179\n",
            "  Batch   405  of    607.    Elapsed: 0:03:29. Loss: 0.41081\n",
            "  Batch   410  of    607.    Elapsed: 0:03:31. Loss: 0.40958\n",
            "  Batch   415  of    607.    Elapsed: 0:03:34. Loss: 0.40729\n",
            "  Batch   420  of    607.    Elapsed: 0:03:37. Loss: 0.40662\n",
            "  Batch   425  of    607.    Elapsed: 0:03:39. Loss: 0.40561\n",
            "  Batch   430  of    607.    Elapsed: 0:03:42. Loss: 0.40627\n",
            "  Batch   435  of    607.    Elapsed: 0:03:44. Loss: 0.40521\n",
            "  Batch   440  of    607.    Elapsed: 0:03:47. Loss: 0.40355\n",
            "  Batch   445  of    607.    Elapsed: 0:03:50. Loss: 0.40388\n",
            "  Batch   450  of    607.    Elapsed: 0:03:52. Loss: 0.40717\n",
            "  Batch   455  of    607.    Elapsed: 0:03:55. Loss: 0.40675\n",
            "  Batch   460  of    607.    Elapsed: 0:03:57. Loss: 0.40661\n",
            "  Batch   465  of    607.    Elapsed: 0:04:00. Loss: 0.40621\n",
            "  Batch   470  of    607.    Elapsed: 0:04:02. Loss: 0.40515\n",
            "  Batch   475  of    607.    Elapsed: 0:04:05. Loss: 0.40499\n",
            "  Batch   480  of    607.    Elapsed: 0:04:08. Loss: 0.40624\n",
            "  Batch   485  of    607.    Elapsed: 0:04:10. Loss: 0.40675\n",
            "  Batch   490  of    607.    Elapsed: 0:04:13. Loss: 0.40830\n",
            "  Batch   495  of    607.    Elapsed: 0:04:15. Loss: 0.40935\n",
            "  Batch   500  of    607.    Elapsed: 0:04:18. Loss: 0.40911\n",
            "  Batch   505  of    607.    Elapsed: 0:04:21. Loss: 0.40841\n",
            "  Batch   510  of    607.    Elapsed: 0:04:23. Loss: 0.40871\n",
            "  Batch   515  of    607.    Elapsed: 0:04:26. Loss: 0.40952\n",
            "  Batch   520  of    607.    Elapsed: 0:04:28. Loss: 0.40881\n",
            "  Batch   525  of    607.    Elapsed: 0:04:31. Loss: 0.40964\n",
            "  Batch   530  of    607.    Elapsed: 0:04:33. Loss: 0.40954\n",
            "  Batch   535  of    607.    Elapsed: 0:04:36. Loss: 0.40971\n",
            "  Batch   540  of    607.    Elapsed: 0:04:38. Loss: 0.41091\n",
            "  Batch   545  of    607.    Elapsed: 0:04:41. Loss: 0.40975\n",
            "  Batch   550  of    607.    Elapsed: 0:04:44. Loss: 0.40961\n",
            "  Batch   555  of    607.    Elapsed: 0:04:46. Loss: 0.41124\n",
            "  Batch   560  of    607.    Elapsed: 0:04:49. Loss: 0.41211\n",
            "  Batch   565  of    607.    Elapsed: 0:04:51. Loss: 0.41346\n",
            "  Batch   570  of    607.    Elapsed: 0:04:54. Loss: 0.41231\n",
            "  Batch   575  of    607.    Elapsed: 0:04:56. Loss: 0.41226\n",
            "  Batch   580  of    607.    Elapsed: 0:04:59. Loss: 0.41268\n",
            "  Batch   585  of    607.    Elapsed: 0:05:02. Loss: 0.41321\n",
            "  Batch   590  of    607.    Elapsed: 0:05:04. Loss: 0.41244\n",
            "  Batch   595  of    607.    Elapsed: 0:05:07. Loss: 0.41153\n",
            "  Batch   600  of    607.    Elapsed: 0:05:09. Loss: 0.41052\n",
            "  Batch   605  of    607.    Elapsed: 0:05:12. Loss: 0.41063\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epcoh took: 0:05:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:25\n",
            "  Accuracy: 0.83\n",
            "  Accuracy: 0.80\n",
            "  Macro F1-score: 0.83\n",
            "  Macro F1-score: 0.80\n",
            "  Weighted F1-score: 0.83\n",
            "  Weighted F1-score: 0.80\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.93      0.85       619\n",
            "         1.0       0.91      0.73      0.81       594\n",
            "\n",
            "    accuracy                           0.83      1213\n",
            "   macro avg       0.84      0.83      0.83      1213\n",
            "weighted avg       0.84      0.83      0.83      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.63      0.76       594\n",
            "         1.0       0.73      0.97      0.83       619\n",
            "\n",
            "    accuracy                           0.80      1213\n",
            "   macro avg       0.84      0.80      0.80      1213\n",
            "weighted avg       0.84      0.80      0.80      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[574  45]\n",
            " [163 431]]\n",
            "[[373 221]\n",
            " [ 17 602]]\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:03. Loss: 0.43675\n",
            "  Batch    10  of    607.    Elapsed: 0:00:05. Loss: 0.33085\n",
            "  Batch    15  of    607.    Elapsed: 0:00:08. Loss: 0.34387\n",
            "  Batch    20  of    607.    Elapsed: 0:00:10. Loss: 0.29672\n",
            "  Batch    25  of    607.    Elapsed: 0:00:13. Loss: 0.33461\n",
            "  Batch    30  of    607.    Elapsed: 0:00:15. Loss: 0.35890\n",
            "  Batch    35  of    607.    Elapsed: 0:00:18. Loss: 0.34300\n",
            "  Batch    40  of    607.    Elapsed: 0:00:21. Loss: 0.38129\n",
            "  Batch    45  of    607.    Elapsed: 0:00:23. Loss: 0.37077\n",
            "  Batch    50  of    607.    Elapsed: 0:00:26. Loss: 0.38578\n",
            "  Batch    55  of    607.    Elapsed: 0:00:28. Loss: 0.37743\n",
            "  Batch    60  of    607.    Elapsed: 0:00:31. Loss: 0.36968\n",
            "  Batch    65  of    607.    Elapsed: 0:00:34. Loss: 0.37103\n",
            "  Batch    70  of    607.    Elapsed: 0:00:36. Loss: 0.35901\n",
            "  Batch    75  of    607.    Elapsed: 0:00:39. Loss: 0.35188\n",
            "  Batch    80  of    607.    Elapsed: 0:00:41. Loss: 0.34406\n",
            "  Batch    85  of    607.    Elapsed: 0:00:44. Loss: 0.33682\n",
            "  Batch    90  of    607.    Elapsed: 0:00:47. Loss: 0.33756\n",
            "  Batch    95  of    607.    Elapsed: 0:00:49. Loss: 0.34222\n",
            "  Batch   100  of    607.    Elapsed: 0:00:52. Loss: 0.33941\n",
            "  Batch   105  of    607.    Elapsed: 0:00:54. Loss: 0.34251\n",
            "  Batch   110  of    607.    Elapsed: 0:00:57. Loss: 0.33523\n",
            "  Batch   115  of    607.    Elapsed: 0:00:59. Loss: 0.34668\n",
            "  Batch   120  of    607.    Elapsed: 0:01:02. Loss: 0.34826\n",
            "  Batch   125  of    607.    Elapsed: 0:01:05. Loss: 0.34007\n",
            "  Batch   130  of    607.    Elapsed: 0:01:07. Loss: 0.34762\n",
            "  Batch   135  of    607.    Elapsed: 0:01:10. Loss: 0.35242\n",
            "  Batch   140  of    607.    Elapsed: 0:01:12. Loss: 0.34845\n",
            "  Batch   145  of    607.    Elapsed: 0:01:15. Loss: 0.34791\n",
            "  Batch   150  of    607.    Elapsed: 0:01:17. Loss: 0.34628\n",
            "  Batch   155  of    607.    Elapsed: 0:01:20. Loss: 0.34231\n",
            "  Batch   160  of    607.    Elapsed: 0:01:23. Loss: 0.34280\n",
            "  Batch   165  of    607.    Elapsed: 0:01:25. Loss: 0.34041\n",
            "  Batch   170  of    607.    Elapsed: 0:01:28. Loss: 0.33508\n",
            "  Batch   175  of    607.    Elapsed: 0:01:30. Loss: 0.33480\n",
            "  Batch   180  of    607.    Elapsed: 0:01:33. Loss: 0.33057\n",
            "  Batch   185  of    607.    Elapsed: 0:01:35. Loss: 0.33556\n",
            "  Batch   190  of    607.    Elapsed: 0:01:38. Loss: 0.34364\n",
            "  Batch   195  of    607.    Elapsed: 0:01:41. Loss: 0.34359\n",
            "  Batch   200  of    607.    Elapsed: 0:01:43. Loss: 0.34797\n",
            "  Batch   205  of    607.    Elapsed: 0:01:46. Loss: 0.34484\n",
            "  Batch   210  of    607.    Elapsed: 0:01:48. Loss: 0.34186\n",
            "  Batch   215  of    607.    Elapsed: 0:01:51. Loss: 0.34066\n",
            "  Batch   220  of    607.    Elapsed: 0:01:53. Loss: 0.33864\n",
            "  Batch   225  of    607.    Elapsed: 0:01:56. Loss: 0.33911\n",
            "  Batch   230  of    607.    Elapsed: 0:01:59. Loss: 0.33853\n",
            "  Batch   235  of    607.    Elapsed: 0:02:01. Loss: 0.33829\n",
            "  Batch   240  of    607.    Elapsed: 0:02:04. Loss: 0.33540\n",
            "  Batch   245  of    607.    Elapsed: 0:02:06. Loss: 0.33490\n",
            "  Batch   250  of    607.    Elapsed: 0:02:09. Loss: 0.33479\n",
            "  Batch   255  of    607.    Elapsed: 0:02:12. Loss: 0.33292\n",
            "  Batch   260  of    607.    Elapsed: 0:02:14. Loss: 0.33027\n",
            "  Batch   265  of    607.    Elapsed: 0:02:17. Loss: 0.32756\n",
            "  Batch   270  of    607.    Elapsed: 0:02:19. Loss: 0.32539\n",
            "  Batch   275  of    607.    Elapsed: 0:02:22. Loss: 0.32468\n",
            "  Batch   280  of    607.    Elapsed: 0:02:24. Loss: 0.32896\n",
            "  Batch   285  of    607.    Elapsed: 0:02:27. Loss: 0.33195\n",
            "  Batch   290  of    607.    Elapsed: 0:02:30. Loss: 0.33153\n",
            "  Batch   295  of    607.    Elapsed: 0:02:32. Loss: 0.33209\n",
            "  Batch   300  of    607.    Elapsed: 0:02:35. Loss: 0.33136\n",
            "  Batch   305  of    607.    Elapsed: 0:02:38. Loss: 0.33274\n",
            "  Batch   310  of    607.    Elapsed: 0:02:40. Loss: 0.33230\n",
            "  Batch   315  of    607.    Elapsed: 0:02:43. Loss: 0.33445\n",
            "  Batch   320  of    607.    Elapsed: 0:02:45. Loss: 0.33533\n",
            "  Batch   325  of    607.    Elapsed: 0:02:48. Loss: 0.33253\n",
            "  Batch   330  of    607.    Elapsed: 0:02:50. Loss: 0.33417\n",
            "  Batch   335  of    607.    Elapsed: 0:02:53. Loss: 0.33252\n",
            "  Batch   340  of    607.    Elapsed: 0:02:56. Loss: 0.33103\n",
            "  Batch   345  of    607.    Elapsed: 0:02:58. Loss: 0.32748\n",
            "  Batch   350  of    607.    Elapsed: 0:03:01. Loss: 0.32576\n",
            "  Batch   355  of    607.    Elapsed: 0:03:03. Loss: 0.32708\n",
            "  Batch   360  of    607.    Elapsed: 0:03:06. Loss: 0.32672\n",
            "  Batch   365  of    607.    Elapsed: 0:03:09. Loss: 0.32574\n",
            "  Batch   370  of    607.    Elapsed: 0:03:11. Loss: 0.32647\n",
            "  Batch   375  of    607.    Elapsed: 0:03:14. Loss: 0.32685\n",
            "  Batch   380  of    607.    Elapsed: 0:03:16. Loss: 0.32990\n",
            "  Batch   385  of    607.    Elapsed: 0:03:19. Loss: 0.33072\n",
            "  Batch   390  of    607.    Elapsed: 0:03:21. Loss: 0.33252\n",
            "  Batch   395  of    607.    Elapsed: 0:03:24. Loss: 0.33240\n",
            "  Batch   400  of    607.    Elapsed: 0:03:27. Loss: 0.33038\n",
            "  Batch   405  of    607.    Elapsed: 0:03:29. Loss: 0.32873\n",
            "  Batch   410  of    607.    Elapsed: 0:03:32. Loss: 0.32913\n",
            "  Batch   415  of    607.    Elapsed: 0:03:34. Loss: 0.33129\n",
            "  Batch   420  of    607.    Elapsed: 0:03:37. Loss: 0.33157\n",
            "  Batch   425  of    607.    Elapsed: 0:03:40. Loss: 0.33052\n",
            "  Batch   430  of    607.    Elapsed: 0:03:42. Loss: 0.33046\n",
            "  Batch   435  of    607.    Elapsed: 0:03:45. Loss: 0.33011\n",
            "  Batch   440  of    607.    Elapsed: 0:03:47. Loss: 0.32845\n",
            "  Batch   445  of    607.    Elapsed: 0:03:50. Loss: 0.32948\n",
            "  Batch   450  of    607.    Elapsed: 0:03:52. Loss: 0.32935\n",
            "  Batch   455  of    607.    Elapsed: 0:03:55. Loss: 0.32865\n",
            "  Batch   460  of    607.    Elapsed: 0:03:58. Loss: 0.33049\n",
            "  Batch   465  of    607.    Elapsed: 0:04:00. Loss: 0.32913\n",
            "  Batch   470  of    607.    Elapsed: 0:04:03. Loss: 0.32843\n",
            "  Batch   475  of    607.    Elapsed: 0:04:05. Loss: 0.32761\n",
            "  Batch   480  of    607.    Elapsed: 0:04:08. Loss: 0.32566\n",
            "  Batch   485  of    607.    Elapsed: 0:04:10. Loss: 0.32570\n",
            "  Batch   490  of    607.    Elapsed: 0:04:13. Loss: 0.32481\n",
            "  Batch   495  of    607.    Elapsed: 0:04:16. Loss: 0.32368\n",
            "  Batch   500  of    607.    Elapsed: 0:04:18. Loss: 0.32372\n",
            "  Batch   505  of    607.    Elapsed: 0:04:21. Loss: 0.32294\n",
            "  Batch   510  of    607.    Elapsed: 0:04:23. Loss: 0.32234\n",
            "  Batch   515  of    607.    Elapsed: 0:04:26. Loss: 0.32209\n",
            "  Batch   520  of    607.    Elapsed: 0:04:28. Loss: 0.32364\n",
            "  Batch   525  of    607.    Elapsed: 0:04:31. Loss: 0.32304\n",
            "  Batch   530  of    607.    Elapsed: 0:04:34. Loss: 0.32191\n",
            "  Batch   535  of    607.    Elapsed: 0:04:36. Loss: 0.32282\n",
            "  Batch   540  of    607.    Elapsed: 0:04:39. Loss: 0.32409\n",
            "  Batch   545  of    607.    Elapsed: 0:04:41. Loss: 0.32418\n",
            "  Batch   550  of    607.    Elapsed: 0:04:44. Loss: 0.32315\n",
            "  Batch   555  of    607.    Elapsed: 0:04:47. Loss: 0.32204\n",
            "  Batch   560  of    607.    Elapsed: 0:04:49. Loss: 0.32332\n",
            "  Batch   565  of    607.    Elapsed: 0:04:52. Loss: 0.32488\n",
            "  Batch   570  of    607.    Elapsed: 0:04:54. Loss: 0.32376\n",
            "  Batch   575  of    607.    Elapsed: 0:04:57. Loss: 0.32280\n",
            "  Batch   580  of    607.    Elapsed: 0:05:00. Loss: 0.32131\n",
            "  Batch   585  of    607.    Elapsed: 0:05:02. Loss: 0.32106\n",
            "  Batch   590  of    607.    Elapsed: 0:05:05. Loss: 0.32268\n",
            "  Batch   595  of    607.    Elapsed: 0:05:07. Loss: 0.32309\n",
            "  Batch   600  of    607.    Elapsed: 0:05:10. Loss: 0.32167\n",
            "  Batch   605  of    607.    Elapsed: 0:05:12. Loss: 0.32267\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:05:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:26\n",
            "  Accuracy: 0.83\n",
            "  Accuracy: 0.84\n",
            "  Macro F1-score: 0.83\n",
            "  Macro F1-score: 0.83\n",
            "  Weighted F1-score: 0.83\n",
            "  Weighted F1-score: 0.83\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.86      0.84       619\n",
            "         1.0       0.84      0.80      0.82       594\n",
            "\n",
            "    accuracy                           0.83      1213\n",
            "   macro avg       0.83      0.83      0.83      1213\n",
            "weighted avg       0.83      0.83      0.83      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.76      0.82       594\n",
            "         1.0       0.80      0.91      0.85       619\n",
            "\n",
            "    accuracy                           0.84      1213\n",
            "   macro avg       0.84      0.83      0.83      1213\n",
            "weighted avg       0.84      0.84      0.83      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[531  88]\n",
            " [119 475]]\n",
            "[[450 144]\n",
            " [ 56 563]]\n"
          ]
        }
      ],
      "source": [
        "#TRAINING and VALIDATION\n",
        "epochs = 3  \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,2))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(2)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(2):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(2):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(2)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(2)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(2)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vTdZosUVLbA",
        "outputId": "b2d3f50b-4cd5-4db5-fb71-2056a9683b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHFf2BDeVOxv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt4ot72zOC6V"
      },
      "outputs": [],
      "source": [
        "def augment_text(text):\n",
        "    # Define the augmentation technique (in this case, synonym replacement)\n",
        "    aug = naw.SynonymAug()\n",
        "\n",
        "    # Augment the text using the specified technique\n",
        "    augmented_text = aug.augment(text)\n",
        "\n",
        "    return augmented_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H392PLq-Wqwl",
        "outputId": "5313b437-0a7b-416e-8011-02dfb4dcc636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wn==0.0.23\n",
            "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wn\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792928 sha256=dad3a88c0085550de5c25b2c6147b77c4c247477da311ce91645735e219a556b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/1a/7d/23a76ce45998af60e47466a694c237fa26023c5674b47672b2\n",
            "Successfully built wn\n",
            "Installing collected packages: wn\n",
            "Successfully installed wn-0.0.23\n"
          ]
        }
      ],
      "source": [
        "!pip install -U wn==0.0.23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puOxMBvXYZ9C",
        "outputId": "df03ddd7-20f3-40d2-88b4-b5ac36054231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pywsd==1.0.2\n",
            "  Downloading pywsd-1.0.2.tar.gz (8.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pywsd==1.0.2) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pywsd==1.0.2) (1.22.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd==1.0.2) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd==1.0.2) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd==1.0.2) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd==1.0.2) (4.65.0)\n",
            "Building wheels for collected packages: pywsd\n",
            "  Building wheel for pywsd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pywsd: filename=pywsd-1.0.2-py3-none-any.whl size=12122 sha256=d8734aee011b5aa4281177b0bed354f1d0a29ec43f806e0612eadc6c7a77ae91\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/4a/27/3c91e7c499b777b847997c8e15b4a4dd83c114b619f1e64987\n",
            "Successfully built pywsd\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install pywsd==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "NA1RAXy0aE5l",
        "outputId": "8a495186-9ac3-44ce-ec86-f531d94be9d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.2) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.2) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.2) (8.1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.6.2) (2022.10.31)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install nltk==3.6.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7g1SjlOWvGf",
        "outputId": "2aa449c3-7498-4e71-da15-eb59f5f95118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyiwn\n",
            "  Downloading pyiwn-0.0.5-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pyiwn) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pyiwn) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyiwn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyiwn) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pyiwn) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pyiwn) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pyiwn) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pyiwn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pyiwn) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pyiwn) (1.16.0)\n",
            "Installing collected packages: pyiwn\n",
            "Successfully installed pyiwn-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyiwn"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00e9b37e652e4d9aacf5fb4b1ecfd6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03d81f93558945d4abc0ca93fc5930fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068d5fcee1da4a688347a130c571f367": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077f3e2778f0424e9c3012d7baac43c1",
            "placeholder": "​",
            "style": "IPY_MODEL_2bba2e0d5a004b3d986cda12454516d6",
            "value": " 135M/135M [00:00&lt;00:00, 208MB/s]"
          }
        },
        "077f3e2778f0424e9c3012d7baac43c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0a9ee121384fb3878112f616d8962c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11357a9c05cb4783924f5985c92eadb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1282f503b52746ee908fbd7845d6aa17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e97d68ae3f47a09eb9a34923cb22e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc3e836518f4006a8870091b1526632",
            "max": 507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a034885c1eeb47b58553185437ca6b53",
            "value": 507
          }
        },
        "191ce1c7be50470086a23f92f46b73ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193924d3bd4c47fa91a55d56ff7fab90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3315773a47bb41c891c8b180ed9b8544",
            "max": 871891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9889eae758e64923b42d902d27a5c17c",
            "value": 871891
          }
        },
        "19b57232018f42e090dfe19ebb192d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "249d296b76c440869969482834331cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e68f2c1379324e359ebc0dee22c6ea11",
              "IPY_MODEL_193924d3bd4c47fa91a55d56ff7fab90",
              "IPY_MODEL_47f371020d6448c589afb4845df104c3"
            ],
            "layout": "IPY_MODEL_191ce1c7be50470086a23f92f46b73ce"
          }
        },
        "25fe45cfbdf54deca03643cac961c608": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29584c9137fa42878952b7411e9732b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6f4cd8e3134486b3dd8adbec1e10e7",
            "placeholder": "​",
            "style": "IPY_MODEL_2ea96408e7204b408378f56f9235e66f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "2bba2e0d5a004b3d986cda12454516d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ea96408e7204b408378f56f9235e66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3315773a47bb41c891c8b180ed9b8544": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368c321ad2ba4afdab7e13add5f3fdb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ce295db8e74c3cacf75f6d73468996": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83a809bc7c49487bbab7ede3e168ad58",
              "IPY_MODEL_d1cf6edcbe424bf687af12333526101b",
              "IPY_MODEL_068d5fcee1da4a688347a130c571f367"
            ],
            "layout": "IPY_MODEL_25fe45cfbdf54deca03643cac961c608"
          }
        },
        "47f371020d6448c589afb4845df104c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c632a6bc98a947e083b46c8a645691d1",
            "placeholder": "​",
            "style": "IPY_MODEL_8e5b28dae67846e48f0b6e2e54c1706f",
            "value": " 872k/872k [00:00&lt;00:00, 10.2MB/s]"
          }
        },
        "499986f2c57e469b98b1e73721788436": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29584c9137fa42878952b7411e9732b1",
              "IPY_MODEL_12e97d68ae3f47a09eb9a34923cb22e1",
              "IPY_MODEL_553e465cacd740c08c911d19ae879f71"
            ],
            "layout": "IPY_MODEL_831f4a268fb74033a00f2db882352b02"
          }
        },
        "4b90dbf447834a9a85098874fa977f96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553e465cacd740c08c911d19ae879f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b90dbf447834a9a85098874fa977f96",
            "placeholder": "​",
            "style": "IPY_MODEL_875e8610c00b4f14bf55bff3a2e1e2cb",
            "value": " 507/507 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "5d5e885f84cb4bdcaf946a78ff301e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919b5aaaec7540a2847e8901ed1c11ea",
            "placeholder": "​",
            "style": "IPY_MODEL_99c30588e33f470191d313fec5df002c",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "5e6f4cd8e3134486b3dd8adbec1e10e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f889a593dd4da896032fb04cbf2bda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c936be896b042e7a8429aaeec145819": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e47ef5247ea4bf0b3b7e3979cfe07bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da0a9fca9caf441db1ddc1726d647bdb",
              "IPY_MODEL_80e769fc7268492bacac65973b6da2a1",
              "IPY_MODEL_b4d8f146a73c44ac939ffbb7e03f376b"
            ],
            "layout": "IPY_MODEL_03d81f93558945d4abc0ca93fc5930fb"
          }
        },
        "6f0caa4d8ef74835bd47dd6068126262": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74dcf26698754e3491ce2e0ffa654a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d2013eb954041ddb37b8ab45c94258e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e769fc7268492bacac65973b6da2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eec95e027b79484394e599b1e7b1f616",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d84be24aaf8d46f18c79983e24555758",
            "value": 28
          }
        },
        "831f4a268fb74033a00f2db882352b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a809bc7c49487bbab7ede3e168ad58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1282f503b52746ee908fbd7845d6aa17",
            "placeholder": "​",
            "style": "IPY_MODEL_e00d35df68f04c5a912f7d08579eb0a9",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "845034af10004c35aac4a0d37060d795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa8d7626893e480a922ae490f5906e59",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e5dfbb6a5d447128851d7f27f7316c6",
            "value": 625
          }
        },
        "875e8610c00b4f14bf55bff3a2e1e2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c9808be8eca4e039a27de4667f8ede0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d5e885f84cb4bdcaf946a78ff301e44",
              "IPY_MODEL_845034af10004c35aac4a0d37060d795",
              "IPY_MODEL_caeb2a53a3df489ea750e557e9eda00c"
            ],
            "layout": "IPY_MODEL_bc92bff70db4469ab6981f312ca88e88"
          }
        },
        "8cef96f98efe473398ef2f9a8ce88b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4a2240694a64b9dbe18ba4572e202f3",
              "IPY_MODEL_cac3c81be3034b34bdec137646037d0b",
              "IPY_MODEL_fb8584c4f6ea4880b0ff57c5f7dee08d"
            ],
            "layout": "IPY_MODEL_6c936be896b042e7a8429aaeec145819"
          }
        },
        "8e5b28dae67846e48f0b6e2e54c1706f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e5dfbb6a5d447128851d7f27f7316c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "919b5aaaec7540a2847e8901ed1c11ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e36e79d5154d72b7eb7f0b76ff2929": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "969e6276a8194f1997fd555ecdb4a680": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9889eae758e64923b42d902d27a5c17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99c30588e33f470191d313fec5df002c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a034885c1eeb47b58553185437ca6b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0851ddc59674b3ba89922e7878bda67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a61d3df12ba0499cbc560f8ccbd3fc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8608d08f44c468198d78f4711060c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d8f146a73c44ac939ffbb7e03f376b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68f889a593dd4da896032fb04cbf2bda",
            "placeholder": "​",
            "style": "IPY_MODEL_a0851ddc59674b3ba89922e7878bda67",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.89kB/s]"
          }
        },
        "bc92bff70db4469ab6981f312ca88e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c38211566c994e7e951008ab4b7dba59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c632a6bc98a947e083b46c8a645691d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cac3c81be3034b34bdec137646037d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11357a9c05cb4783924f5985c92eadb6",
            "max": 5646064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a61d3df12ba0499cbc560f8ccbd3fc8a",
            "value": 5646064
          }
        },
        "caeb2a53a3df489ea750e557e9eda00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8608d08f44c468198d78f4711060c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_19b57232018f42e090dfe19ebb192d7c",
            "value": " 625/625 [00:00&lt;00:00, 47.7kB/s]"
          }
        },
        "cbc3e836518f4006a8870091b1526632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16a92eb09924bf0b56bb67b5796fc56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1cf6edcbe424bf687af12333526101b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_969e6276a8194f1997fd555ecdb4a680",
            "max": 134982446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74dcf26698754e3491ce2e0ffa654a01",
            "value": 134982446
          }
        },
        "d84be24aaf8d46f18c79983e24555758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da0a9fca9caf441db1ddc1726d647bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2013eb954041ddb37b8ab45c94258e",
            "placeholder": "​",
            "style": "IPY_MODEL_00e9b37e652e4d9aacf5fb4b1ecfd6d6",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e00d35df68f04c5a912f7d08579eb0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a2240694a64b9dbe18ba4572e202f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c38211566c994e7e951008ab4b7dba59",
            "placeholder": "​",
            "style": "IPY_MODEL_0f0a9ee121384fb3878112f616d8962c",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "e68f2c1379324e359ebc0dee22c6ea11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16a92eb09924bf0b56bb67b5796fc56",
            "placeholder": "​",
            "style": "IPY_MODEL_6f0caa4d8ef74835bd47dd6068126262",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "eec95e027b79484394e599b1e7b1f616": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa8d7626893e480a922ae490f5906e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb8584c4f6ea4880b0ff57c5f7dee08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_368c321ad2ba4afdab7e13add5f3fdb1",
            "placeholder": "​",
            "style": "IPY_MODEL_91e36e79d5154d72b7eb7f0b76ff2929",
            "value": " 5.65M/5.65M [00:00&lt;00:00, 50.0MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}