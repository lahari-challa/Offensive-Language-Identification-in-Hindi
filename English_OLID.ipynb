{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d83cf0d3bdd47428f05309972d65800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2aa99986e66b419aada16cf0d3b8fe1a",
              "IPY_MODEL_fcaf6f32e1e340e482a738b79a1d2090",
              "IPY_MODEL_9b8f4b878c2b4a04b075db7911e9381e"
            ],
            "layout": "IPY_MODEL_34a10f3b7e374ecaa9de2ab6e0b7ad1c"
          }
        },
        "2aa99986e66b419aada16cf0d3b8fe1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf23d957f45a41ef8b601532a5d437df",
            "placeholder": "​",
            "style": "IPY_MODEL_69984623488849c3bb358c7e72f13577",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "fcaf6f32e1e340e482a738b79a1d2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc7cd4536f84d73b04f8483ba88acd2",
            "max": 134982446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da74baab668a49bbbb1aa20d97c80edd",
            "value": 134982446
          }
        },
        "9b8f4b878c2b4a04b075db7911e9381e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94615dfb06c41b9beffecb4eaccf170",
            "placeholder": "​",
            "style": "IPY_MODEL_8651afb57c4f4838bb6ae6f9d4563630",
            "value": " 135M/135M [00:23&lt;00:00, 10.4MB/s]"
          }
        },
        "34a10f3b7e374ecaa9de2ab6e0b7ad1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf23d957f45a41ef8b601532a5d437df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69984623488849c3bb358c7e72f13577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc7cd4536f84d73b04f8483ba88acd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da74baab668a49bbbb1aa20d97c80edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c94615dfb06c41b9beffecb4eaccf170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8651afb57c4f4838bb6ae6f9d4563630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a26b0806f44c229bc5c258a594ad24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c439dfbc6d347c797fa7adca24a069e",
              "IPY_MODEL_eff7c684d83b4244b0484a11da4b6bd9",
              "IPY_MODEL_3ecbc28f922c4075b25aedaebb80da9a"
            ],
            "layout": "IPY_MODEL_6c8e635ddcae437ba1de064e83e03f1a"
          }
        },
        "5c439dfbc6d347c797fa7adca24a069e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941313d64f0c498db2b1f9eeef072d47",
            "placeholder": "​",
            "style": "IPY_MODEL_f6ce81f0b94d48ee967b9ff829c49991",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "eff7c684d83b4244b0484a11da4b6bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_460c8e4124f6478fa2c1367b6e223f6e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e20d5cf2df4b4b898472f6f66656f458",
            "value": 231508
          }
        },
        "3ecbc28f922c4075b25aedaebb80da9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56bf15672f924f15bd44280259315f45",
            "placeholder": "​",
            "style": "IPY_MODEL_3919a7b51b5545baa487380b4e71dada",
            "value": " 232k/232k [00:00&lt;00:00, 1.09MB/s]"
          }
        },
        "6c8e635ddcae437ba1de064e83e03f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941313d64f0c498db2b1f9eeef072d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ce81f0b94d48ee967b9ff829c49991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "460c8e4124f6478fa2c1367b6e223f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e20d5cf2df4b4b898472f6f66656f458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56bf15672f924f15bd44280259315f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3919a7b51b5545baa487380b4e71dada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f0a9bc795de410a8f6b0ecba4976de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e57737ace7ab41e58c4cd0de182bafdc",
              "IPY_MODEL_ce738540b95c4b83a3fc778e1d7770e5",
              "IPY_MODEL_5a14c49adb9640e483a6f8ce329375c6"
            ],
            "layout": "IPY_MODEL_08fd565fc93e4f3c91d70a4cf61969cf"
          }
        },
        "e57737ace7ab41e58c4cd0de182bafdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88d9d37523324f2cac1e24e633d48013",
            "placeholder": "​",
            "style": "IPY_MODEL_3a8cb4f0ad8b4341ba64e4908fe75606",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "ce738540b95c4b83a3fc778e1d7770e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1d2233ab05405f9bb1c53e0665dea4",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47ea14d885e147d3ac3bdde866a5d3aa",
            "value": 28
          }
        },
        "5a14c49adb9640e483a6f8ce329375c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f93abd57a8844c35b6722295b16e9365",
            "placeholder": "​",
            "style": "IPY_MODEL_a2ccf19fae7e4e5f94739d728c17665f",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.59kB/s]"
          }
        },
        "08fd565fc93e4f3c91d70a4cf61969cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88d9d37523324f2cac1e24e633d48013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8cb4f0ad8b4341ba64e4908fe75606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1d2233ab05405f9bb1c53e0665dea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ea14d885e147d3ac3bdde866a5d3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f93abd57a8844c35b6722295b16e9365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ccf19fae7e4e5f94739d728c17665f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cc9b9a993c84a9e82cd238f5e9f6cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dc300ab3b744ac0b5b69907b2e518ef",
              "IPY_MODEL_8a5abe67b79a4826a42ef51b9d501cf8",
              "IPY_MODEL_0ce05cdf1ab94b77a22f59cc907cc4ad"
            ],
            "layout": "IPY_MODEL_9ab9f8e79b9d426da6fa9c31047904eb"
          }
        },
        "6dc300ab3b744ac0b5b69907b2e518ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75fd1cc8a565411c90befd36e50b2319",
            "placeholder": "​",
            "style": "IPY_MODEL_98bfa9a9119c420cbfdc840029667630",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "8a5abe67b79a4826a42ef51b9d501cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8766b9a5ba1400093db09ed6113a832",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_498bad5849de4d9a980e17b27ffa175c",
            "value": 570
          }
        },
        "0ce05cdf1ab94b77a22f59cc907cc4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_517a1532998a407c80d942f21fd17dbf",
            "placeholder": "​",
            "style": "IPY_MODEL_fd52adc93346404b9fd07bb8dd80f5ac",
            "value": " 570/570 [00:00&lt;00:00, 42.1kB/s]"
          }
        },
        "9ab9f8e79b9d426da6fa9c31047904eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75fd1cc8a565411c90befd36e50b2319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98bfa9a9119c420cbfdc840029667630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8766b9a5ba1400093db09ed6113a832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498bad5849de4d9a980e17b27ffa175c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "517a1532998a407c80d942f21fd17dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd52adc93346404b9fd07bb8dd80f5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSNlgx4sXMV",
        "outputId": "b192e262-df78-4e1e-c5c1-452c9c818707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGxJpxGKYXOb",
        "outputId": "c14d849f-f8f0-4926-ce2a-f0296f3bc6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "cD-1mnduYUFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    # torch.cuda.set_device(0)\n",
        "    device = torch.device('cuda')\n",
        "    print('Using GPU: ', torch.cuda.current_device())\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtZRXoOYkxf",
        "outputId": "ef3d3239-1bd4-4247-bcc6-d99bd2aec09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU:  0\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import string"
      ],
      "metadata": {
        "id": "u8yfMc-6jjmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('drive/MyDrive/olid-training-v1.0.tsv', delimiter='\\t', encoding='utf-8')\n",
        "\n",
        "train_tweets = train_data[['tweet']] #Extract tweets\n",
        "train_task_a_labels= train_data[['subtask_a']] #Extract subtsak_a labels\n",
        "train_task_b_labels= train_data[['subtask_b']] #Extract subtsak_b labels\n",
        "train_task_c_labels= train_data[['subtask_c']] #Extract subtsak_c labels\n",
        "\n",
        "train_task_a_labels.columns.values[0] = 'class_a' #Rename class attribute\n",
        "train_task_b_labels.columns.values[0] = 'class_b' #Rename class attribute\n",
        "train_task_c_labels.columns.values[0] = 'class_c' #Rename class attribute\n",
        "\n",
        "print(train_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlRHV4mamGBx",
        "outputId": "7bc262b4-1e80-457f-a9c9-27b5d3954b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id                                              tweet subtask_a  \\\n",
            "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
            "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
            "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
            "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
            "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
            "...      ...                                                ...       ...   \n",
            "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
            "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
            "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
            "13238  27429                                        @USER Pussy       OFF   \n",
            "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
            "\n",
            "      subtask_b subtask_c  \n",
            "0           UNT       NaN  \n",
            "1           TIN       IND  \n",
            "2           NaN       NaN  \n",
            "3           UNT       NaN  \n",
            "4           NaN       NaN  \n",
            "...         ...       ...  \n",
            "13235       TIN       IND  \n",
            "13236       NaN       NaN  \n",
            "13237       TIN       OTH  \n",
            "13238       UNT       NaN  \n",
            "13239       NaN       NaN  \n",
            "\n",
            "[13240 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweets(df):\n",
        "    \n",
        "    punctuations = string.punctuation\n",
        "    \n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('@USER', '') #Remove mentions (@USER)\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('URL', '') #Remove URLs\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('&amp', 'and') #Replace ampersand (&) with and\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('&lt','') #Remove &lt\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('&gt','') #Remove &gt\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n",
        "\n",
        "    #Remove punctuations\n",
        "    for punctuation in punctuations:\n",
        "        df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
        "\n",
        "    df.loc[:, 'tweet'] = df.astype(str).apply(\n",
        "        lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii')\n",
        "    ) #Remove emojis\n",
        "    df.loc[:, 'tweet'] = df.tweet.str.strip()"
      ],
      "metadata": {
        "id": "JKjZ77b8mShE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tweets(train_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-LXjVQymT5X",
        "outputId": "ad628fc2-eb0c-4f8f-ed49-21bc74420987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-bb7f92e91289>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('@USER', '') #Remove mentions (@USER)\n",
            "<ipython-input-7-bb7f92e91289>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('URL', '') #Remove URLs\n",
            "<ipython-input-7-bb7f92e91289>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('&amp', 'and') #Replace ampersand (&) with and\n",
            "<ipython-input-7-bb7f92e91289>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('&lt','') #Remove &lt\n",
            "<ipython-input-7-bb7f92e91289>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('&gt','') #Remove &gt\n",
            "<ipython-input-7-bb7f92e91289>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n",
            "<ipython-input-7-bb7f92e91289>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
            "<ipython-input-7-bb7f92e91289>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.astype(str).apply(\n",
            "<ipython-input-7-bb7f92e91289>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.strip()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_task_a_data = train_tweets.join(train_task_a_labels)\n",
        "\n",
        "train_task_b_data = train_tweets.join(train_task_b_labels)\n",
        "train_task_b_data = train_task_b_data.dropna() #Drop records with missing values\n",
        "\n",
        "train_task_c_data = train_tweets.join(train_task_c_labels)\n",
        "train_task_c_data = train_task_c_data.dropna() #Drop records with missing values\n",
        "\n",
        "#Apply quotes to cleaned tweets\n",
        "train_task_a_data.update(train_task_a_data[['tweet']].applymap('\\'{}\\''.format))\n",
        "train_task_b_data.update(train_task_b_data[['tweet']].applymap('\\'{}\\''.format))\n",
        "train_task_c_data.update(train_task_c_data[['tweet']].applymap('\\'{}\\''.format))"
      ],
      "metadata": {
        "id": "lWJi-gJUmWpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_task_a_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9An9xObmdDp",
        "outputId": "6d5627e9-f35c-4903-8d0c-92c8705177a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   tweet class_a\n",
            "0      'She should ask a few native Americans what th...     OFF\n",
            "1                      'Go home youre drunk  MAGA Trump'     OFF\n",
            "2      'Amazon is investigating Chinese employees who...     NOT\n",
            "3      'Someone shouldveTaken this piece of shit to a...     OFF\n",
            "4      'Obama wanted liberals and illegals to move in...     NOT\n",
            "...                                                  ...     ...\n",
            "13235  'Sometimes I get strong vibes from people and ...     OFF\n",
            "13236  'Benidorm   Creamfields   Maga    Not too shab...     NOT\n",
            "13237  'And why report this garbage  We dont give a c...     OFF\n",
            "13238                                            'Pussy'     OFF\n",
            "13239  'Spanishrevenge vs justice HumanRights and Fre...     NOT\n",
            "\n",
            "[13240 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet_a=pd.read_csv('drive/MyDrive/testset-levela.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_tweet_b=pd.read_csv('drive/MyDrive/testset-levelb.tsv', delimiter='\\t', encoding='utf-8')\n",
        "test_tweet_c=pd.read_csv('drive/MyDrive/testset-levelc.tsv', delimiter='\\t', encoding='utf-8')\n",
        "\n",
        "#Read tweet labels\n",
        "test_label_a=pd.read_csv('drive/MyDrive/labels-levela.csv', encoding='utf-8', \n",
        "                         index_col=False, names=['id', 'class_a'])\n",
        "test_label_b=pd.read_csv('drive/MyDrive/labels-levelb.csv', encoding='utf-8', \n",
        "                         index_col=False, names=['id', 'class_b'])\n",
        "test_label_c=pd.read_csv('drive/MyDrive/labels-levelc.csv', encoding='utf-8', \n",
        "                         index_col=False, names=['id', 'class_c'])\n",
        "\n",
        "#Merge tweets with labels by id\n",
        "test_tweet_a = test_tweet_a.merge(test_label_a, on='id')\n",
        "test_tweet_b = test_tweet_b.merge(test_label_b, on='id')\n",
        "test_tweet_c = test_tweet_c.merge(test_label_c, on='id')\n",
        "\n",
        "# #Drop id column\n",
        "# test_tweet_a = test_tweet_a.drop(columns='id')\n",
        "# test_tweet_b = test_tweet_b.drop(columns='id')\n",
        "# test_tweet_c = test_tweet_c.drop(columns='id')\n",
        "\n",
        "#Clean tweets in test sets\n",
        "clean_tweets(test_tweet_a)\n",
        "clean_tweets(test_tweet_b)\n",
        "clean_tweets(test_tweet_c)\n",
        "\n",
        "#Apply quotes to cleaned tweets\n",
        "test_tweet_a.update(test_tweet_a[['tweet']].applymap('\\'{}\\''.format))\n",
        "test_tweet_b.update(test_tweet_b[['tweet']].applymap('\\'{}\\''.format))\n",
        "test_tweet_c.update(test_tweet_c[['tweet']].applymap('\\'{}\\''.format))\n",
        "print(test_tweet_a.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Tayx8zmfTa",
        "outputId": "2893ae35-2ed3-4325-9b45-95537b492fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                              tweet class_a\n",
            "0  15923  'WhoIsQ WheresTheServer DumpNike DECLASFISA De...     OFF\n",
            "1  27014  'ConstitutionDay is revered by Conservatives h...     NOT\n",
            "2  30530  'FOXNews NRA MAGA POTUS TRUMP ndAmendment RNC ...     NOT\n",
            "3  13876  'Watching Boomer getting the news that she is ...     NOT\n",
            "4  60133  'NoPasaran Unity demo to oppose the farright i...     OFF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-bb7f92e91289>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n",
            "<ipython-input-7-bb7f92e91289>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_tweet_a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik4Csffdmt_K",
        "outputId": "55be4ee7-0b1f-48cb-b402-9aad62a7f9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                              tweet class_a\n",
            "0    15923  'WhoIsQ WheresTheServer DumpNike DECLASFISA De...     OFF\n",
            "1    27014  'ConstitutionDay is revered by Conservatives h...     NOT\n",
            "2    30530  'FOXNews NRA MAGA POTUS TRUMP ndAmendment RNC ...     NOT\n",
            "3    13876  'Watching Boomer getting the news that she is ...     NOT\n",
            "4    60133  'NoPasaran Unity demo to oppose the farright i...     OFF\n",
            "..     ...                                                ...     ...\n",
            "855  73439  'DespicableDems lie again about rifles Dem Dis...     OFF\n",
            "856  25657  'MeetTheSpeakers   will present in our event O...     NOT\n",
            "857  67018  'people just unfollowed me for talking about m...     OFF\n",
            "858  50665  'WednesdayWisdom Antifa calls the right fascis...     NOT\n",
            "859  24583            'Kavanaugh typical liberals  Democrats'     NOT\n",
            "\n",
            "[860 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_task_a_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dMk9szvOWr1x",
        "outputId": "7f4fd67b-44b1-4056-e212-8e8c562dfbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tweet class_a\n",
              "0      'She should ask a few native Americans what th...     OFF\n",
              "1                      'Go home youre drunk  MAGA Trump'     OFF\n",
              "2      'Amazon is investigating Chinese employees who...     NOT\n",
              "3      'Someone shouldveTaken this piece of shit to a...     OFF\n",
              "4      'Obama wanted liberals and illegals to move in...     NOT\n",
              "...                                                  ...     ...\n",
              "13235  'Sometimes I get strong vibes from people and ...     OFF\n",
              "13236  'Benidorm   Creamfields   Maga    Not too shab...     NOT\n",
              "13237  'And why report this garbage  We dont give a c...     OFF\n",
              "13238                                            'Pussy'     OFF\n",
              "13239  'Spanishrevenge vs justice HumanRights and Fre...     NOT\n",
              "\n",
              "[13240 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20f71c71-de6a-44e8-b96e-115cb5cb5d13\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class_a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'She should ask a few native Americans what th...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Go home youre drunk  MAGA Trump'</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Amazon is investigating Chinese employees who...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Someone shouldveTaken this piece of shit to a...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'Obama wanted liberals and illegals to move in...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13235</th>\n",
              "      <td>'Sometimes I get strong vibes from people and ...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13236</th>\n",
              "      <td>'Benidorm   Creamfields   Maga    Not too shab...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13237</th>\n",
              "      <td>'And why report this garbage  We dont give a c...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13238</th>\n",
              "      <td>'Pussy'</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13239</th>\n",
              "      <td>'Spanishrevenge vs justice HumanRights and Fre...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13240 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20f71c71-de6a-44e8-b96e-115cb5cb5d13')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20f71c71-de6a-44e8-b96e-115cb5cb5d13 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20f71c71-de6a-44e8-b96e-115cb5cb5d13');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_task_a_data.assign(Offensive=0)\n",
        "train_df = df.assign(NotOffensive=0)\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lalQBixmYOXQ",
        "outputId": "3ac2d156-1cb5-47f2-bac6-8f23f70f11d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tweet class_a  Offensive  \\\n",
              "0      'She should ask a few native Americans what th...     OFF          0   \n",
              "1                      'Go home youre drunk  MAGA Trump'     OFF          0   \n",
              "2      'Amazon is investigating Chinese employees who...     NOT          0   \n",
              "3      'Someone shouldveTaken this piece of shit to a...     OFF          0   \n",
              "4      'Obama wanted liberals and illegals to move in...     NOT          0   \n",
              "...                                                  ...     ...        ...   \n",
              "13235  'Sometimes I get strong vibes from people and ...     OFF          0   \n",
              "13236  'Benidorm   Creamfields   Maga    Not too shab...     NOT          0   \n",
              "13237  'And why report this garbage  We dont give a c...     OFF          0   \n",
              "13238                                            'Pussy'     OFF          0   \n",
              "13239  'Spanishrevenge vs justice HumanRights and Fre...     NOT          0   \n",
              "\n",
              "       NotOffensive  \n",
              "0                 0  \n",
              "1                 0  \n",
              "2                 0  \n",
              "3                 0  \n",
              "4                 0  \n",
              "...             ...  \n",
              "13235             0  \n",
              "13236             0  \n",
              "13237             0  \n",
              "13238             0  \n",
              "13239             0  \n",
              "\n",
              "[13240 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0acb8fb0-3f01-4f9b-8935-d69b6f597b2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class_a</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'She should ask a few native Americans what th...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Go home youre drunk  MAGA Trump'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Amazon is investigating Chinese employees who...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Someone shouldveTaken this piece of shit to a...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'Obama wanted liberals and illegals to move in...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13235</th>\n",
              "      <td>'Sometimes I get strong vibes from people and ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13236</th>\n",
              "      <td>'Benidorm   Creamfields   Maga    Not too shab...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13237</th>\n",
              "      <td>'And why report this garbage  We dont give a c...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13238</th>\n",
              "      <td>'Pussy'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13239</th>\n",
              "      <td>'Spanishrevenge vs justice HumanRights and Fre...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13240 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0acb8fb0-3f01-4f9b-8935-d69b6f597b2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0acb8fb0-3f01-4f9b-8935-d69b6f597b2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0acb8fb0-3f01-4f9b-8935-d69b6f597b2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index in train_df.index:\n",
        "    k = train_df['class_a'][index]\n",
        "    if k == 'OFF':\n",
        "        train_df['Offensive'][index] = 1\n",
        "        train_df['NotOffensive'][index] = 0\n",
        "    else:\n",
        "        train_df['Offensive'][index] = 0\n",
        "        train_df['NotOffensive'][index] = 1\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "hla8yx0sXWMn",
        "outputId": "43462ef1-7acc-4999-d282-58d2333707f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-87d85e887622>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 1\n",
            "<ipython-input-15-87d85e887622>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 0\n",
            "<ipython-input-15-87d85e887622>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 0\n",
            "<ipython-input-15-87d85e887622>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tweet class_a  Offensive  \\\n",
              "0      'She should ask a few native Americans what th...     OFF          1   \n",
              "1                      'Go home youre drunk  MAGA Trump'     OFF          1   \n",
              "2      'Amazon is investigating Chinese employees who...     NOT          0   \n",
              "3      'Someone shouldveTaken this piece of shit to a...     OFF          1   \n",
              "4      'Obama wanted liberals and illegals to move in...     NOT          0   \n",
              "...                                                  ...     ...        ...   \n",
              "13235  'Sometimes I get strong vibes from people and ...     OFF          1   \n",
              "13236  'Benidorm   Creamfields   Maga    Not too shab...     NOT          0   \n",
              "13237  'And why report this garbage  We dont give a c...     OFF          1   \n",
              "13238                                            'Pussy'     OFF          1   \n",
              "13239  'Spanishrevenge vs justice HumanRights and Fre...     NOT          0   \n",
              "\n",
              "       NotOffensive  \n",
              "0                 0  \n",
              "1                 0  \n",
              "2                 1  \n",
              "3                 0  \n",
              "4                 1  \n",
              "...             ...  \n",
              "13235             0  \n",
              "13236             1  \n",
              "13237             0  \n",
              "13238             0  \n",
              "13239             1  \n",
              "\n",
              "[13240 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74faf28b-9097-4893-b774-c75a3335f5c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class_a</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'She should ask a few native Americans what th...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Go home youre drunk  MAGA Trump'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Amazon is investigating Chinese employees who...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Someone shouldveTaken this piece of shit to a...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'Obama wanted liberals and illegals to move in...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13235</th>\n",
              "      <td>'Sometimes I get strong vibes from people and ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13236</th>\n",
              "      <td>'Benidorm   Creamfields   Maga    Not too shab...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13237</th>\n",
              "      <td>'And why report this garbage  We dont give a c...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13238</th>\n",
              "      <td>'Pussy'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13239</th>\n",
              "      <td>'Spanishrevenge vs justice HumanRights and Fre...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13240 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74faf28b-9097-4893-b774-c75a3335f5c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74faf28b-9097-4893-b774-c75a3335f5c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74faf28b-9097-4893-b774-c75a3335f5c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PofN6-5adH8H",
        "outputId": "bd5d1ed6-10e0-4fa2-812e-81cdfdfa2a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tweet class_a  Offensive  \\\n",
              "0      'She should ask a few native Americans what th...     OFF          1   \n",
              "1                      'Go home youre drunk  MAGA Trump'     OFF          1   \n",
              "2      'Amazon is investigating Chinese employees who...     NOT          0   \n",
              "3      'Someone shouldveTaken this piece of shit to a...     OFF          1   \n",
              "4      'Obama wanted liberals and illegals to move in...     NOT          0   \n",
              "...                                                  ...     ...        ...   \n",
              "13235  'Sometimes I get strong vibes from people and ...     OFF          1   \n",
              "13236  'Benidorm   Creamfields   Maga    Not too shab...     NOT          0   \n",
              "13237  'And why report this garbage  We dont give a c...     OFF          1   \n",
              "13238                                            'Pussy'     OFF          1   \n",
              "13239  'Spanishrevenge vs justice HumanRights and Fre...     NOT          0   \n",
              "\n",
              "       NotOffensive  \n",
              "0                 0  \n",
              "1                 0  \n",
              "2                 1  \n",
              "3                 0  \n",
              "4                 1  \n",
              "...             ...  \n",
              "13235             0  \n",
              "13236             1  \n",
              "13237             0  \n",
              "13238             0  \n",
              "13239             1  \n",
              "\n",
              "[13240 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b25b24c-9ee6-45c9-bfe3-b7d6172f0006\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>class_a</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'She should ask a few native Americans what th...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Go home youre drunk  MAGA Trump'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'Amazon is investigating Chinese employees who...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Someone shouldveTaken this piece of shit to a...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'Obama wanted liberals and illegals to move in...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13235</th>\n",
              "      <td>'Sometimes I get strong vibes from people and ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13236</th>\n",
              "      <td>'Benidorm   Creamfields   Maga    Not too shab...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13237</th>\n",
              "      <td>'And why report this garbage  We dont give a c...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13238</th>\n",
              "      <td>'Pussy'</td>\n",
              "      <td>OFF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13239</th>\n",
              "      <td>'Spanishrevenge vs justice HumanRights and Fre...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13240 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b25b24c-9ee6-45c9-bfe3-b7d6172f0006')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b25b24c-9ee6-45c9-bfe3-b7d6172f0006 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b25b24c-9ee6-45c9-bfe3-b7d6172f0006');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45tr0jnLXu-g",
        "outputId": "23ed8632-5908-4892-b0b4-ad18d8bce9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tweet', 'class_a', 'Offensive', 'NotOffensive'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = train_df.columns[2:]\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, train_df[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "PojbJedcXyET",
        "outputId": "14a66d92-e87d-4a8f-c98b-eff11ff55b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       category  number of comments\n",
              "0     Offensive                4400\n",
              "1  NotOffensive                8840"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4922794-b31f-4c78-a977-a540297449cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>number of comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>4400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotOffensive</td>\n",
              "      <td>8840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4922794-b31f-4c78-a977-a540297449cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4922794-b31f-4c78-a977-a540297449cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4922794-b31f-4c78-a977-a540297449cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_list = categories"
      ],
      "metadata": {
        "id": "Zl2U8x5zYE82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "K9wRuSMauRXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KW4AauZvFEE",
        "outputId": "31308f29-c5a2-4b25-d701-6565cdad47cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVOHwM9wu4zS",
        "outputId": "63d09439-388e-4303-f5e1-595a47987085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "nCoDXu2ouvt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"
      ],
      "metadata": {
        "id": "OB1YDb4sYJXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ],
      "metadata": {
        "id": "qGSpcK_7YzO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = tokenizeWithBert(\"Hello world\")"
      ],
      "metadata": {
        "id": "AE9K3rm9Y3mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhsnp_DRY6Ox",
        "outputId": "a2415ca7-9284-4fb8-c98b-cb6f7de32ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 2088,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'New Delhi is the capital of India'  #\"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSppu3s8Y-kw",
        "outputId": "fccf3520-3be4-42e5-baf0-2ea581db092a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'new', 'delhi', 'is', 'the', 'capital', 'of', 'india', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodingsCapital1 = tokenizeWithBert(\"New Delhi is the capital of India\")"
      ],
      "metadata": {
        "id": "-BdXNrMVZCrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodingsCapital1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoti5huGZEGL",
        "outputId": "b7f7a3e6-7643-488b-be25-6efe8f2e6dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2047, 6768, 2003, 1996, 3007, 1997, 2634,  102,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['tweet'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "8H7fIK_TZHJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2qKNQlKZdEu",
        "outputId": "a42ae528-784f-468a-f62a-5be9aba3d0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Input:  torch.Size([13240, 300])\n",
            "Text Input Attention:  torch.Size([13240, 300])\n",
            "Labels:  torch.Size([13240, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBL5rE8eZp86",
        "outputId": "3a756563-3474-406b-b6d9-96837b53b753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Js8uJ71eZt02",
        "outputId": "41a2f4b1-de51-4987-945f-5f82b4ba784e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.25.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bertmodel = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
        "        self.ffn1 = nn.Linear(768, hidden_dim)\n",
        "        self.dp1 = nn.Dropout()\n",
        "        self.ffn2 = nn.Linear(hidden_dim, num_labels)\n",
        "        \n",
        "    def forward(self, in_T, in_T_attn_masks):\n",
        "        outputs = self.bertmodel(in_T, in_T_attn_masks)\n",
        "        x = torch.mean(outputs.last_hidden_state, dim=1) \n",
        "        x = F.relu(self.ffn1(x))\n",
        "        x = self.dp1(x)\n",
        "        x = torch.sigmoid(self.ffn2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "R0vvtLMqZzJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiClassClassifier(100, 2).to(device) # 100 hidden dimension, 2 lables\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8) # Adam with weight decay\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "3d83cf0d3bdd47428f05309972d65800",
            "2aa99986e66b419aada16cf0d3b8fe1a",
            "fcaf6f32e1e340e482a738b79a1d2090",
            "9b8f4b878c2b4a04b075db7911e9381e",
            "34a10f3b7e374ecaa9de2ab6e0b7ad1c",
            "cf23d957f45a41ef8b601532a5d437df",
            "69984623488849c3bb358c7e72f13577",
            "7dc7cd4536f84d73b04f8483ba88acd2",
            "da74baab668a49bbbb1aa20d97c80edd",
            "c94615dfb06c41b9beffecb4eaccf170",
            "8651afb57c4f4838bb6ae6f9d4563630"
          ]
        },
        "id": "7IRZuOJTZ21_",
        "outputId": "9d2d4e39-24f6-45e4-da24-b7905f7946d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d83cf0d3bdd47428f05309972d65800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'sop_classifier.classifier.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#format time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "0U43eLFsaAJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING and VALIDATION\n",
        "epochs = 3   #5, reduced to one epoch as it is taking lot of time\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,2))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(2)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(2):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(2):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(2)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(2)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(2)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcKMmyImaDaq",
        "outputId": "ff7524e0-de33-4791-df23-88443bb15a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  1,324.    Elapsed: 0:00:03. Loss: 0.68169\n",
            "  Batch    10  of  1,324.    Elapsed: 0:00:04. Loss: 0.68726\n",
            "  Batch    15  of  1,324.    Elapsed: 0:00:06. Loss: 0.68930\n",
            "  Batch    20  of  1,324.    Elapsed: 0:00:08. Loss: 0.68156\n",
            "  Batch    25  of  1,324.    Elapsed: 0:00:10. Loss: 0.66997\n",
            "  Batch    30  of  1,324.    Elapsed: 0:00:12. Loss: 0.66491\n",
            "  Batch    35  of  1,324.    Elapsed: 0:00:13. Loss: 0.66935\n",
            "  Batch    40  of  1,324.    Elapsed: 0:00:15. Loss: 0.66525\n",
            "  Batch    45  of  1,324.    Elapsed: 0:00:17. Loss: 0.66014\n",
            "  Batch    50  of  1,324.    Elapsed: 0:00:19. Loss: 0.66351\n",
            "  Batch    55  of  1,324.    Elapsed: 0:00:21. Loss: 0.65660\n",
            "  Batch    60  of  1,324.    Elapsed: 0:00:23. Loss: 0.65563\n",
            "  Batch    65  of  1,324.    Elapsed: 0:00:24. Loss: 0.65423\n",
            "  Batch    70  of  1,324.    Elapsed: 0:00:26. Loss: 0.65225\n",
            "  Batch    75  of  1,324.    Elapsed: 0:00:28. Loss: 0.65228\n",
            "  Batch    80  of  1,324.    Elapsed: 0:00:30. Loss: 0.65225\n",
            "  Batch    85  of  1,324.    Elapsed: 0:00:32. Loss: 0.65516\n",
            "  Batch    90  of  1,324.    Elapsed: 0:00:34. Loss: 0.65453\n",
            "  Batch    95  of  1,324.    Elapsed: 0:00:35. Loss: 0.65149\n",
            "  Batch   100  of  1,324.    Elapsed: 0:00:37. Loss: 0.65263\n",
            "  Batch   105  of  1,324.    Elapsed: 0:00:39. Loss: 0.64972\n",
            "  Batch   110  of  1,324.    Elapsed: 0:00:41. Loss: 0.64918\n",
            "  Batch   115  of  1,324.    Elapsed: 0:00:43. Loss: 0.64777\n",
            "  Batch   120  of  1,324.    Elapsed: 0:00:45. Loss: 0.64457\n",
            "  Batch   125  of  1,324.    Elapsed: 0:00:47. Loss: 0.64803\n",
            "  Batch   130  of  1,324.    Elapsed: 0:00:49. Loss: 0.64740\n",
            "  Batch   135  of  1,324.    Elapsed: 0:00:50. Loss: 0.64940\n",
            "  Batch   140  of  1,324.    Elapsed: 0:00:52. Loss: 0.65165\n",
            "  Batch   145  of  1,324.    Elapsed: 0:00:54. Loss: 0.65158\n",
            "  Batch   150  of  1,324.    Elapsed: 0:00:56. Loss: 0.65042\n",
            "  Batch   155  of  1,324.    Elapsed: 0:00:58. Loss: 0.64958\n",
            "  Batch   160  of  1,324.    Elapsed: 0:01:00. Loss: 0.64919\n",
            "  Batch   165  of  1,324.    Elapsed: 0:01:02. Loss: 0.64930\n",
            "  Batch   170  of  1,324.    Elapsed: 0:01:04. Loss: 0.64955\n",
            "  Batch   175  of  1,324.    Elapsed: 0:01:06. Loss: 0.65344\n",
            "  Batch   180  of  1,324.    Elapsed: 0:01:08. Loss: 0.65520\n",
            "  Batch   185  of  1,324.    Elapsed: 0:01:10. Loss: 0.65567\n",
            "  Batch   190  of  1,324.    Elapsed: 0:01:11. Loss: 0.65567\n",
            "  Batch   195  of  1,324.    Elapsed: 0:01:13. Loss: 0.65485\n",
            "  Batch   200  of  1,324.    Elapsed: 0:01:15. Loss: 0.65420\n",
            "  Batch   205  of  1,324.    Elapsed: 0:01:17. Loss: 0.65376\n",
            "  Batch   210  of  1,324.    Elapsed: 0:01:19. Loss: 0.65325\n",
            "  Batch   215  of  1,324.    Elapsed: 0:01:21. Loss: 0.65359\n",
            "  Batch   220  of  1,324.    Elapsed: 0:01:23. Loss: 0.65226\n",
            "  Batch   225  of  1,324.    Elapsed: 0:01:25. Loss: 0.65106\n",
            "  Batch   230  of  1,324.    Elapsed: 0:01:27. Loss: 0.64797\n",
            "  Batch   235  of  1,324.    Elapsed: 0:01:29. Loss: 0.64801\n",
            "  Batch   240  of  1,324.    Elapsed: 0:01:31. Loss: 0.64799\n",
            "  Batch   245  of  1,324.    Elapsed: 0:01:33. Loss: 0.64627\n",
            "  Batch   250  of  1,324.    Elapsed: 0:01:35. Loss: 0.64528\n",
            "  Batch   255  of  1,324.    Elapsed: 0:01:37. Loss: 0.64348\n",
            "  Batch   260  of  1,324.    Elapsed: 0:01:39. Loss: 0.64103\n",
            "  Batch   265  of  1,324.    Elapsed: 0:01:41. Loss: 0.63939\n",
            "  Batch   270  of  1,324.    Elapsed: 0:01:43. Loss: 0.64049\n",
            "  Batch   275  of  1,324.    Elapsed: 0:01:44. Loss: 0.64166\n",
            "  Batch   280  of  1,324.    Elapsed: 0:01:46. Loss: 0.64015\n",
            "  Batch   285  of  1,324.    Elapsed: 0:01:48. Loss: 0.63796\n",
            "  Batch   290  of  1,324.    Elapsed: 0:01:50. Loss: 0.63762\n",
            "  Batch   295  of  1,324.    Elapsed: 0:01:52. Loss: 0.63648\n",
            "  Batch   300  of  1,324.    Elapsed: 0:01:54. Loss: 0.63622\n",
            "  Batch   305  of  1,324.    Elapsed: 0:01:56. Loss: 0.63619\n",
            "  Batch   310  of  1,324.    Elapsed: 0:01:58. Loss: 0.63657\n",
            "  Batch   315  of  1,324.    Elapsed: 0:02:00. Loss: 0.63552\n",
            "  Batch   320  of  1,324.    Elapsed: 0:02:02. Loss: 0.63414\n",
            "  Batch   325  of  1,324.    Elapsed: 0:02:04. Loss: 0.63414\n",
            "  Batch   330  of  1,324.    Elapsed: 0:02:06. Loss: 0.63466\n",
            "  Batch   335  of  1,324.    Elapsed: 0:02:08. Loss: 0.63392\n",
            "  Batch   340  of  1,324.    Elapsed: 0:02:10. Loss: 0.63457\n",
            "  Batch   345  of  1,324.    Elapsed: 0:02:12. Loss: 0.63381\n",
            "  Batch   350  of  1,324.    Elapsed: 0:02:14. Loss: 0.63299\n",
            "  Batch   355  of  1,324.    Elapsed: 0:02:16. Loss: 0.63150\n",
            "  Batch   360  of  1,324.    Elapsed: 0:02:18. Loss: 0.63247\n",
            "  Batch   365  of  1,324.    Elapsed: 0:02:20. Loss: 0.63216\n",
            "  Batch   370  of  1,324.    Elapsed: 0:02:22. Loss: 0.62994\n",
            "  Batch   375  of  1,324.    Elapsed: 0:02:24. Loss: 0.62818\n",
            "  Batch   380  of  1,324.    Elapsed: 0:02:26. Loss: 0.62831\n",
            "  Batch   385  of  1,324.    Elapsed: 0:02:28. Loss: 0.62741\n",
            "  Batch   390  of  1,324.    Elapsed: 0:02:30. Loss: 0.62712\n",
            "  Batch   395  of  1,324.    Elapsed: 0:02:32. Loss: 0.62563\n",
            "  Batch   400  of  1,324.    Elapsed: 0:02:34. Loss: 0.62501\n",
            "  Batch   405  of  1,324.    Elapsed: 0:02:36. Loss: 0.62516\n",
            "  Batch   410  of  1,324.    Elapsed: 0:02:38. Loss: 0.62544\n",
            "  Batch   415  of  1,324.    Elapsed: 0:02:40. Loss: 0.62364\n",
            "  Batch   420  of  1,324.    Elapsed: 0:02:42. Loss: 0.62218\n",
            "  Batch   425  of  1,324.    Elapsed: 0:02:44. Loss: 0.62052\n",
            "  Batch   430  of  1,324.    Elapsed: 0:02:46. Loss: 0.61990\n",
            "  Batch   435  of  1,324.    Elapsed: 0:02:48. Loss: 0.61888\n",
            "  Batch   440  of  1,324.    Elapsed: 0:02:50. Loss: 0.62000\n",
            "  Batch   445  of  1,324.    Elapsed: 0:02:52. Loss: 0.61987\n",
            "  Batch   450  of  1,324.    Elapsed: 0:02:54. Loss: 0.61935\n",
            "  Batch   455  of  1,324.    Elapsed: 0:02:56. Loss: 0.61898\n",
            "  Batch   460  of  1,324.    Elapsed: 0:02:58. Loss: 0.61967\n",
            "  Batch   465  of  1,324.    Elapsed: 0:03:00. Loss: 0.61929\n",
            "  Batch   470  of  1,324.    Elapsed: 0:03:02. Loss: 0.61950\n",
            "  Batch   475  of  1,324.    Elapsed: 0:03:04. Loss: 0.61966\n",
            "  Batch   480  of  1,324.    Elapsed: 0:03:06. Loss: 0.61996\n",
            "  Batch   485  of  1,324.    Elapsed: 0:03:08. Loss: 0.61887\n",
            "  Batch   490  of  1,324.    Elapsed: 0:03:10. Loss: 0.61797\n",
            "  Batch   495  of  1,324.    Elapsed: 0:03:12. Loss: 0.61796\n",
            "  Batch   500  of  1,324.    Elapsed: 0:03:14. Loss: 0.61661\n",
            "  Batch   505  of  1,324.    Elapsed: 0:03:16. Loss: 0.61587\n",
            "  Batch   510  of  1,324.    Elapsed: 0:03:18. Loss: 0.61610\n",
            "  Batch   515  of  1,324.    Elapsed: 0:03:20. Loss: 0.61789\n",
            "  Batch   520  of  1,324.    Elapsed: 0:03:22. Loss: 0.61678\n",
            "  Batch   525  of  1,324.    Elapsed: 0:03:24. Loss: 0.61515\n",
            "  Batch   530  of  1,324.    Elapsed: 0:03:26. Loss: 0.61468\n",
            "  Batch   535  of  1,324.    Elapsed: 0:03:28. Loss: 0.61325\n",
            "  Batch   540  of  1,324.    Elapsed: 0:03:30. Loss: 0.61363\n",
            "  Batch   545  of  1,324.    Elapsed: 0:03:32. Loss: 0.61295\n",
            "  Batch   550  of  1,324.    Elapsed: 0:03:34. Loss: 0.61205\n",
            "  Batch   555  of  1,324.    Elapsed: 0:03:36. Loss: 0.61153\n",
            "  Batch   560  of  1,324.    Elapsed: 0:03:38. Loss: 0.61118\n",
            "  Batch   565  of  1,324.    Elapsed: 0:03:40. Loss: 0.61101\n",
            "  Batch   570  of  1,324.    Elapsed: 0:03:42. Loss: 0.61039\n",
            "  Batch   575  of  1,324.    Elapsed: 0:03:44. Loss: 0.61128\n",
            "  Batch   580  of  1,324.    Elapsed: 0:03:46. Loss: 0.61142\n",
            "  Batch   585  of  1,324.    Elapsed: 0:03:48. Loss: 0.61101\n",
            "  Batch   590  of  1,324.    Elapsed: 0:03:50. Loss: 0.60998\n",
            "  Batch   595  of  1,324.    Elapsed: 0:03:53. Loss: 0.60927\n",
            "  Batch   600  of  1,324.    Elapsed: 0:03:55. Loss: 0.60915\n",
            "  Batch   605  of  1,324.    Elapsed: 0:03:57. Loss: 0.60824\n",
            "  Batch   610  of  1,324.    Elapsed: 0:03:59. Loss: 0.60872\n",
            "  Batch   615  of  1,324.    Elapsed: 0:04:01. Loss: 0.60854\n",
            "  Batch   620  of  1,324.    Elapsed: 0:04:03. Loss: 0.60885\n",
            "  Batch   625  of  1,324.    Elapsed: 0:04:05. Loss: 0.60871\n",
            "  Batch   630  of  1,324.    Elapsed: 0:04:07. Loss: 0.60791\n",
            "  Batch   635  of  1,324.    Elapsed: 0:04:09. Loss: 0.60840\n",
            "  Batch   640  of  1,324.    Elapsed: 0:04:11. Loss: 0.60786\n",
            "  Batch   645  of  1,324.    Elapsed: 0:04:13. Loss: 0.60746\n",
            "  Batch   650  of  1,324.    Elapsed: 0:04:15. Loss: 0.60873\n",
            "  Batch   655  of  1,324.    Elapsed: 0:04:17. Loss: 0.60792\n",
            "  Batch   660  of  1,324.    Elapsed: 0:04:19. Loss: 0.60682\n",
            "  Batch   665  of  1,324.    Elapsed: 0:04:21. Loss: 0.60524\n",
            "  Batch   670  of  1,324.    Elapsed: 0:04:23. Loss: 0.60411\n",
            "  Batch   675  of  1,324.    Elapsed: 0:04:25. Loss: 0.60340\n",
            "  Batch   680  of  1,324.    Elapsed: 0:04:27. Loss: 0.60229\n",
            "  Batch   685  of  1,324.    Elapsed: 0:04:29. Loss: 0.60298\n",
            "  Batch   690  of  1,324.    Elapsed: 0:04:31. Loss: 0.60241\n",
            "  Batch   695  of  1,324.    Elapsed: 0:04:33. Loss: 0.60133\n",
            "  Batch   700  of  1,324.    Elapsed: 0:04:35. Loss: 0.60128\n",
            "  Batch   705  of  1,324.    Elapsed: 0:04:37. Loss: 0.60033\n",
            "  Batch   710  of  1,324.    Elapsed: 0:04:39. Loss: 0.59988\n",
            "  Batch   715  of  1,324.    Elapsed: 0:04:41. Loss: 0.59965\n",
            "  Batch   720  of  1,324.    Elapsed: 0:04:43. Loss: 0.60037\n",
            "  Batch   725  of  1,324.    Elapsed: 0:04:45. Loss: 0.59992\n",
            "  Batch   730  of  1,324.    Elapsed: 0:04:48. Loss: 0.59921\n",
            "  Batch   735  of  1,324.    Elapsed: 0:04:50. Loss: 0.59962\n",
            "  Batch   740  of  1,324.    Elapsed: 0:04:52. Loss: 0.59907\n",
            "  Batch   745  of  1,324.    Elapsed: 0:04:54. Loss: 0.59941\n",
            "  Batch   750  of  1,324.    Elapsed: 0:04:56. Loss: 0.59903\n",
            "  Batch   755  of  1,324.    Elapsed: 0:04:58. Loss: 0.59878\n",
            "  Batch   760  of  1,324.    Elapsed: 0:05:00. Loss: 0.59782\n",
            "  Batch   765  of  1,324.    Elapsed: 0:05:02. Loss: 0.59821\n",
            "  Batch   770  of  1,324.    Elapsed: 0:05:04. Loss: 0.59833\n",
            "  Batch   775  of  1,324.    Elapsed: 0:05:06. Loss: 0.59744\n",
            "  Batch   780  of  1,324.    Elapsed: 0:05:08. Loss: 0.59718\n",
            "  Batch   785  of  1,324.    Elapsed: 0:05:10. Loss: 0.59710\n",
            "  Batch   790  of  1,324.    Elapsed: 0:05:12. Loss: 0.59654\n",
            "  Batch   795  of  1,324.    Elapsed: 0:05:14. Loss: 0.59576\n",
            "  Batch   800  of  1,324.    Elapsed: 0:05:16. Loss: 0.59633\n",
            "  Batch   805  of  1,324.    Elapsed: 0:05:18. Loss: 0.59572\n",
            "  Batch   810  of  1,324.    Elapsed: 0:05:20. Loss: 0.59580\n",
            "  Batch   815  of  1,324.    Elapsed: 0:05:22. Loss: 0.59547\n",
            "  Batch   820  of  1,324.    Elapsed: 0:05:24. Loss: 0.59496\n",
            "  Batch   825  of  1,324.    Elapsed: 0:05:26. Loss: 0.59448\n",
            "  Batch   830  of  1,324.    Elapsed: 0:05:29. Loss: 0.59395\n",
            "  Batch   835  of  1,324.    Elapsed: 0:05:31. Loss: 0.59453\n",
            "  Batch   840  of  1,324.    Elapsed: 0:05:33. Loss: 0.59518\n",
            "  Batch   845  of  1,324.    Elapsed: 0:05:35. Loss: 0.59526\n",
            "  Batch   850  of  1,324.    Elapsed: 0:05:37. Loss: 0.59505\n",
            "  Batch   855  of  1,324.    Elapsed: 0:05:39. Loss: 0.59514\n",
            "  Batch   860  of  1,324.    Elapsed: 0:05:41. Loss: 0.59466\n",
            "  Batch   865  of  1,324.    Elapsed: 0:05:43. Loss: 0.59421\n",
            "  Batch   870  of  1,324.    Elapsed: 0:05:45. Loss: 0.59366\n",
            "  Batch   875  of  1,324.    Elapsed: 0:05:47. Loss: 0.59381\n",
            "  Batch   880  of  1,324.    Elapsed: 0:05:49. Loss: 0.59333\n",
            "  Batch   885  of  1,324.    Elapsed: 0:05:51. Loss: 0.59293\n",
            "  Batch   890  of  1,324.    Elapsed: 0:05:53. Loss: 0.59222\n",
            "  Batch   895  of  1,324.    Elapsed: 0:05:55. Loss: 0.59190\n",
            "  Batch   900  of  1,324.    Elapsed: 0:05:57. Loss: 0.59172\n",
            "  Batch   905  of  1,324.    Elapsed: 0:05:59. Loss: 0.59203\n",
            "  Batch   910  of  1,324.    Elapsed: 0:06:01. Loss: 0.59141\n",
            "  Batch   915  of  1,324.    Elapsed: 0:06:04. Loss: 0.59113\n",
            "  Batch   920  of  1,324.    Elapsed: 0:06:06. Loss: 0.59056\n",
            "  Batch   925  of  1,324.    Elapsed: 0:06:08. Loss: 0.59024\n",
            "  Batch   930  of  1,324.    Elapsed: 0:06:10. Loss: 0.59069\n",
            "  Batch   935  of  1,324.    Elapsed: 0:06:12. Loss: 0.59021\n",
            "  Batch   940  of  1,324.    Elapsed: 0:06:14. Loss: 0.59010\n",
            "  Batch   945  of  1,324.    Elapsed: 0:06:16. Loss: 0.59007\n",
            "  Batch   950  of  1,324.    Elapsed: 0:06:18. Loss: 0.58955\n",
            "  Batch   955  of  1,324.    Elapsed: 0:06:20. Loss: 0.58906\n",
            "  Batch   960  of  1,324.    Elapsed: 0:06:22. Loss: 0.58891\n",
            "  Batch   965  of  1,324.    Elapsed: 0:06:24. Loss: 0.58756\n",
            "  Batch   970  of  1,324.    Elapsed: 0:06:26. Loss: 0.58781\n",
            "  Batch   975  of  1,324.    Elapsed: 0:06:29. Loss: 0.58790\n",
            "  Batch   980  of  1,324.    Elapsed: 0:06:31. Loss: 0.58761\n",
            "  Batch   985  of  1,324.    Elapsed: 0:06:33. Loss: 0.58747\n",
            "  Batch   990  of  1,324.    Elapsed: 0:06:35. Loss: 0.58728\n",
            "  Batch   995  of  1,324.    Elapsed: 0:06:37. Loss: 0.58670\n",
            "  Batch 1,000  of  1,324.    Elapsed: 0:06:39. Loss: 0.58705\n",
            "  Batch 1,005  of  1,324.    Elapsed: 0:06:41. Loss: 0.58699\n",
            "  Batch 1,010  of  1,324.    Elapsed: 0:06:43. Loss: 0.58714\n",
            "  Batch 1,015  of  1,324.    Elapsed: 0:06:45. Loss: 0.58664\n",
            "  Batch 1,020  of  1,324.    Elapsed: 0:06:47. Loss: 0.58642\n",
            "  Batch 1,025  of  1,324.    Elapsed: 0:06:49. Loss: 0.58594\n",
            "  Batch 1,030  of  1,324.    Elapsed: 0:06:51. Loss: 0.58586\n",
            "  Batch 1,035  of  1,324.    Elapsed: 0:06:53. Loss: 0.58563\n",
            "  Batch 1,040  of  1,324.    Elapsed: 0:06:55. Loss: 0.58532\n",
            "  Batch 1,045  of  1,324.    Elapsed: 0:06:58. Loss: 0.58503\n",
            "  Batch 1,050  of  1,324.    Elapsed: 0:07:00. Loss: 0.58528\n",
            "  Batch 1,055  of  1,324.    Elapsed: 0:07:02. Loss: 0.58538\n",
            "  Batch 1,060  of  1,324.    Elapsed: 0:07:04. Loss: 0.58536\n",
            "  Batch 1,065  of  1,324.    Elapsed: 0:07:06. Loss: 0.58534\n",
            "  Batch 1,070  of  1,324.    Elapsed: 0:07:08. Loss: 0.58500\n",
            "  Batch 1,075  of  1,324.    Elapsed: 0:07:10. Loss: 0.58462\n",
            "  Batch 1,080  of  1,324.    Elapsed: 0:07:12. Loss: 0.58369\n",
            "  Batch 1,085  of  1,324.    Elapsed: 0:07:14. Loss: 0.58317\n",
            "  Batch 1,090  of  1,324.    Elapsed: 0:07:16. Loss: 0.58391\n",
            "  Batch 1,095  of  1,324.    Elapsed: 0:07:18. Loss: 0.58375\n",
            "  Batch 1,100  of  1,324.    Elapsed: 0:07:20. Loss: 0.58432\n",
            "  Batch 1,105  of  1,324.    Elapsed: 0:07:22. Loss: 0.58459\n",
            "  Batch 1,110  of  1,324.    Elapsed: 0:07:24. Loss: 0.58492\n",
            "  Batch 1,115  of  1,324.    Elapsed: 0:07:26. Loss: 0.58441\n",
            "  Batch 1,120  of  1,324.    Elapsed: 0:07:28. Loss: 0.58435\n",
            "  Batch 1,125  of  1,324.    Elapsed: 0:07:31. Loss: 0.58377\n",
            "  Batch 1,130  of  1,324.    Elapsed: 0:07:33. Loss: 0.58339\n",
            "  Batch 1,135  of  1,324.    Elapsed: 0:07:35. Loss: 0.58309\n",
            "  Batch 1,140  of  1,324.    Elapsed: 0:07:37. Loss: 0.58276\n",
            "  Batch 1,145  of  1,324.    Elapsed: 0:07:39. Loss: 0.58271\n",
            "  Batch 1,150  of  1,324.    Elapsed: 0:07:41. Loss: 0.58349\n",
            "  Batch 1,155  of  1,324.    Elapsed: 0:07:43. Loss: 0.58324\n",
            "  Batch 1,160  of  1,324.    Elapsed: 0:07:45. Loss: 0.58274\n",
            "  Batch 1,165  of  1,324.    Elapsed: 0:07:47. Loss: 0.58272\n",
            "  Batch 1,170  of  1,324.    Elapsed: 0:07:49. Loss: 0.58230\n",
            "  Batch 1,175  of  1,324.    Elapsed: 0:07:51. Loss: 0.58202\n",
            "  Batch 1,180  of  1,324.    Elapsed: 0:07:53. Loss: 0.58191\n",
            "  Batch 1,185  of  1,324.    Elapsed: 0:07:55. Loss: 0.58108\n",
            "  Batch 1,190  of  1,324.    Elapsed: 0:07:57. Loss: 0.58091\n",
            "  Batch 1,195  of  1,324.    Elapsed: 0:08:00. Loss: 0.58038\n",
            "  Batch 1,200  of  1,324.    Elapsed: 0:08:02. Loss: 0.58067\n",
            "  Batch 1,205  of  1,324.    Elapsed: 0:08:04. Loss: 0.58069\n",
            "  Batch 1,210  of  1,324.    Elapsed: 0:08:06. Loss: 0.58052\n",
            "  Batch 1,215  of  1,324.    Elapsed: 0:08:08. Loss: 0.58026\n",
            "  Batch 1,220  of  1,324.    Elapsed: 0:08:10. Loss: 0.58000\n",
            "  Batch 1,225  of  1,324.    Elapsed: 0:08:12. Loss: 0.57960\n",
            "  Batch 1,230  of  1,324.    Elapsed: 0:08:14. Loss: 0.57915\n",
            "  Batch 1,235  of  1,324.    Elapsed: 0:08:16. Loss: 0.57845\n",
            "  Batch 1,240  of  1,324.    Elapsed: 0:08:18. Loss: 0.57823\n",
            "  Batch 1,245  of  1,324.    Elapsed: 0:08:20. Loss: 0.57810\n",
            "  Batch 1,250  of  1,324.    Elapsed: 0:08:22. Loss: 0.57858\n",
            "  Batch 1,255  of  1,324.    Elapsed: 0:08:24. Loss: 0.57795\n",
            "  Batch 1,260  of  1,324.    Elapsed: 0:08:26. Loss: 0.57857\n",
            "  Batch 1,265  of  1,324.    Elapsed: 0:08:29. Loss: 0.57857\n",
            "  Batch 1,270  of  1,324.    Elapsed: 0:08:31. Loss: 0.57876\n",
            "  Batch 1,275  of  1,324.    Elapsed: 0:08:33. Loss: 0.57851\n",
            "  Batch 1,280  of  1,324.    Elapsed: 0:08:35. Loss: 0.57866\n",
            "  Batch 1,285  of  1,324.    Elapsed: 0:08:37. Loss: 0.57829\n",
            "  Batch 1,290  of  1,324.    Elapsed: 0:08:39. Loss: 0.57778\n",
            "  Batch 1,295  of  1,324.    Elapsed: 0:08:41. Loss: 0.57753\n",
            "  Batch 1,300  of  1,324.    Elapsed: 0:08:43. Loss: 0.57688\n",
            "  Batch 1,305  of  1,324.    Elapsed: 0:08:45. Loss: 0.57648\n",
            "  Batch 1,310  of  1,324.    Elapsed: 0:08:47. Loss: 0.57611\n",
            "  Batch 1,315  of  1,324.    Elapsed: 0:08:49. Loss: 0.57556\n",
            "  Batch 1,320  of  1,324.    Elapsed: 0:08:51. Loss: 0.57547\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epcoh took: 0:08:53\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:51\n",
            "  Accuracy: 0.75\n",
            "  Accuracy: 0.67\n",
            "  Macro F1-score: 0.71\n",
            "  Macro F1-score: 0.43\n",
            "  Weighted F1-score: 0.74\n",
            "  Weighted F1-score: 0.54\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.85      0.82      1736\n",
            "         1.0       0.66      0.56      0.60       912\n",
            "\n",
            "    accuracy                           0.75      2648\n",
            "   macro avg       0.72      0.70      0.71      2648\n",
            "weighted avg       0.74      0.75      0.74      2648\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.03      0.06       912\n",
            "         1.0       0.66      1.00      0.80      1736\n",
            "\n",
            "    accuracy                           0.67      2648\n",
            "   macro avg       0.80      0.52      0.43      2648\n",
            "weighted avg       0.76      0.67      0.54      2648\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1472  264]\n",
            " [ 403  509]]\n",
            "[[  30  882]\n",
            " [   2 1734]]\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  1,324.    Elapsed: 0:00:02. Loss: 0.37764\n",
            "  Batch    10  of  1,324.    Elapsed: 0:00:04. Loss: 0.50429\n",
            "  Batch    15  of  1,324.    Elapsed: 0:00:06. Loss: 0.52976\n",
            "  Batch    20  of  1,324.    Elapsed: 0:00:08. Loss: 0.49451\n",
            "  Batch    25  of  1,324.    Elapsed: 0:00:10. Loss: 0.48148\n",
            "  Batch    30  of  1,324.    Elapsed: 0:00:12. Loss: 0.49453\n",
            "  Batch    35  of  1,324.    Elapsed: 0:00:14. Loss: 0.49177\n",
            "  Batch    40  of  1,324.    Elapsed: 0:00:17. Loss: 0.49813\n",
            "  Batch    45  of  1,324.    Elapsed: 0:00:19. Loss: 0.49136\n",
            "  Batch    50  of  1,324.    Elapsed: 0:00:21. Loss: 0.48330\n",
            "  Batch    55  of  1,324.    Elapsed: 0:00:23. Loss: 0.49608\n",
            "  Batch    60  of  1,324.    Elapsed: 0:00:25. Loss: 0.49875\n",
            "  Batch    65  of  1,324.    Elapsed: 0:00:27. Loss: 0.50802\n",
            "  Batch    70  of  1,324.    Elapsed: 0:00:29. Loss: 0.49595\n",
            "  Batch    75  of  1,324.    Elapsed: 0:00:31. Loss: 0.49308\n",
            "  Batch    80  of  1,324.    Elapsed: 0:00:33. Loss: 0.49071\n",
            "  Batch    85  of  1,324.    Elapsed: 0:00:35. Loss: 0.49734\n",
            "  Batch    90  of  1,324.    Elapsed: 0:00:37. Loss: 0.49320\n",
            "  Batch    95  of  1,324.    Elapsed: 0:00:39. Loss: 0.48309\n",
            "  Batch   100  of  1,324.    Elapsed: 0:00:41. Loss: 0.48594\n",
            "  Batch   105  of  1,324.    Elapsed: 0:00:44. Loss: 0.49000\n",
            "  Batch   110  of  1,324.    Elapsed: 0:00:46. Loss: 0.49071\n",
            "  Batch   115  of  1,324.    Elapsed: 0:00:48. Loss: 0.49206\n",
            "  Batch   120  of  1,324.    Elapsed: 0:00:50. Loss: 0.49287\n",
            "  Batch   125  of  1,324.    Elapsed: 0:00:52. Loss: 0.49001\n",
            "  Batch   130  of  1,324.    Elapsed: 0:00:54. Loss: 0.49129\n",
            "  Batch   135  of  1,324.    Elapsed: 0:00:56. Loss: 0.49827\n",
            "  Batch   140  of  1,324.    Elapsed: 0:00:58. Loss: 0.49606\n",
            "  Batch   145  of  1,324.    Elapsed: 0:01:00. Loss: 0.48754\n",
            "  Batch   150  of  1,324.    Elapsed: 0:01:02. Loss: 0.48678\n",
            "  Batch   155  of  1,324.    Elapsed: 0:01:05. Loss: 0.48581\n",
            "  Batch   160  of  1,324.    Elapsed: 0:01:07. Loss: 0.48745\n",
            "  Batch   165  of  1,324.    Elapsed: 0:01:09. Loss: 0.48755\n",
            "  Batch   170  of  1,324.    Elapsed: 0:01:11. Loss: 0.48985\n",
            "  Batch   175  of  1,324.    Elapsed: 0:01:13. Loss: 0.49039\n",
            "  Batch   180  of  1,324.    Elapsed: 0:01:15. Loss: 0.48903\n",
            "  Batch   185  of  1,324.    Elapsed: 0:01:17. Loss: 0.49122\n",
            "  Batch   190  of  1,324.    Elapsed: 0:01:19. Loss: 0.49396\n",
            "  Batch   195  of  1,324.    Elapsed: 0:01:21. Loss: 0.49537\n",
            "  Batch   200  of  1,324.    Elapsed: 0:01:23. Loss: 0.49250\n",
            "  Batch   205  of  1,324.    Elapsed: 0:01:25. Loss: 0.49181\n",
            "  Batch   210  of  1,324.    Elapsed: 0:01:27. Loss: 0.48972\n",
            "  Batch   215  of  1,324.    Elapsed: 0:01:30. Loss: 0.49070\n",
            "  Batch   220  of  1,324.    Elapsed: 0:01:32. Loss: 0.49312\n",
            "  Batch   225  of  1,324.    Elapsed: 0:01:34. Loss: 0.49033\n",
            "  Batch   230  of  1,324.    Elapsed: 0:01:36. Loss: 0.49165\n",
            "  Batch   235  of  1,324.    Elapsed: 0:01:38. Loss: 0.49042\n",
            "  Batch   240  of  1,324.    Elapsed: 0:01:40. Loss: 0.49663\n",
            "  Batch   245  of  1,324.    Elapsed: 0:01:42. Loss: 0.49781\n",
            "  Batch   250  of  1,324.    Elapsed: 0:01:44. Loss: 0.49466\n",
            "  Batch   255  of  1,324.    Elapsed: 0:01:46. Loss: 0.49200\n",
            "  Batch   260  of  1,324.    Elapsed: 0:01:48. Loss: 0.49262\n",
            "  Batch   265  of  1,324.    Elapsed: 0:01:50. Loss: 0.49417\n",
            "  Batch   270  of  1,324.    Elapsed: 0:01:52. Loss: 0.49665\n",
            "  Batch   275  of  1,324.    Elapsed: 0:01:54. Loss: 0.49633\n",
            "  Batch   280  of  1,324.    Elapsed: 0:01:56. Loss: 0.49555\n",
            "  Batch   285  of  1,324.    Elapsed: 0:01:59. Loss: 0.49528\n",
            "  Batch   290  of  1,324.    Elapsed: 0:02:01. Loss: 0.49514\n",
            "  Batch   295  of  1,324.    Elapsed: 0:02:03. Loss: 0.49456\n",
            "  Batch   300  of  1,324.    Elapsed: 0:02:05. Loss: 0.49203\n",
            "  Batch   305  of  1,324.    Elapsed: 0:02:07. Loss: 0.48859\n",
            "  Batch   310  of  1,324.    Elapsed: 0:02:09. Loss: 0.49112\n",
            "  Batch   315  of  1,324.    Elapsed: 0:02:11. Loss: 0.48936\n",
            "  Batch   320  of  1,324.    Elapsed: 0:02:13. Loss: 0.48740\n",
            "  Batch   325  of  1,324.    Elapsed: 0:02:15. Loss: 0.48723\n",
            "  Batch   330  of  1,324.    Elapsed: 0:02:17. Loss: 0.48522\n",
            "  Batch   335  of  1,324.    Elapsed: 0:02:19. Loss: 0.48406\n",
            "  Batch   340  of  1,324.    Elapsed: 0:02:21. Loss: 0.48498\n",
            "  Batch   345  of  1,324.    Elapsed: 0:02:23. Loss: 0.48617\n",
            "  Batch   350  of  1,324.    Elapsed: 0:02:25. Loss: 0.48699\n",
            "  Batch   355  of  1,324.    Elapsed: 0:02:28. Loss: 0.48651\n",
            "  Batch   360  of  1,324.    Elapsed: 0:02:30. Loss: 0.48811\n",
            "  Batch   365  of  1,324.    Elapsed: 0:02:32. Loss: 0.48608\n",
            "  Batch   370  of  1,324.    Elapsed: 0:02:34. Loss: 0.48563\n",
            "  Batch   375  of  1,324.    Elapsed: 0:02:36. Loss: 0.48547\n",
            "  Batch   380  of  1,324.    Elapsed: 0:02:38. Loss: 0.48768\n",
            "  Batch   385  of  1,324.    Elapsed: 0:02:40. Loss: 0.48715\n",
            "  Batch   390  of  1,324.    Elapsed: 0:02:42. Loss: 0.48771\n",
            "  Batch   395  of  1,324.    Elapsed: 0:02:44. Loss: 0.48896\n",
            "  Batch   400  of  1,324.    Elapsed: 0:02:46. Loss: 0.48936\n",
            "  Batch   405  of  1,324.    Elapsed: 0:02:48. Loss: 0.48773\n",
            "  Batch   410  of  1,324.    Elapsed: 0:02:50. Loss: 0.48777\n",
            "  Batch   415  of  1,324.    Elapsed: 0:02:53. Loss: 0.48831\n",
            "  Batch   420  of  1,324.    Elapsed: 0:02:55. Loss: 0.48797\n",
            "  Batch   425  of  1,324.    Elapsed: 0:02:57. Loss: 0.48813\n",
            "  Batch   430  of  1,324.    Elapsed: 0:02:59. Loss: 0.48805\n",
            "  Batch   435  of  1,324.    Elapsed: 0:03:01. Loss: 0.48790\n",
            "  Batch   440  of  1,324.    Elapsed: 0:03:03. Loss: 0.48924\n",
            "  Batch   445  of  1,324.    Elapsed: 0:03:05. Loss: 0.48986\n",
            "  Batch   450  of  1,324.    Elapsed: 0:03:07. Loss: 0.48927\n",
            "  Batch   455  of  1,324.    Elapsed: 0:03:09. Loss: 0.48926\n",
            "  Batch   460  of  1,324.    Elapsed: 0:03:11. Loss: 0.48943\n",
            "  Batch   465  of  1,324.    Elapsed: 0:03:13. Loss: 0.48839\n",
            "  Batch   470  of  1,324.    Elapsed: 0:03:15. Loss: 0.48827\n",
            "  Batch   475  of  1,324.    Elapsed: 0:03:17. Loss: 0.48634\n",
            "  Batch   480  of  1,324.    Elapsed: 0:03:20. Loss: 0.48785\n",
            "  Batch   485  of  1,324.    Elapsed: 0:03:22. Loss: 0.48755\n",
            "  Batch   490  of  1,324.    Elapsed: 0:03:24. Loss: 0.48757\n",
            "  Batch   495  of  1,324.    Elapsed: 0:03:26. Loss: 0.48668\n",
            "  Batch   500  of  1,324.    Elapsed: 0:03:28. Loss: 0.48657\n",
            "  Batch   505  of  1,324.    Elapsed: 0:03:30. Loss: 0.48792\n",
            "  Batch   510  of  1,324.    Elapsed: 0:03:32. Loss: 0.48967\n",
            "  Batch   515  of  1,324.    Elapsed: 0:03:34. Loss: 0.48976\n",
            "  Batch   520  of  1,324.    Elapsed: 0:03:36. Loss: 0.49185\n",
            "  Batch   525  of  1,324.    Elapsed: 0:03:38. Loss: 0.49222\n",
            "  Batch   530  of  1,324.    Elapsed: 0:03:40. Loss: 0.49206\n",
            "  Batch   535  of  1,324.    Elapsed: 0:03:42. Loss: 0.49288\n",
            "  Batch   540  of  1,324.    Elapsed: 0:03:44. Loss: 0.49294\n",
            "  Batch   545  of  1,324.    Elapsed: 0:03:47. Loss: 0.49362\n",
            "  Batch   550  of  1,324.    Elapsed: 0:03:49. Loss: 0.49352\n",
            "  Batch   555  of  1,324.    Elapsed: 0:03:51. Loss: 0.49346\n",
            "  Batch   560  of  1,324.    Elapsed: 0:03:53. Loss: 0.49309\n",
            "  Batch   565  of  1,324.    Elapsed: 0:03:55. Loss: 0.49320\n",
            "  Batch   570  of  1,324.    Elapsed: 0:03:57. Loss: 0.49140\n",
            "  Batch   575  of  1,324.    Elapsed: 0:03:59. Loss: 0.49131\n",
            "  Batch   580  of  1,324.    Elapsed: 0:04:01. Loss: 0.49179\n",
            "  Batch   585  of  1,324.    Elapsed: 0:04:03. Loss: 0.49160\n",
            "  Batch   590  of  1,324.    Elapsed: 0:04:05. Loss: 0.49261\n",
            "  Batch   595  of  1,324.    Elapsed: 0:04:07. Loss: 0.49339\n",
            "  Batch   600  of  1,324.    Elapsed: 0:04:09. Loss: 0.49316\n",
            "  Batch   605  of  1,324.    Elapsed: 0:04:11. Loss: 0.49243\n",
            "  Batch   610  of  1,324.    Elapsed: 0:04:14. Loss: 0.49148\n",
            "  Batch   615  of  1,324.    Elapsed: 0:04:16. Loss: 0.49203\n",
            "  Batch   620  of  1,324.    Elapsed: 0:04:18. Loss: 0.49273\n",
            "  Batch   625  of  1,324.    Elapsed: 0:04:20. Loss: 0.49168\n",
            "  Batch   630  of  1,324.    Elapsed: 0:04:22. Loss: 0.49120\n",
            "  Batch   635  of  1,324.    Elapsed: 0:04:24. Loss: 0.49058\n",
            "  Batch   640  of  1,324.    Elapsed: 0:04:26. Loss: 0.48961\n",
            "  Batch   645  of  1,324.    Elapsed: 0:04:28. Loss: 0.48980\n",
            "  Batch   650  of  1,324.    Elapsed: 0:04:30. Loss: 0.48924\n",
            "  Batch   655  of  1,324.    Elapsed: 0:04:32. Loss: 0.48862\n",
            "  Batch   660  of  1,324.    Elapsed: 0:04:34. Loss: 0.48958\n",
            "  Batch   665  of  1,324.    Elapsed: 0:04:36. Loss: 0.48916\n",
            "  Batch   670  of  1,324.    Elapsed: 0:04:38. Loss: 0.48838\n",
            "  Batch   675  of  1,324.    Elapsed: 0:04:40. Loss: 0.48849\n",
            "  Batch   680  of  1,324.    Elapsed: 0:04:43. Loss: 0.48846\n",
            "  Batch   685  of  1,324.    Elapsed: 0:04:45. Loss: 0.48813\n",
            "  Batch   690  of  1,324.    Elapsed: 0:04:47. Loss: 0.48936\n",
            "  Batch   695  of  1,324.    Elapsed: 0:04:49. Loss: 0.48995\n",
            "  Batch   700  of  1,324.    Elapsed: 0:04:51. Loss: 0.49063\n",
            "  Batch   705  of  1,324.    Elapsed: 0:04:53. Loss: 0.49050\n",
            "  Batch   710  of  1,324.    Elapsed: 0:04:55. Loss: 0.49043\n",
            "  Batch   715  of  1,324.    Elapsed: 0:04:57. Loss: 0.48979\n",
            "  Batch   720  of  1,324.    Elapsed: 0:04:59. Loss: 0.48868\n",
            "  Batch   725  of  1,324.    Elapsed: 0:05:01. Loss: 0.48899\n",
            "  Batch   730  of  1,324.    Elapsed: 0:05:03. Loss: 0.48814\n",
            "  Batch   735  of  1,324.    Elapsed: 0:05:06. Loss: 0.48850\n",
            "  Batch   740  of  1,324.    Elapsed: 0:05:08. Loss: 0.48813\n",
            "  Batch   745  of  1,324.    Elapsed: 0:05:10. Loss: 0.48821\n",
            "  Batch   750  of  1,324.    Elapsed: 0:05:12. Loss: 0.48821\n",
            "  Batch   755  of  1,324.    Elapsed: 0:05:14. Loss: 0.48795\n",
            "  Batch   760  of  1,324.    Elapsed: 0:05:16. Loss: 0.48839\n",
            "  Batch   765  of  1,324.    Elapsed: 0:05:18. Loss: 0.48905\n",
            "  Batch   770  of  1,324.    Elapsed: 0:05:20. Loss: 0.48917\n",
            "  Batch   775  of  1,324.    Elapsed: 0:05:22. Loss: 0.48910\n",
            "  Batch   780  of  1,324.    Elapsed: 0:05:24. Loss: 0.48939\n",
            "  Batch   785  of  1,324.    Elapsed: 0:05:27. Loss: 0.48950\n",
            "  Batch   790  of  1,324.    Elapsed: 0:05:29. Loss: 0.48885\n",
            "  Batch   795  of  1,324.    Elapsed: 0:05:31. Loss: 0.48958\n",
            "  Batch   800  of  1,324.    Elapsed: 0:05:33. Loss: 0.48900\n",
            "  Batch   805  of  1,324.    Elapsed: 0:05:35. Loss: 0.48870\n",
            "  Batch   810  of  1,324.    Elapsed: 0:05:37. Loss: 0.48892\n",
            "  Batch   815  of  1,324.    Elapsed: 0:05:39. Loss: 0.48946\n",
            "  Batch   820  of  1,324.    Elapsed: 0:05:41. Loss: 0.48862\n",
            "  Batch   825  of  1,324.    Elapsed: 0:05:43. Loss: 0.48809\n",
            "  Batch   830  of  1,324.    Elapsed: 0:05:45. Loss: 0.48798\n",
            "  Batch   835  of  1,324.    Elapsed: 0:05:47. Loss: 0.48846\n",
            "  Batch   840  of  1,324.    Elapsed: 0:05:49. Loss: 0.48892\n",
            "  Batch   845  of  1,324.    Elapsed: 0:05:52. Loss: 0.48949\n",
            "  Batch   850  of  1,324.    Elapsed: 0:05:54. Loss: 0.48934\n",
            "  Batch   855  of  1,324.    Elapsed: 0:05:56. Loss: 0.48978\n",
            "  Batch   860  of  1,324.    Elapsed: 0:05:58. Loss: 0.48968\n",
            "  Batch   865  of  1,324.    Elapsed: 0:06:00. Loss: 0.48903\n",
            "  Batch   870  of  1,324.    Elapsed: 0:06:02. Loss: 0.48910\n",
            "  Batch   875  of  1,324.    Elapsed: 0:06:04. Loss: 0.48950\n",
            "  Batch   880  of  1,324.    Elapsed: 0:06:06. Loss: 0.48914\n",
            "  Batch   885  of  1,324.    Elapsed: 0:06:08. Loss: 0.48937\n",
            "  Batch   890  of  1,324.    Elapsed: 0:06:10. Loss: 0.48914\n",
            "  Batch   895  of  1,324.    Elapsed: 0:06:12. Loss: 0.48889\n",
            "  Batch   900  of  1,324.    Elapsed: 0:06:14. Loss: 0.48886\n",
            "  Batch   905  of  1,324.    Elapsed: 0:06:16. Loss: 0.48927\n",
            "  Batch   910  of  1,324.    Elapsed: 0:06:18. Loss: 0.48890\n",
            "  Batch   915  of  1,324.    Elapsed: 0:06:21. Loss: 0.48965\n",
            "  Batch   920  of  1,324.    Elapsed: 0:06:23. Loss: 0.48951\n",
            "  Batch   925  of  1,324.    Elapsed: 0:06:25. Loss: 0.48923\n",
            "  Batch   930  of  1,324.    Elapsed: 0:06:27. Loss: 0.48899\n",
            "  Batch   935  of  1,324.    Elapsed: 0:06:29. Loss: 0.48901\n",
            "  Batch   940  of  1,324.    Elapsed: 0:06:31. Loss: 0.48894\n",
            "  Batch   945  of  1,324.    Elapsed: 0:06:33. Loss: 0.48916\n",
            "  Batch   950  of  1,324.    Elapsed: 0:06:35. Loss: 0.48930\n",
            "  Batch   955  of  1,324.    Elapsed: 0:06:37. Loss: 0.48880\n",
            "  Batch   960  of  1,324.    Elapsed: 0:06:39. Loss: 0.48817\n",
            "  Batch   965  of  1,324.    Elapsed: 0:06:41. Loss: 0.48799\n",
            "  Batch   970  of  1,324.    Elapsed: 0:06:43. Loss: 0.48815\n",
            "  Batch   975  of  1,324.    Elapsed: 0:06:45. Loss: 0.48864\n",
            "  Batch   980  of  1,324.    Elapsed: 0:06:47. Loss: 0.48805\n",
            "  Batch   985  of  1,324.    Elapsed: 0:06:50. Loss: 0.48760\n",
            "  Batch   990  of  1,324.    Elapsed: 0:06:52. Loss: 0.48710\n",
            "  Batch   995  of  1,324.    Elapsed: 0:06:54. Loss: 0.48748\n",
            "  Batch 1,000  of  1,324.    Elapsed: 0:06:56. Loss: 0.48807\n",
            "  Batch 1,005  of  1,324.    Elapsed: 0:06:58. Loss: 0.48804\n",
            "  Batch 1,010  of  1,324.    Elapsed: 0:07:00. Loss: 0.48872\n",
            "  Batch 1,015  of  1,324.    Elapsed: 0:07:02. Loss: 0.48838\n",
            "  Batch 1,020  of  1,324.    Elapsed: 0:07:04. Loss: 0.48917\n",
            "  Batch 1,025  of  1,324.    Elapsed: 0:07:06. Loss: 0.48944\n",
            "  Batch 1,030  of  1,324.    Elapsed: 0:07:08. Loss: 0.48946\n",
            "  Batch 1,035  of  1,324.    Elapsed: 0:07:10. Loss: 0.48951\n",
            "  Batch 1,040  of  1,324.    Elapsed: 0:07:12. Loss: 0.48962\n",
            "  Batch 1,045  of  1,324.    Elapsed: 0:07:14. Loss: 0.48918\n",
            "  Batch 1,050  of  1,324.    Elapsed: 0:07:17. Loss: 0.49014\n",
            "  Batch 1,055  of  1,324.    Elapsed: 0:07:19. Loss: 0.48947\n",
            "  Batch 1,060  of  1,324.    Elapsed: 0:07:21. Loss: 0.48923\n",
            "  Batch 1,065  of  1,324.    Elapsed: 0:07:23. Loss: 0.48937\n",
            "  Batch 1,070  of  1,324.    Elapsed: 0:07:25. Loss: 0.48952\n",
            "  Batch 1,075  of  1,324.    Elapsed: 0:07:27. Loss: 0.48949\n",
            "  Batch 1,080  of  1,324.    Elapsed: 0:07:29. Loss: 0.48964\n",
            "  Batch 1,085  of  1,324.    Elapsed: 0:07:31. Loss: 0.48952\n",
            "  Batch 1,090  of  1,324.    Elapsed: 0:07:33. Loss: 0.48972\n",
            "  Batch 1,095  of  1,324.    Elapsed: 0:07:35. Loss: 0.48886\n",
            "  Batch 1,100  of  1,324.    Elapsed: 0:07:37. Loss: 0.48877\n",
            "  Batch 1,105  of  1,324.    Elapsed: 0:07:39. Loss: 0.48884\n",
            "  Batch 1,110  of  1,324.    Elapsed: 0:07:41. Loss: 0.48860\n",
            "  Batch 1,115  of  1,324.    Elapsed: 0:07:44. Loss: 0.48788\n",
            "  Batch 1,120  of  1,324.    Elapsed: 0:07:46. Loss: 0.48852\n",
            "  Batch 1,125  of  1,324.    Elapsed: 0:07:48. Loss: 0.48880\n",
            "  Batch 1,130  of  1,324.    Elapsed: 0:07:50. Loss: 0.48872\n",
            "  Batch 1,135  of  1,324.    Elapsed: 0:07:52. Loss: 0.48874\n",
            "  Batch 1,140  of  1,324.    Elapsed: 0:07:54. Loss: 0.48870\n",
            "  Batch 1,145  of  1,324.    Elapsed: 0:07:56. Loss: 0.48879\n",
            "  Batch 1,150  of  1,324.    Elapsed: 0:07:58. Loss: 0.48847\n",
            "  Batch 1,155  of  1,324.    Elapsed: 0:08:00. Loss: 0.48813\n",
            "  Batch 1,160  of  1,324.    Elapsed: 0:08:02. Loss: 0.48879\n",
            "  Batch 1,165  of  1,324.    Elapsed: 0:08:04. Loss: 0.48859\n",
            "  Batch 1,170  of  1,324.    Elapsed: 0:08:06. Loss: 0.48858\n",
            "  Batch 1,175  of  1,324.    Elapsed: 0:08:08. Loss: 0.48857\n",
            "  Batch 1,180  of  1,324.    Elapsed: 0:08:11. Loss: 0.48804\n",
            "  Batch 1,185  of  1,324.    Elapsed: 0:08:13. Loss: 0.48823\n",
            "  Batch 1,190  of  1,324.    Elapsed: 0:08:15. Loss: 0.48800\n",
            "  Batch 1,195  of  1,324.    Elapsed: 0:08:17. Loss: 0.48797\n",
            "  Batch 1,200  of  1,324.    Elapsed: 0:08:19. Loss: 0.48769\n",
            "  Batch 1,205  of  1,324.    Elapsed: 0:08:21. Loss: 0.48760\n",
            "  Batch 1,210  of  1,324.    Elapsed: 0:08:23. Loss: 0.48694\n",
            "  Batch 1,215  of  1,324.    Elapsed: 0:08:25. Loss: 0.48659\n",
            "  Batch 1,220  of  1,324.    Elapsed: 0:08:27. Loss: 0.48702\n",
            "  Batch 1,225  of  1,324.    Elapsed: 0:08:29. Loss: 0.48688\n",
            "  Batch 1,230  of  1,324.    Elapsed: 0:08:31. Loss: 0.48716\n",
            "  Batch 1,235  of  1,324.    Elapsed: 0:08:33. Loss: 0.48766\n",
            "  Batch 1,240  of  1,324.    Elapsed: 0:08:36. Loss: 0.48696\n",
            "  Batch 1,245  of  1,324.    Elapsed: 0:08:38. Loss: 0.48651\n",
            "  Batch 1,250  of  1,324.    Elapsed: 0:08:40. Loss: 0.48681\n",
            "  Batch 1,255  of  1,324.    Elapsed: 0:08:42. Loss: 0.48646\n",
            "  Batch 1,260  of  1,324.    Elapsed: 0:08:44. Loss: 0.48632\n",
            "  Batch 1,265  of  1,324.    Elapsed: 0:08:46. Loss: 0.48660\n",
            "  Batch 1,270  of  1,324.    Elapsed: 0:08:48. Loss: 0.48646\n",
            "  Batch 1,275  of  1,324.    Elapsed: 0:08:50. Loss: 0.48658\n",
            "  Batch 1,280  of  1,324.    Elapsed: 0:08:52. Loss: 0.48614\n",
            "  Batch 1,285  of  1,324.    Elapsed: 0:08:54. Loss: 0.48603\n",
            "  Batch 1,290  of  1,324.    Elapsed: 0:08:56. Loss: 0.48516\n",
            "  Batch 1,295  of  1,324.    Elapsed: 0:08:58. Loss: 0.48501\n",
            "  Batch 1,300  of  1,324.    Elapsed: 0:09:00. Loss: 0.48505\n",
            "  Batch 1,305  of  1,324.    Elapsed: 0:09:03. Loss: 0.48481\n",
            "  Batch 1,310  of  1,324.    Elapsed: 0:09:05. Loss: 0.48460\n",
            "  Batch 1,315  of  1,324.    Elapsed: 0:09:07. Loss: 0.48428\n",
            "  Batch 1,320  of  1,324.    Elapsed: 0:09:09. Loss: 0.48482\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:09:10\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:51\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.76\n",
            "  Macro F1-score: 0.74\n",
            "  Macro F1-score: 0.68\n",
            "  Weighted F1-score: 0.77\n",
            "  Weighted F1-score: 0.73\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.83      0.82      1736\n",
            "         1.0       0.66      0.65      0.66       912\n",
            "\n",
            "    accuracy                           0.77      2648\n",
            "   macro avg       0.74      0.74      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.37      0.51       912\n",
            "         1.0       0.74      0.96      0.84      1736\n",
            "\n",
            "    accuracy                           0.76      2648\n",
            "   macro avg       0.79      0.67      0.68      2648\n",
            "weighted avg       0.77      0.76      0.73      2648\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1434  302]\n",
            " [ 316  596]]\n",
            "[[ 340  572]\n",
            " [  70 1666]]\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  1,324.    Elapsed: 0:00:02. Loss: 0.41506\n",
            "  Batch    10  of  1,324.    Elapsed: 0:00:04. Loss: 0.49972\n",
            "  Batch    15  of  1,324.    Elapsed: 0:00:06. Loss: 0.43578\n",
            "  Batch    20  of  1,324.    Elapsed: 0:00:08. Loss: 0.41241\n",
            "  Batch    25  of  1,324.    Elapsed: 0:00:10. Loss: 0.43839\n",
            "  Batch    30  of  1,324.    Elapsed: 0:00:12. Loss: 0.43893\n",
            "  Batch    35  of  1,324.    Elapsed: 0:00:14. Loss: 0.42858\n",
            "  Batch    40  of  1,324.    Elapsed: 0:00:17. Loss: 0.42883\n",
            "  Batch    45  of  1,324.    Elapsed: 0:00:19. Loss: 0.42215\n",
            "  Batch    50  of  1,324.    Elapsed: 0:00:21. Loss: 0.42803\n",
            "  Batch    55  of  1,324.    Elapsed: 0:00:23. Loss: 0.43021\n",
            "  Batch    60  of  1,324.    Elapsed: 0:00:25. Loss: 0.42089\n",
            "  Batch    65  of  1,324.    Elapsed: 0:00:27. Loss: 0.43245\n",
            "  Batch    70  of  1,324.    Elapsed: 0:00:29. Loss: 0.43730\n",
            "  Batch    75  of  1,324.    Elapsed: 0:00:31. Loss: 0.43060\n",
            "  Batch    80  of  1,324.    Elapsed: 0:00:33. Loss: 0.43378\n",
            "  Batch    85  of  1,324.    Elapsed: 0:00:35. Loss: 0.43183\n",
            "  Batch    90  of  1,324.    Elapsed: 0:00:37. Loss: 0.42538\n",
            "  Batch    95  of  1,324.    Elapsed: 0:00:39. Loss: 0.42947\n",
            "  Batch   100  of  1,324.    Elapsed: 0:00:41. Loss: 0.43567\n",
            "  Batch   105  of  1,324.    Elapsed: 0:00:43. Loss: 0.43259\n",
            "  Batch   110  of  1,324.    Elapsed: 0:00:45. Loss: 0.44094\n",
            "  Batch   115  of  1,324.    Elapsed: 0:00:47. Loss: 0.43622\n",
            "  Batch   120  of  1,324.    Elapsed: 0:00:50. Loss: 0.43933\n",
            "  Batch   125  of  1,324.    Elapsed: 0:00:52. Loss: 0.43899\n",
            "  Batch   130  of  1,324.    Elapsed: 0:00:54. Loss: 0.43734\n",
            "  Batch   135  of  1,324.    Elapsed: 0:00:56. Loss: 0.43381\n",
            "  Batch   140  of  1,324.    Elapsed: 0:00:58. Loss: 0.43716\n",
            "  Batch   145  of  1,324.    Elapsed: 0:01:00. Loss: 0.43685\n",
            "  Batch   150  of  1,324.    Elapsed: 0:01:02. Loss: 0.43714\n",
            "  Batch   155  of  1,324.    Elapsed: 0:01:04. Loss: 0.44886\n",
            "  Batch   160  of  1,324.    Elapsed: 0:01:06. Loss: 0.44812\n",
            "  Batch   165  of  1,324.    Elapsed: 0:01:08. Loss: 0.44935\n",
            "  Batch   170  of  1,324.    Elapsed: 0:01:10. Loss: 0.44683\n",
            "  Batch   175  of  1,324.    Elapsed: 0:01:12. Loss: 0.44813\n",
            "  Batch   180  of  1,324.    Elapsed: 0:01:14. Loss: 0.44746\n",
            "  Batch   185  of  1,324.    Elapsed: 0:01:16. Loss: 0.44523\n",
            "  Batch   190  of  1,324.    Elapsed: 0:01:18. Loss: 0.43951\n",
            "  Batch   195  of  1,324.    Elapsed: 0:01:21. Loss: 0.44253\n",
            "  Batch   200  of  1,324.    Elapsed: 0:01:23. Loss: 0.44624\n",
            "  Batch   205  of  1,324.    Elapsed: 0:01:25. Loss: 0.44839\n",
            "  Batch   210  of  1,324.    Elapsed: 0:01:27. Loss: 0.44359\n",
            "  Batch   215  of  1,324.    Elapsed: 0:01:29. Loss: 0.44292\n",
            "  Batch   220  of  1,324.    Elapsed: 0:01:31. Loss: 0.44226\n",
            "  Batch   225  of  1,324.    Elapsed: 0:01:33. Loss: 0.44232\n",
            "  Batch   230  of  1,324.    Elapsed: 0:01:35. Loss: 0.44330\n",
            "  Batch   235  of  1,324.    Elapsed: 0:01:37. Loss: 0.44553\n",
            "  Batch   240  of  1,324.    Elapsed: 0:01:39. Loss: 0.44241\n",
            "  Batch   245  of  1,324.    Elapsed: 0:01:41. Loss: 0.44163\n",
            "  Batch   250  of  1,324.    Elapsed: 0:01:43. Loss: 0.44086\n",
            "  Batch   255  of  1,324.    Elapsed: 0:01:45. Loss: 0.43952\n",
            "  Batch   260  of  1,324.    Elapsed: 0:01:47. Loss: 0.43907\n",
            "  Batch   265  of  1,324.    Elapsed: 0:01:49. Loss: 0.43633\n",
            "  Batch   270  of  1,324.    Elapsed: 0:01:52. Loss: 0.43515\n",
            "  Batch   275  of  1,324.    Elapsed: 0:01:54. Loss: 0.43500\n",
            "  Batch   280  of  1,324.    Elapsed: 0:01:56. Loss: 0.43376\n",
            "  Batch   285  of  1,324.    Elapsed: 0:01:58. Loss: 0.43412\n",
            "  Batch   290  of  1,324.    Elapsed: 0:02:00. Loss: 0.43417\n",
            "  Batch   295  of  1,324.    Elapsed: 0:02:02. Loss: 0.43367\n",
            "  Batch   300  of  1,324.    Elapsed: 0:02:04. Loss: 0.43138\n",
            "  Batch   305  of  1,324.    Elapsed: 0:02:06. Loss: 0.43199\n",
            "  Batch   310  of  1,324.    Elapsed: 0:02:08. Loss: 0.43122\n",
            "  Batch   315  of  1,324.    Elapsed: 0:02:10. Loss: 0.43376\n",
            "  Batch   320  of  1,324.    Elapsed: 0:02:12. Loss: 0.43441\n",
            "  Batch   325  of  1,324.    Elapsed: 0:02:14. Loss: 0.43459\n",
            "  Batch   330  of  1,324.    Elapsed: 0:02:16. Loss: 0.43289\n",
            "  Batch   335  of  1,324.    Elapsed: 0:02:18. Loss: 0.43363\n",
            "  Batch   340  of  1,324.    Elapsed: 0:02:20. Loss: 0.43294\n",
            "  Batch   345  of  1,324.    Elapsed: 0:02:22. Loss: 0.43328\n",
            "  Batch   350  of  1,324.    Elapsed: 0:02:25. Loss: 0.43473\n",
            "  Batch   355  of  1,324.    Elapsed: 0:02:27. Loss: 0.43480\n",
            "  Batch   360  of  1,324.    Elapsed: 0:02:29. Loss: 0.43812\n",
            "  Batch   365  of  1,324.    Elapsed: 0:02:31. Loss: 0.44096\n",
            "  Batch   370  of  1,324.    Elapsed: 0:02:33. Loss: 0.44420\n",
            "  Batch   375  of  1,324.    Elapsed: 0:02:35. Loss: 0.44315\n",
            "  Batch   380  of  1,324.    Elapsed: 0:02:37. Loss: 0.44323\n",
            "  Batch   385  of  1,324.    Elapsed: 0:02:39. Loss: 0.44335\n",
            "  Batch   390  of  1,324.    Elapsed: 0:02:41. Loss: 0.44288\n",
            "  Batch   395  of  1,324.    Elapsed: 0:02:43. Loss: 0.44250\n",
            "  Batch   400  of  1,324.    Elapsed: 0:02:45. Loss: 0.44318\n",
            "  Batch   405  of  1,324.    Elapsed: 0:02:47. Loss: 0.44139\n",
            "  Batch   410  of  1,324.    Elapsed: 0:02:49. Loss: 0.44156\n",
            "  Batch   415  of  1,324.    Elapsed: 0:02:51. Loss: 0.44281\n",
            "  Batch   420  of  1,324.    Elapsed: 0:02:53. Loss: 0.44218\n",
            "  Batch   425  of  1,324.    Elapsed: 0:02:56. Loss: 0.44008\n",
            "  Batch   430  of  1,324.    Elapsed: 0:02:58. Loss: 0.44051\n",
            "  Batch   435  of  1,324.    Elapsed: 0:03:00. Loss: 0.43899\n",
            "  Batch   440  of  1,324.    Elapsed: 0:03:02. Loss: 0.43906\n",
            "  Batch   445  of  1,324.    Elapsed: 0:03:04. Loss: 0.43947\n",
            "  Batch   450  of  1,324.    Elapsed: 0:03:06. Loss: 0.43860\n",
            "  Batch   455  of  1,324.    Elapsed: 0:03:08. Loss: 0.43954\n",
            "  Batch   460  of  1,324.    Elapsed: 0:03:10. Loss: 0.43932\n",
            "  Batch   465  of  1,324.    Elapsed: 0:03:12. Loss: 0.44073\n",
            "  Batch   470  of  1,324.    Elapsed: 0:03:14. Loss: 0.43990\n",
            "  Batch   475  of  1,324.    Elapsed: 0:03:16. Loss: 0.43813\n",
            "  Batch   480  of  1,324.    Elapsed: 0:03:18. Loss: 0.43613\n",
            "  Batch   485  of  1,324.    Elapsed: 0:03:20. Loss: 0.43546\n",
            "  Batch   490  of  1,324.    Elapsed: 0:03:22. Loss: 0.43488\n",
            "  Batch   495  of  1,324.    Elapsed: 0:03:24. Loss: 0.43630\n",
            "  Batch   500  of  1,324.    Elapsed: 0:03:26. Loss: 0.43569\n",
            "  Batch   505  of  1,324.    Elapsed: 0:03:29. Loss: 0.43512\n",
            "  Batch   510  of  1,324.    Elapsed: 0:03:31. Loss: 0.43526\n",
            "  Batch   515  of  1,324.    Elapsed: 0:03:33. Loss: 0.43741\n",
            "  Batch   520  of  1,324.    Elapsed: 0:03:35. Loss: 0.43707\n",
            "  Batch   525  of  1,324.    Elapsed: 0:03:37. Loss: 0.43694\n",
            "  Batch   530  of  1,324.    Elapsed: 0:03:39. Loss: 0.43515\n",
            "  Batch   535  of  1,324.    Elapsed: 0:03:41. Loss: 0.43460\n",
            "  Batch   540  of  1,324.    Elapsed: 0:03:43. Loss: 0.43338\n",
            "  Batch   545  of  1,324.    Elapsed: 0:03:45. Loss: 0.43261\n",
            "  Batch   550  of  1,324.    Elapsed: 0:03:47. Loss: 0.43327\n",
            "  Batch   555  of  1,324.    Elapsed: 0:03:49. Loss: 0.43255\n",
            "  Batch   560  of  1,324.    Elapsed: 0:03:52. Loss: 0.43313\n",
            "  Batch   565  of  1,324.    Elapsed: 0:03:54. Loss: 0.43262\n",
            "  Batch   570  of  1,324.    Elapsed: 0:03:56. Loss: 0.43341\n",
            "  Batch   575  of  1,324.    Elapsed: 0:03:58. Loss: 0.43393\n",
            "  Batch   580  of  1,324.    Elapsed: 0:04:00. Loss: 0.43305\n",
            "  Batch   585  of  1,324.    Elapsed: 0:04:02. Loss: 0.43357\n",
            "  Batch   590  of  1,324.    Elapsed: 0:04:04. Loss: 0.43412\n",
            "  Batch   595  of  1,324.    Elapsed: 0:04:06. Loss: 0.43428\n",
            "  Batch   600  of  1,324.    Elapsed: 0:04:08. Loss: 0.43309\n",
            "  Batch   605  of  1,324.    Elapsed: 0:04:10. Loss: 0.43271\n",
            "  Batch   610  of  1,324.    Elapsed: 0:04:13. Loss: 0.43260\n",
            "  Batch   615  of  1,324.    Elapsed: 0:04:15. Loss: 0.43183\n",
            "  Batch   620  of  1,324.    Elapsed: 0:04:17. Loss: 0.43133\n",
            "  Batch   625  of  1,324.    Elapsed: 0:04:19. Loss: 0.43120\n",
            "  Batch   630  of  1,324.    Elapsed: 0:04:21. Loss: 0.43146\n",
            "  Batch   635  of  1,324.    Elapsed: 0:04:23. Loss: 0.43083\n",
            "  Batch   640  of  1,324.    Elapsed: 0:04:25. Loss: 0.43093\n",
            "  Batch   645  of  1,324.    Elapsed: 0:04:27. Loss: 0.42992\n",
            "  Batch   650  of  1,324.    Elapsed: 0:04:29. Loss: 0.42900\n",
            "  Batch   655  of  1,324.    Elapsed: 0:04:31. Loss: 0.42909\n",
            "  Batch   660  of  1,324.    Elapsed: 0:04:33. Loss: 0.42916\n",
            "  Batch   665  of  1,324.    Elapsed: 0:04:35. Loss: 0.42874\n",
            "  Batch   670  of  1,324.    Elapsed: 0:04:37. Loss: 0.42972\n",
            "  Batch   675  of  1,324.    Elapsed: 0:04:40. Loss: 0.42990\n",
            "  Batch   680  of  1,324.    Elapsed: 0:04:42. Loss: 0.43049\n",
            "  Batch   685  of  1,324.    Elapsed: 0:04:44. Loss: 0.43158\n",
            "  Batch   690  of  1,324.    Elapsed: 0:04:46. Loss: 0.43138\n",
            "  Batch   695  of  1,324.    Elapsed: 0:04:48. Loss: 0.43140\n",
            "  Batch   700  of  1,324.    Elapsed: 0:04:50. Loss: 0.43122\n",
            "  Batch   705  of  1,324.    Elapsed: 0:04:52. Loss: 0.43174\n",
            "  Batch   710  of  1,324.    Elapsed: 0:04:54. Loss: 0.43244\n",
            "  Batch   715  of  1,324.    Elapsed: 0:04:56. Loss: 0.43173\n",
            "  Batch   720  of  1,324.    Elapsed: 0:04:58. Loss: 0.43184\n",
            "  Batch   725  of  1,324.    Elapsed: 0:05:00. Loss: 0.43143\n",
            "  Batch   730  of  1,324.    Elapsed: 0:05:02. Loss: 0.43006\n",
            "  Batch   735  of  1,324.    Elapsed: 0:05:04. Loss: 0.42916\n",
            "  Batch   740  of  1,324.    Elapsed: 0:05:07. Loss: 0.42829\n",
            "  Batch   745  of  1,324.    Elapsed: 0:05:09. Loss: 0.42785\n",
            "  Batch   750  of  1,324.    Elapsed: 0:05:11. Loss: 0.42829\n",
            "  Batch   755  of  1,324.    Elapsed: 0:05:13. Loss: 0.42996\n",
            "  Batch   760  of  1,324.    Elapsed: 0:05:15. Loss: 0.42913\n",
            "  Batch   765  of  1,324.    Elapsed: 0:05:17. Loss: 0.42795\n",
            "  Batch   770  of  1,324.    Elapsed: 0:05:19. Loss: 0.42725\n",
            "  Batch   775  of  1,324.    Elapsed: 0:05:21. Loss: 0.42605\n",
            "  Batch   780  of  1,324.    Elapsed: 0:05:23. Loss: 0.42606\n",
            "  Batch   785  of  1,324.    Elapsed: 0:05:25. Loss: 0.42516\n",
            "  Batch   790  of  1,324.    Elapsed: 0:05:27. Loss: 0.42490\n",
            "  Batch   795  of  1,324.    Elapsed: 0:05:29. Loss: 0.42585\n",
            "  Batch   800  of  1,324.    Elapsed: 0:05:31. Loss: 0.42579\n",
            "  Batch   805  of  1,324.    Elapsed: 0:05:33. Loss: 0.42712\n",
            "  Batch   810  of  1,324.    Elapsed: 0:05:36. Loss: 0.42656\n",
            "  Batch   815  of  1,324.    Elapsed: 0:05:38. Loss: 0.42630\n",
            "  Batch   820  of  1,324.    Elapsed: 0:05:40. Loss: 0.42619\n",
            "  Batch   825  of  1,324.    Elapsed: 0:05:42. Loss: 0.42507\n",
            "  Batch   830  of  1,324.    Elapsed: 0:05:44. Loss: 0.42470\n",
            "  Batch   835  of  1,324.    Elapsed: 0:05:46. Loss: 0.42444\n",
            "  Batch   840  of  1,324.    Elapsed: 0:05:48. Loss: 0.42441\n",
            "  Batch   845  of  1,324.    Elapsed: 0:05:50. Loss: 0.42525\n",
            "  Batch   850  of  1,324.    Elapsed: 0:05:52. Loss: 0.42460\n",
            "  Batch   855  of  1,324.    Elapsed: 0:05:54. Loss: 0.42541\n",
            "  Batch   860  of  1,324.    Elapsed: 0:05:56. Loss: 0.42537\n",
            "  Batch   865  of  1,324.    Elapsed: 0:05:58. Loss: 0.42483\n",
            "  Batch   870  of  1,324.    Elapsed: 0:06:01. Loss: 0.42411\n",
            "  Batch   875  of  1,324.    Elapsed: 0:06:03. Loss: 0.42370\n",
            "  Batch   880  of  1,324.    Elapsed: 0:06:05. Loss: 0.42383\n",
            "  Batch   885  of  1,324.    Elapsed: 0:06:07. Loss: 0.42262\n",
            "  Batch   890  of  1,324.    Elapsed: 0:06:09. Loss: 0.42294\n",
            "  Batch   895  of  1,324.    Elapsed: 0:06:11. Loss: 0.42361\n",
            "  Batch   900  of  1,324.    Elapsed: 0:06:13. Loss: 0.42302\n",
            "  Batch   905  of  1,324.    Elapsed: 0:06:15. Loss: 0.42209\n",
            "  Batch   910  of  1,324.    Elapsed: 0:06:17. Loss: 0.42254\n",
            "  Batch   915  of  1,324.    Elapsed: 0:06:19. Loss: 0.42313\n",
            "  Batch   920  of  1,324.    Elapsed: 0:06:21. Loss: 0.42295\n",
            "  Batch   925  of  1,324.    Elapsed: 0:06:23. Loss: 0.42328\n",
            "  Batch   930  of  1,324.    Elapsed: 0:06:26. Loss: 0.42304\n",
            "  Batch   935  of  1,324.    Elapsed: 0:06:28. Loss: 0.42243\n",
            "  Batch   940  of  1,324.    Elapsed: 0:06:30. Loss: 0.42205\n",
            "  Batch   945  of  1,324.    Elapsed: 0:06:32. Loss: 0.42215\n",
            "  Batch   950  of  1,324.    Elapsed: 0:06:34. Loss: 0.42206\n",
            "  Batch   955  of  1,324.    Elapsed: 0:06:36. Loss: 0.42202\n",
            "  Batch   960  of  1,324.    Elapsed: 0:06:38. Loss: 0.42189\n",
            "  Batch   965  of  1,324.    Elapsed: 0:06:40. Loss: 0.42201\n",
            "  Batch   970  of  1,324.    Elapsed: 0:06:42. Loss: 0.42193\n",
            "  Batch   975  of  1,324.    Elapsed: 0:06:44. Loss: 0.42146\n",
            "  Batch   980  of  1,324.    Elapsed: 0:06:46. Loss: 0.42157\n",
            "  Batch   985  of  1,324.    Elapsed: 0:06:48. Loss: 0.42285\n",
            "  Batch   990  of  1,324.    Elapsed: 0:06:51. Loss: 0.42298\n",
            "  Batch   995  of  1,324.    Elapsed: 0:06:53. Loss: 0.42332\n",
            "  Batch 1,000  of  1,324.    Elapsed: 0:06:55. Loss: 0.42284\n",
            "  Batch 1,005  of  1,324.    Elapsed: 0:06:57. Loss: 0.42399\n",
            "  Batch 1,010  of  1,324.    Elapsed: 0:06:59. Loss: 0.42429\n",
            "  Batch 1,015  of  1,324.    Elapsed: 0:07:01. Loss: 0.42420\n",
            "  Batch 1,020  of  1,324.    Elapsed: 0:07:03. Loss: 0.42375\n",
            "  Batch 1,025  of  1,324.    Elapsed: 0:07:05. Loss: 0.42368\n",
            "  Batch 1,030  of  1,324.    Elapsed: 0:07:07. Loss: 0.42376\n",
            "  Batch 1,035  of  1,324.    Elapsed: 0:07:09. Loss: 0.42360\n",
            "  Batch 1,040  of  1,324.    Elapsed: 0:07:11. Loss: 0.42478\n",
            "  Batch 1,045  of  1,324.    Elapsed: 0:07:13. Loss: 0.42472\n",
            "  Batch 1,050  of  1,324.    Elapsed: 0:07:16. Loss: 0.42505\n",
            "  Batch 1,055  of  1,324.    Elapsed: 0:07:18. Loss: 0.42604\n",
            "  Batch 1,060  of  1,324.    Elapsed: 0:07:20. Loss: 0.42585\n",
            "  Batch 1,065  of  1,324.    Elapsed: 0:07:22. Loss: 0.42670\n",
            "  Batch 1,070  of  1,324.    Elapsed: 0:07:24. Loss: 0.42672\n",
            "  Batch 1,075  of  1,324.    Elapsed: 0:07:26. Loss: 0.42632\n",
            "  Batch 1,080  of  1,324.    Elapsed: 0:07:28. Loss: 0.42612\n",
            "  Batch 1,085  of  1,324.    Elapsed: 0:07:30. Loss: 0.42639\n",
            "  Batch 1,090  of  1,324.    Elapsed: 0:07:32. Loss: 0.42604\n",
            "  Batch 1,095  of  1,324.    Elapsed: 0:07:34. Loss: 0.42603\n",
            "  Batch 1,100  of  1,324.    Elapsed: 0:07:36. Loss: 0.42589\n",
            "  Batch 1,105  of  1,324.    Elapsed: 0:07:38. Loss: 0.42634\n",
            "  Batch 1,110  of  1,324.    Elapsed: 0:07:40. Loss: 0.42716\n",
            "  Batch 1,115  of  1,324.    Elapsed: 0:07:43. Loss: 0.42740\n",
            "  Batch 1,120  of  1,324.    Elapsed: 0:07:45. Loss: 0.42750\n",
            "  Batch 1,125  of  1,324.    Elapsed: 0:07:47. Loss: 0.42674\n",
            "  Batch 1,130  of  1,324.    Elapsed: 0:07:49. Loss: 0.42639\n",
            "  Batch 1,135  of  1,324.    Elapsed: 0:07:51. Loss: 0.42604\n",
            "  Batch 1,140  of  1,324.    Elapsed: 0:07:53. Loss: 0.42590\n",
            "  Batch 1,145  of  1,324.    Elapsed: 0:07:55. Loss: 0.42565\n",
            "  Batch 1,150  of  1,324.    Elapsed: 0:07:57. Loss: 0.42615\n",
            "  Batch 1,155  of  1,324.    Elapsed: 0:07:59. Loss: 0.42630\n",
            "  Batch 1,160  of  1,324.    Elapsed: 0:08:01. Loss: 0.42598\n",
            "  Batch 1,165  of  1,324.    Elapsed: 0:08:03. Loss: 0.42659\n",
            "  Batch 1,170  of  1,324.    Elapsed: 0:08:05. Loss: 0.42615\n",
            "  Batch 1,175  of  1,324.    Elapsed: 0:08:08. Loss: 0.42644\n",
            "  Batch 1,180  of  1,324.    Elapsed: 0:08:10. Loss: 0.42659\n",
            "  Batch 1,185  of  1,324.    Elapsed: 0:08:12. Loss: 0.42633\n",
            "  Batch 1,190  of  1,324.    Elapsed: 0:08:14. Loss: 0.42645\n",
            "  Batch 1,195  of  1,324.    Elapsed: 0:08:16. Loss: 0.42710\n",
            "  Batch 1,200  of  1,324.    Elapsed: 0:08:18. Loss: 0.42716\n",
            "  Batch 1,205  of  1,324.    Elapsed: 0:08:20. Loss: 0.42725\n",
            "  Batch 1,210  of  1,324.    Elapsed: 0:08:22. Loss: 0.42796\n",
            "  Batch 1,215  of  1,324.    Elapsed: 0:08:24. Loss: 0.42802\n",
            "  Batch 1,220  of  1,324.    Elapsed: 0:08:26. Loss: 0.42819\n",
            "  Batch 1,225  of  1,324.    Elapsed: 0:08:29. Loss: 0.42840\n",
            "  Batch 1,230  of  1,324.    Elapsed: 0:08:31. Loss: 0.42821\n",
            "  Batch 1,235  of  1,324.    Elapsed: 0:08:33. Loss: 0.42748\n",
            "  Batch 1,240  of  1,324.    Elapsed: 0:08:35. Loss: 0.42735\n",
            "  Batch 1,245  of  1,324.    Elapsed: 0:08:37. Loss: 0.42737\n",
            "  Batch 1,250  of  1,324.    Elapsed: 0:08:39. Loss: 0.42736\n",
            "  Batch 1,255  of  1,324.    Elapsed: 0:08:41. Loss: 0.42723\n",
            "  Batch 1,260  of  1,324.    Elapsed: 0:08:43. Loss: 0.42721\n",
            "  Batch 1,265  of  1,324.    Elapsed: 0:08:45. Loss: 0.42736\n",
            "  Batch 1,270  of  1,324.    Elapsed: 0:08:47. Loss: 0.42791\n",
            "  Batch 1,275  of  1,324.    Elapsed: 0:08:50. Loss: 0.42797\n",
            "  Batch 1,280  of  1,324.    Elapsed: 0:08:52. Loss: 0.42718\n",
            "  Batch 1,285  of  1,324.    Elapsed: 0:08:54. Loss: 0.42699\n",
            "  Batch 1,290  of  1,324.    Elapsed: 0:08:56. Loss: 0.42728\n",
            "  Batch 1,295  of  1,324.    Elapsed: 0:08:58. Loss: 0.42712\n",
            "  Batch 1,300  of  1,324.    Elapsed: 0:09:00. Loss: 0.42700\n",
            "  Batch 1,305  of  1,324.    Elapsed: 0:09:02. Loss: 0.42676\n",
            "  Batch 1,310  of  1,324.    Elapsed: 0:09:04. Loss: 0.42630\n",
            "  Batch 1,315  of  1,324.    Elapsed: 0:09:06. Loss: 0.42599\n",
            "  Batch 1,320  of  1,324.    Elapsed: 0:09:08. Loss: 0.42624\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:09:10\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.53\n",
            "  Validation took: 0:00:51\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.78\n",
            "  Macro F1-score: 0.74\n",
            "  Macro F1-score: 0.74\n",
            "  Weighted F1-score: 0.77\n",
            "  Weighted F1-score: 0.77\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83      1736\n",
            "         1.0       0.69      0.62      0.65       912\n",
            "\n",
            "    accuracy                           0.77      2648\n",
            "   macro avg       0.75      0.74      0.74      2648\n",
            "weighted avg       0.77      0.77      0.77      2648\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.54      0.63       912\n",
            "         1.0       0.79      0.91      0.84      1736\n",
            "\n",
            "    accuracy                           0.78      2648\n",
            "   macro avg       0.77      0.72      0.74      2648\n",
            "weighted avg       0.78      0.78      0.77      2648\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1484  252]\n",
            " [ 346  566]]\n",
            "[[ 490  422]\n",
            " [ 158 1578]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'model_state_dict.pt'\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "QNKjmdCsbFJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W_Ms5p0eN-S",
        "outputId": "a0dcec7f-bf80-4a06-b26f-5e44b4cdd839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelPathDrive = '/content/drive/MyDrive/Bert.pt'\n",
        "torch.save(model.state_dict(), modelPathDrive)"
      ],
      "metadata": {
        "id": "-BBOSSdDbPOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/Bert.pt')\n",
        "print(state_dict.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7QUa60JbVbI",
        "outputId": "0c92ec2c-e585-4235-b4c1-4c44dc2e3201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['bertmodel.embeddings.position_ids', 'bertmodel.embeddings.word_embeddings.weight', 'bertmodel.embeddings.position_embeddings.weight', 'bertmodel.embeddings.token_type_embeddings.weight', 'bertmodel.embeddings.LayerNorm.weight', 'bertmodel.embeddings.LayerNorm.bias', 'bertmodel.encoder.embedding_hidden_mapping_in.weight', 'bertmodel.encoder.embedding_hidden_mapping_in.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'bertmodel.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'bertmodel.pooler.weight', 'bertmodel.pooler.bias', 'ffn1.weight', 'ffn1.bias', 'ffn2.weight', 'ffn2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzAeDFqSbZt4",
        "outputId": "6eaf8228-87e3-46be-fe41-31e9907bc8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgdDur7jbcX8",
        "outputId": "87c9b432-c5ff-4550-fe41-97ecfd4664cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model: \n",
            "\n",
            " MultiClassClassifier(\n",
            "  (bertmodel): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (ffn1): Linear(in_features=768, out_features=100, bias=True)\n",
            "  (dp1): Dropout(p=0.5, inplace=False)\n",
            "  (ffn2): Linear(in_features=100, out_features=2, bias=True)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['bertmodel.embeddings.position_ids', 'bertmodel.embeddings.word_embeddings.weight', 'bertmodel.embeddings.position_embeddings.weight', 'bertmodel.embeddings.token_type_embeddings.weight', 'bertmodel.embeddings.LayerNorm.weight', 'bertmodel.embeddings.LayerNorm.bias', 'bertmodel.encoder.layer.0.attention.self.query.weight', 'bertmodel.encoder.layer.0.attention.self.query.bias', 'bertmodel.encoder.layer.0.attention.self.key.weight', 'bertmodel.encoder.layer.0.attention.self.key.bias', 'bertmodel.encoder.layer.0.attention.self.value.weight', 'bertmodel.encoder.layer.0.attention.self.value.bias', 'bertmodel.encoder.layer.0.attention.output.dense.weight', 'bertmodel.encoder.layer.0.attention.output.dense.bias', 'bertmodel.encoder.layer.0.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.0.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.0.intermediate.dense.weight', 'bertmodel.encoder.layer.0.intermediate.dense.bias', 'bertmodel.encoder.layer.0.output.dense.weight', 'bertmodel.encoder.layer.0.output.dense.bias', 'bertmodel.encoder.layer.0.output.LayerNorm.weight', 'bertmodel.encoder.layer.0.output.LayerNorm.bias', 'bertmodel.encoder.layer.1.attention.self.query.weight', 'bertmodel.encoder.layer.1.attention.self.query.bias', 'bertmodel.encoder.layer.1.attention.self.key.weight', 'bertmodel.encoder.layer.1.attention.self.key.bias', 'bertmodel.encoder.layer.1.attention.self.value.weight', 'bertmodel.encoder.layer.1.attention.self.value.bias', 'bertmodel.encoder.layer.1.attention.output.dense.weight', 'bertmodel.encoder.layer.1.attention.output.dense.bias', 'bertmodel.encoder.layer.1.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.1.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.1.intermediate.dense.weight', 'bertmodel.encoder.layer.1.intermediate.dense.bias', 'bertmodel.encoder.layer.1.output.dense.weight', 'bertmodel.encoder.layer.1.output.dense.bias', 'bertmodel.encoder.layer.1.output.LayerNorm.weight', 'bertmodel.encoder.layer.1.output.LayerNorm.bias', 'bertmodel.encoder.layer.2.attention.self.query.weight', 'bertmodel.encoder.layer.2.attention.self.query.bias', 'bertmodel.encoder.layer.2.attention.self.key.weight', 'bertmodel.encoder.layer.2.attention.self.key.bias', 'bertmodel.encoder.layer.2.attention.self.value.weight', 'bertmodel.encoder.layer.2.attention.self.value.bias', 'bertmodel.encoder.layer.2.attention.output.dense.weight', 'bertmodel.encoder.layer.2.attention.output.dense.bias', 'bertmodel.encoder.layer.2.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.2.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.2.intermediate.dense.weight', 'bertmodel.encoder.layer.2.intermediate.dense.bias', 'bertmodel.encoder.layer.2.output.dense.weight', 'bertmodel.encoder.layer.2.output.dense.bias', 'bertmodel.encoder.layer.2.output.LayerNorm.weight', 'bertmodel.encoder.layer.2.output.LayerNorm.bias', 'bertmodel.encoder.layer.3.attention.self.query.weight', 'bertmodel.encoder.layer.3.attention.self.query.bias', 'bertmodel.encoder.layer.3.attention.self.key.weight', 'bertmodel.encoder.layer.3.attention.self.key.bias', 'bertmodel.encoder.layer.3.attention.self.value.weight', 'bertmodel.encoder.layer.3.attention.self.value.bias', 'bertmodel.encoder.layer.3.attention.output.dense.weight', 'bertmodel.encoder.layer.3.attention.output.dense.bias', 'bertmodel.encoder.layer.3.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.3.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.3.intermediate.dense.weight', 'bertmodel.encoder.layer.3.intermediate.dense.bias', 'bertmodel.encoder.layer.3.output.dense.weight', 'bertmodel.encoder.layer.3.output.dense.bias', 'bertmodel.encoder.layer.3.output.LayerNorm.weight', 'bertmodel.encoder.layer.3.output.LayerNorm.bias', 'bertmodel.encoder.layer.4.attention.self.query.weight', 'bertmodel.encoder.layer.4.attention.self.query.bias', 'bertmodel.encoder.layer.4.attention.self.key.weight', 'bertmodel.encoder.layer.4.attention.self.key.bias', 'bertmodel.encoder.layer.4.attention.self.value.weight', 'bertmodel.encoder.layer.4.attention.self.value.bias', 'bertmodel.encoder.layer.4.attention.output.dense.weight', 'bertmodel.encoder.layer.4.attention.output.dense.bias', 'bertmodel.encoder.layer.4.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.4.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.4.intermediate.dense.weight', 'bertmodel.encoder.layer.4.intermediate.dense.bias', 'bertmodel.encoder.layer.4.output.dense.weight', 'bertmodel.encoder.layer.4.output.dense.bias', 'bertmodel.encoder.layer.4.output.LayerNorm.weight', 'bertmodel.encoder.layer.4.output.LayerNorm.bias', 'bertmodel.encoder.layer.5.attention.self.query.weight', 'bertmodel.encoder.layer.5.attention.self.query.bias', 'bertmodel.encoder.layer.5.attention.self.key.weight', 'bertmodel.encoder.layer.5.attention.self.key.bias', 'bertmodel.encoder.layer.5.attention.self.value.weight', 'bertmodel.encoder.layer.5.attention.self.value.bias', 'bertmodel.encoder.layer.5.attention.output.dense.weight', 'bertmodel.encoder.layer.5.attention.output.dense.bias', 'bertmodel.encoder.layer.5.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.5.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.5.intermediate.dense.weight', 'bertmodel.encoder.layer.5.intermediate.dense.bias', 'bertmodel.encoder.layer.5.output.dense.weight', 'bertmodel.encoder.layer.5.output.dense.bias', 'bertmodel.encoder.layer.5.output.LayerNorm.weight', 'bertmodel.encoder.layer.5.output.LayerNorm.bias', 'bertmodel.encoder.layer.6.attention.self.query.weight', 'bertmodel.encoder.layer.6.attention.self.query.bias', 'bertmodel.encoder.layer.6.attention.self.key.weight', 'bertmodel.encoder.layer.6.attention.self.key.bias', 'bertmodel.encoder.layer.6.attention.self.value.weight', 'bertmodel.encoder.layer.6.attention.self.value.bias', 'bertmodel.encoder.layer.6.attention.output.dense.weight', 'bertmodel.encoder.layer.6.attention.output.dense.bias', 'bertmodel.encoder.layer.6.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.6.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.6.intermediate.dense.weight', 'bertmodel.encoder.layer.6.intermediate.dense.bias', 'bertmodel.encoder.layer.6.output.dense.weight', 'bertmodel.encoder.layer.6.output.dense.bias', 'bertmodel.encoder.layer.6.output.LayerNorm.weight', 'bertmodel.encoder.layer.6.output.LayerNorm.bias', 'bertmodel.encoder.layer.7.attention.self.query.weight', 'bertmodel.encoder.layer.7.attention.self.query.bias', 'bertmodel.encoder.layer.7.attention.self.key.weight', 'bertmodel.encoder.layer.7.attention.self.key.bias', 'bertmodel.encoder.layer.7.attention.self.value.weight', 'bertmodel.encoder.layer.7.attention.self.value.bias', 'bertmodel.encoder.layer.7.attention.output.dense.weight', 'bertmodel.encoder.layer.7.attention.output.dense.bias', 'bertmodel.encoder.layer.7.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.7.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.7.intermediate.dense.weight', 'bertmodel.encoder.layer.7.intermediate.dense.bias', 'bertmodel.encoder.layer.7.output.dense.weight', 'bertmodel.encoder.layer.7.output.dense.bias', 'bertmodel.encoder.layer.7.output.LayerNorm.weight', 'bertmodel.encoder.layer.7.output.LayerNorm.bias', 'bertmodel.encoder.layer.8.attention.self.query.weight', 'bertmodel.encoder.layer.8.attention.self.query.bias', 'bertmodel.encoder.layer.8.attention.self.key.weight', 'bertmodel.encoder.layer.8.attention.self.key.bias', 'bertmodel.encoder.layer.8.attention.self.value.weight', 'bertmodel.encoder.layer.8.attention.self.value.bias', 'bertmodel.encoder.layer.8.attention.output.dense.weight', 'bertmodel.encoder.layer.8.attention.output.dense.bias', 'bertmodel.encoder.layer.8.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.8.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.8.intermediate.dense.weight', 'bertmodel.encoder.layer.8.intermediate.dense.bias', 'bertmodel.encoder.layer.8.output.dense.weight', 'bertmodel.encoder.layer.8.output.dense.bias', 'bertmodel.encoder.layer.8.output.LayerNorm.weight', 'bertmodel.encoder.layer.8.output.LayerNorm.bias', 'bertmodel.encoder.layer.9.attention.self.query.weight', 'bertmodel.encoder.layer.9.attention.self.query.bias', 'bertmodel.encoder.layer.9.attention.self.key.weight', 'bertmodel.encoder.layer.9.attention.self.key.bias', 'bertmodel.encoder.layer.9.attention.self.value.weight', 'bertmodel.encoder.layer.9.attention.self.value.bias', 'bertmodel.encoder.layer.9.attention.output.dense.weight', 'bertmodel.encoder.layer.9.attention.output.dense.bias', 'bertmodel.encoder.layer.9.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.9.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.9.intermediate.dense.weight', 'bertmodel.encoder.layer.9.intermediate.dense.bias', 'bertmodel.encoder.layer.9.output.dense.weight', 'bertmodel.encoder.layer.9.output.dense.bias', 'bertmodel.encoder.layer.9.output.LayerNorm.weight', 'bertmodel.encoder.layer.9.output.LayerNorm.bias', 'bertmodel.encoder.layer.10.attention.self.query.weight', 'bertmodel.encoder.layer.10.attention.self.query.bias', 'bertmodel.encoder.layer.10.attention.self.key.weight', 'bertmodel.encoder.layer.10.attention.self.key.bias', 'bertmodel.encoder.layer.10.attention.self.value.weight', 'bertmodel.encoder.layer.10.attention.self.value.bias', 'bertmodel.encoder.layer.10.attention.output.dense.weight', 'bertmodel.encoder.layer.10.attention.output.dense.bias', 'bertmodel.encoder.layer.10.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.10.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.10.intermediate.dense.weight', 'bertmodel.encoder.layer.10.intermediate.dense.bias', 'bertmodel.encoder.layer.10.output.dense.weight', 'bertmodel.encoder.layer.10.output.dense.bias', 'bertmodel.encoder.layer.10.output.LayerNorm.weight', 'bertmodel.encoder.layer.10.output.LayerNorm.bias', 'bertmodel.encoder.layer.11.attention.self.query.weight', 'bertmodel.encoder.layer.11.attention.self.query.bias', 'bertmodel.encoder.layer.11.attention.self.key.weight', 'bertmodel.encoder.layer.11.attention.self.key.bias', 'bertmodel.encoder.layer.11.attention.self.value.weight', 'bertmodel.encoder.layer.11.attention.self.value.bias', 'bertmodel.encoder.layer.11.attention.output.dense.weight', 'bertmodel.encoder.layer.11.attention.output.dense.bias', 'bertmodel.encoder.layer.11.attention.output.LayerNorm.weight', 'bertmodel.encoder.layer.11.attention.output.LayerNorm.bias', 'bertmodel.encoder.layer.11.intermediate.dense.weight', 'bertmodel.encoder.layer.11.intermediate.dense.bias', 'bertmodel.encoder.layer.11.output.dense.weight', 'bertmodel.encoder.layer.11.output.dense.bias', 'bertmodel.encoder.layer.11.output.LayerNorm.weight', 'bertmodel.encoder.layer.11.output.LayerNorm.bias', 'bertmodel.pooler.dense.weight', 'bertmodel.pooler.dense.bias', 'ffn1.weight', 'ffn1.bias', 'ffn2.weight', 'ffn2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/hindi_dataset.tsv',sep='\\t')\n",
        "df"
      ],
      "metadata": {
        "id": "dezNA2_8DbGj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "00855145-0d26-4eeb-ca52-e136829ff854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            text_id                                               text task_1  \\\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
              "...             ...                                                ...    ...   \n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT   \n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
              "\n",
              "     task_2 task_3  \n",
              "0      NONE   NONE  \n",
              "1      PRFN    UNT  \n",
              "2      PRFN    TIN  \n",
              "3      NONE   NONE  \n",
              "4      NONE   NONE  \n",
              "...     ...    ...  \n",
              "4660   NONE   NONE  \n",
              "4661   PRFN    TIN  \n",
              "4662   NONE   NONE  \n",
              "4663   HATE    TIN  \n",
              "4664   HATE    TIN  \n",
              "\n",
              "[4665 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-402ce882-83ec-462b-a22e-ca4b62626d72\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>UNT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-402ce882-83ec-462b-a22e-ca4b62626d72')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-402ce882-83ec-462b-a22e-ca4b62626d72 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-402ce882-83ec-462b-a22e-ca4b62626d72');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['task_2','task_3'],inplace=True,axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "gTKDz7Jj6q-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "d4e6141b-9252-49af-e2ee-70bd0d9d62d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            text_id                                               text task_1\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT\n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF\n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF\n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT\n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT\n",
              "...             ...                                                ...    ...\n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT\n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF\n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT\n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF\n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF\n",
              "\n",
              "[4665 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0be7cd04-0227-4456-bd6f-1591f56382ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0be7cd04-0227-4456-bd6f-1591f56382ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0be7cd04-0227-4456-bd6f-1591f56382ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0be7cd04-0227-4456-bd6f-1591f56382ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.assign(Offensive=0)\n",
        "df1"
      ],
      "metadata": {
        "id": "I2_DD0bC6tB4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "4f9ff904-e69f-4723-fe5f-bb1d3c101187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            text_id                                               text task_1  \\\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
              "...             ...                                                ...    ...   \n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT   \n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
              "\n",
              "      Offensive  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             0  \n",
              "4             0  \n",
              "...         ...  \n",
              "4660          0  \n",
              "4661          0  \n",
              "4662          0  \n",
              "4663          0  \n",
              "4664          0  \n",
              "\n",
              "[4665 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-458be9ec-be6c-440b-aa09-a22c598c8f83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-458be9ec-be6c-440b-aa09-a22c598c8f83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-458be9ec-be6c-440b-aa09-a22c598c8f83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-458be9ec-be6c-440b-aa09-a22c598c8f83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df1.assign(NotOffensive=0)\n",
        "train_df"
      ],
      "metadata": {
        "id": "qvtGLMsz6vOu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "d328c3b1-cbfb-4eec-d4d8-51c07cba6f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            text_id                                               text task_1  \\\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
              "...             ...                                                ...    ...   \n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT   \n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
              "\n",
              "      Offensive  NotOffensive  \n",
              "0             0             0  \n",
              "1             0             0  \n",
              "2             0             0  \n",
              "3             0             0  \n",
              "4             0             0  \n",
              "...         ...           ...  \n",
              "4660          0             0  \n",
              "4661          0             0  \n",
              "4662          0             0  \n",
              "4663          0             0  \n",
              "4664          0             0  \n",
              "\n",
              "[4665 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52b8987e-d829-4423-a05f-bc42b2c101ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52b8987e-d829-4423-a05f-bc42b2c101ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52b8987e-d829-4423-a05f-bc42b2c101ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52b8987e-d829-4423-a05f-bc42b2c101ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index in train_df.index:\n",
        "    k = train_df['task_1'][index]\n",
        "    if k == 'HOF':\n",
        "        train_df['Offensive'][index] = 1\n",
        "        train_df['NotOffensive'][index] = 0\n",
        "    else:\n",
        "        train_df['Offensive'][index] = 0\n",
        "        train_df['NotOffensive'][index] = 1\n",
        "train_df"
      ],
      "metadata": {
        "id": "AJZwBZYM6xYT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "outputId": "2a2e86cc-460d-4cac-d7cf-c4ee848debe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-4b42d279e6cb>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 0\n",
            "<ipython-input-42-4b42d279e6cb>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 1\n",
            "<ipython-input-42-4b42d279e6cb>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Offensive'][index] = 1\n",
            "<ipython-input-42-4b42d279e6cb>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['NotOffensive'][index] = 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            text_id                                               text task_1  \\\n",
              "0     hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
              "1     hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
              "2      hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
              "3     hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
              "4     hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
              "...             ...                                                ...    ...   \n",
              "4660  hasoc_hi_6606  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT   \n",
              "4661  hasoc_hi_4931  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF   \n",
              "4662  hasoc_hi_1059      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT   \n",
              "4663  hasoc_hi_5429  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF   \n",
              "4664  hasoc_hi_1656  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF   \n",
              "\n",
              "      Offensive  NotOffensive  \n",
              "0             0             1  \n",
              "1             1             0  \n",
              "2             1             0  \n",
              "3             0             1  \n",
              "4             0             1  \n",
              "...         ...           ...  \n",
              "4660          0             1  \n",
              "4661          1             0  \n",
              "4662          0             1  \n",
              "4663          1             0  \n",
              "4664          1             0  \n",
              "\n",
              "[4665 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07f8e3dd-6d68-4dbb-ab0f-d4478fa6799f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>hasoc_hi_6606</td>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>hasoc_hi_4931</td>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>hasoc_hi_1059</td>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>hasoc_hi_5429</td>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>hasoc_hi_1656</td>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07f8e3dd-6d68-4dbb-ab0f-d4478fa6799f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07f8e3dd-6d68-4dbb-ab0f-d4478fa6799f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07f8e3dd-6d68-4dbb-ab0f-d4478fa6799f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(['text_id'],inplace=True,axis=1)\n",
        "train_df"
      ],
      "metadata": {
        "id": "-VmWtes36048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "3eecc5bc-1a7e-48e5-dadc-eb501e4bb47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          1   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          1   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "4660  पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...    NOT          0   \n",
              "4661  कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...    HOF          1   \n",
              "4662      परशुराम? वही जिसने अपनी मां की हत्या की थीं?     NOT          0   \n",
              "4663  जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...    HOF          1   \n",
              "4664  इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...    HOF          1   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "4660             1  \n",
              "4661             0  \n",
              "4662             1  \n",
              "4663             0  \n",
              "4664             0  \n",
              "\n",
              "[4665 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f8b97ce-d4d3-4a33-a169-65f962d8b2cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>पाकिस्तान ने हिंदुओं के ख़िलाफ़ बोलने वाले को ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4661</th>\n",
              "      <td>कोहली है #नेहरू नहीं जो अंग्रेजों के तलवे चाटन...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>परशुराम? वही जिसने अपनी मां की हत्या की थीं?</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4663</th>\n",
              "      <td>जिस देश में #कन्हैया_कुमार जैसा पढ़ा लिखा युवा...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4664</th>\n",
              "      <td>इनके बापों मैं भी दम नहीं जो भारत को इस्लामिक ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4665 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f8b97ce-d4d3-4a33-a169-65f962d8b2cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f8b97ce-d4d3-4a33-a169-65f962d8b2cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f8b97ce-d4d3-4a33-a169-65f962d8b2cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "Xr30G69g61ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff970be-5d90-438f-ce3b-620e0316fd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'task_1', 'Offensive', 'NotOffensive'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('drive/MyDrive/final_hindi_backtranslated.csv')\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PYvcfKHgpec7",
        "outputId": "9f7f83ec-80ad-4c6b-e903-a70920fb4365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text task_1  Offensive  \\\n",
              "0     बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT          0   \n",
              "1     सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF          1   \n",
              "2     तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF          1   \n",
              "3     बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT          0   \n",
              "4     चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT          0   \n",
              "...                                                 ...    ...        ...   \n",
              "6060                                 कश्मीर भगवान से है    HOF          1   \n",
              "6061  अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...    NOT          0   \n",
              "6062     ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...    NOT          0   \n",
              "6063  90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...    HOF          1   \n",
              "6064  करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...    NOT          0   \n",
              "\n",
              "      NotOffensive  \n",
              "0                1  \n",
              "1                0  \n",
              "2                0  \n",
              "3                1  \n",
              "4                1  \n",
              "...            ...  \n",
              "6060             0  \n",
              "6061             1  \n",
              "6062             1  \n",
              "6063             0  \n",
              "6064             1  \n",
              "\n",
              "[6065 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7f6413e-b04a-47bc-898c-c3885d5d6481\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>NotOffensive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6060</th>\n",
              "      <td>कश्मीर भगवान से है</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061</th>\n",
              "      <td>अंग्रेजों के खिलाफ पहला संगठित संघर्ष रानी लक्...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6062</th>\n",
              "      <td>ऑटो से मोबाइल तक टेक की 5 बड़ी खबरें जानें ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6063</th>\n",
              "      <td>90 % लोग पहले से ही जानते थे कि भारत मैच हार ज...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6064</th>\n",
              "      <td>करतपुर कॉरिडोर: इंडो-पाक में 11 और 14 जुलाई के...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6065 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7f6413e-b04a-47bc-898c-c3885d5d6481')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7f6413e-b04a-47bc-898c-c3885d5d6481 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7f6413e-b04a-47bc-898c-c3885d5d6481');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = train_df.columns[2:]\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, train_df[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ],
      "metadata": {
        "id": "C_ODsSFK63ot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "b4f9cf08-fcc4-4901-e508-fc1626a6b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       category  number of comments\n",
              "0     Offensive                2888\n",
              "1  NotOffensive                3177"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dba73abd-abe9-4012-85cc-92c4b5efaa91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>number of comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>2888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotOffensive</td>\n",
              "      <td>3177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dba73abd-abe9-4012-85cc-92c4b5efaa91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dba73abd-abe9-4012-85cc-92c4b5efaa91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dba73abd-abe9-4012-85cc-92c4b5efaa91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_list = categories"
      ],
      "metadata": {
        "id": "AOzQ1t6k65kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)"
      ],
      "metadata": {
        "id": "V3tm6cXy6_7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "68a26b0806f44c229bc5c258a594ad24",
            "5c439dfbc6d347c797fa7adca24a069e",
            "eff7c684d83b4244b0484a11da4b6bd9",
            "3ecbc28f922c4075b25aedaebb80da9a",
            "6c8e635ddcae437ba1de064e83e03f1a",
            "941313d64f0c498db2b1f9eeef072d47",
            "f6ce81f0b94d48ee967b9ff829c49991",
            "460c8e4124f6478fa2c1367b6e223f6e",
            "e20d5cf2df4b4b898472f6f66656f458",
            "56bf15672f924f15bd44280259315f45",
            "3919a7b51b5545baa487380b4e71dada",
            "0f0a9bc795de410a8f6b0ecba4976de6",
            "e57737ace7ab41e58c4cd0de182bafdc",
            "ce738540b95c4b83a3fc778e1d7770e5",
            "5a14c49adb9640e483a6f8ce329375c6",
            "08fd565fc93e4f3c91d70a4cf61969cf",
            "88d9d37523324f2cac1e24e633d48013",
            "3a8cb4f0ad8b4341ba64e4908fe75606",
            "fd1d2233ab05405f9bb1c53e0665dea4",
            "47ea14d885e147d3ac3bdde866a5d3aa",
            "f93abd57a8844c35b6722295b16e9365",
            "a2ccf19fae7e4e5f94739d728c17665f",
            "3cc9b9a993c84a9e82cd238f5e9f6cd9",
            "6dc300ab3b744ac0b5b69907b2e518ef",
            "8a5abe67b79a4826a42ef51b9d501cf8",
            "0ce05cdf1ab94b77a22f59cc907cc4ad",
            "9ab9f8e79b9d426da6fa9c31047904eb",
            "75fd1cc8a565411c90befd36e50b2319",
            "98bfa9a9119c420cbfdc840029667630",
            "c8766b9a5ba1400093db09ed6113a832",
            "498bad5849de4d9a980e17b27ffa175c",
            "517a1532998a407c80d942f21fd17dbf",
            "fd52adc93346404b9fd07bb8dd80f5ac"
          ]
        },
        "outputId": "075c80ce-0428-4d74-9a08-0542799428fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68a26b0806f44c229bc5c258a594ad24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f0a9bc795de410a8f6b0ecba4976de6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cc9b9a993c84a9e82cd238f5e9f6cd9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"
      ],
      "metadata": {
        "id": "LrLG68M43ih3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ],
      "metadata": {
        "id": "mUdjDIuH7Ako"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['text'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "3IZ7fh0a7CVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ],
      "metadata": {
        "id": "kitz7mVQ7EEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea20347-cee8-48dd-dd10-826494e027e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Input:  torch.Size([6065, 300])\n",
            "Text Input Attention:  torch.Size([6065, 300])\n",
            "Labels:  torch.Size([6065, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhACtNQ3pfs",
        "outputId": "eeb212b9-f0d5-4b57-afbc-cf916b93ddd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Input:  torch.Size([6065, 300])\n",
            "Text Input Attention:  torch.Size([6065, 300])\n",
            "Labels:  torch.Size([6065, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ],
      "metadata": {
        "id": "vcPYdaoe7Gg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97adf4b-f15a-49a3-cf88-bea3b3e93092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING and VALIDATION\n",
        "epochs = 3   #5, reduced to one epoch as it is taking lot of time\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,2))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(2)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(2):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(2):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(2):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(2)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(2)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(2)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "6WX6Dr1E7IeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f51ad4-e845-4d70-a5fd-bed4d459e383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 1.00029\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.91382\n",
            "  Batch    15  of    607.    Elapsed: 0:00:06. Loss: 0.90042\n",
            "  Batch    20  of    607.    Elapsed: 0:00:07. Loss: 0.84139\n",
            "  Batch    25  of    607.    Elapsed: 0:00:09. Loss: 0.82332\n",
            "  Batch    30  of    607.    Elapsed: 0:00:11. Loss: 0.79778\n",
            "  Batch    35  of    607.    Elapsed: 0:00:13. Loss: 0.76302\n",
            "  Batch    40  of    607.    Elapsed: 0:00:15. Loss: 0.74490\n",
            "  Batch    45  of    607.    Elapsed: 0:00:17. Loss: 0.72824\n",
            "  Batch    50  of    607.    Elapsed: 0:00:19. Loss: 0.72678\n",
            "  Batch    55  of    607.    Elapsed: 0:00:21. Loss: 0.70282\n",
            "  Batch    60  of    607.    Elapsed: 0:00:23. Loss: 0.69676\n",
            "  Batch    65  of    607.    Elapsed: 0:00:24. Loss: 0.68650\n",
            "  Batch    70  of    607.    Elapsed: 0:00:26. Loss: 0.67165\n",
            "  Batch    75  of    607.    Elapsed: 0:00:28. Loss: 0.66721\n",
            "  Batch    80  of    607.    Elapsed: 0:00:30. Loss: 0.66550\n",
            "  Batch    85  of    607.    Elapsed: 0:00:32. Loss: 0.66101\n",
            "  Batch    90  of    607.    Elapsed: 0:00:34. Loss: 0.65240\n",
            "  Batch    95  of    607.    Elapsed: 0:00:36. Loss: 0.64928\n",
            "  Batch   100  of    607.    Elapsed: 0:00:38. Loss: 0.64671\n",
            "  Batch   105  of    607.    Elapsed: 0:00:40. Loss: 0.64351\n",
            "  Batch   110  of    607.    Elapsed: 0:00:42. Loss: 0.64513\n",
            "  Batch   115  of    607.    Elapsed: 0:00:44. Loss: 0.64113\n",
            "  Batch   120  of    607.    Elapsed: 0:00:46. Loss: 0.63449\n",
            "  Batch   125  of    607.    Elapsed: 0:00:48. Loss: 0.63768\n",
            "  Batch   130  of    607.    Elapsed: 0:00:50. Loss: 0.63576\n",
            "  Batch   135  of    607.    Elapsed: 0:00:52. Loss: 0.63676\n",
            "  Batch   140  of    607.    Elapsed: 0:00:53. Loss: 0.63558\n",
            "  Batch   145  of    607.    Elapsed: 0:00:55. Loss: 0.63866\n",
            "  Batch   150  of    607.    Elapsed: 0:00:57. Loss: 0.63797\n",
            "  Batch   155  of    607.    Elapsed: 0:00:59. Loss: 0.63765\n",
            "  Batch   160  of    607.    Elapsed: 0:01:01. Loss: 0.63473\n",
            "  Batch   165  of    607.    Elapsed: 0:01:03. Loss: 0.63514\n",
            "  Batch   170  of    607.    Elapsed: 0:01:05. Loss: 0.63722\n",
            "  Batch   175  of    607.    Elapsed: 0:01:07. Loss: 0.63649\n",
            "  Batch   180  of    607.    Elapsed: 0:01:09. Loss: 0.63474\n",
            "  Batch   185  of    607.    Elapsed: 0:01:11. Loss: 0.63495\n",
            "  Batch   190  of    607.    Elapsed: 0:01:13. Loss: 0.63259\n",
            "  Batch   195  of    607.    Elapsed: 0:01:15. Loss: 0.63033\n",
            "  Batch   200  of    607.    Elapsed: 0:01:17. Loss: 0.62899\n",
            "  Batch   205  of    607.    Elapsed: 0:01:19. Loss: 0.62794\n",
            "  Batch   210  of    607.    Elapsed: 0:01:21. Loss: 0.62903\n",
            "  Batch   215  of    607.    Elapsed: 0:01:23. Loss: 0.62544\n",
            "  Batch   220  of    607.    Elapsed: 0:01:25. Loss: 0.62227\n",
            "  Batch   225  of    607.    Elapsed: 0:01:27. Loss: 0.62419\n",
            "  Batch   230  of    607.    Elapsed: 0:01:29. Loss: 0.62167\n",
            "  Batch   235  of    607.    Elapsed: 0:01:31. Loss: 0.62198\n",
            "  Batch   240  of    607.    Elapsed: 0:01:33. Loss: 0.62187\n",
            "  Batch   245  of    607.    Elapsed: 0:01:35. Loss: 0.62073\n",
            "  Batch   250  of    607.    Elapsed: 0:01:37. Loss: 0.61954\n",
            "  Batch   255  of    607.    Elapsed: 0:01:39. Loss: 0.62068\n",
            "  Batch   260  of    607.    Elapsed: 0:01:41. Loss: 0.61866\n",
            "  Batch   265  of    607.    Elapsed: 0:01:43. Loss: 0.61873\n",
            "  Batch   270  of    607.    Elapsed: 0:01:45. Loss: 0.61793\n",
            "  Batch   275  of    607.    Elapsed: 0:01:47. Loss: 0.61562\n",
            "  Batch   280  of    607.    Elapsed: 0:01:49. Loss: 0.61583\n",
            "  Batch   285  of    607.    Elapsed: 0:01:51. Loss: 0.61690\n",
            "  Batch   290  of    607.    Elapsed: 0:01:53. Loss: 0.61319\n",
            "  Batch   295  of    607.    Elapsed: 0:01:55. Loss: 0.61363\n",
            "  Batch   300  of    607.    Elapsed: 0:01:57. Loss: 0.61347\n",
            "  Batch   305  of    607.    Elapsed: 0:01:59. Loss: 0.61182\n",
            "  Batch   310  of    607.    Elapsed: 0:02:01. Loss: 0.61060\n",
            "  Batch   315  of    607.    Elapsed: 0:02:03. Loss: 0.60859\n",
            "  Batch   320  of    607.    Elapsed: 0:02:05. Loss: 0.61106\n",
            "  Batch   325  of    607.    Elapsed: 0:02:07. Loss: 0.60955\n",
            "  Batch   330  of    607.    Elapsed: 0:02:09. Loss: 0.60935\n",
            "  Batch   335  of    607.    Elapsed: 0:02:11. Loss: 0.60902\n",
            "  Batch   340  of    607.    Elapsed: 0:02:13. Loss: 0.60695\n",
            "  Batch   345  of    607.    Elapsed: 0:02:15. Loss: 0.60560\n",
            "  Batch   350  of    607.    Elapsed: 0:02:17. Loss: 0.60537\n",
            "  Batch   355  of    607.    Elapsed: 0:02:19. Loss: 0.60599\n",
            "  Batch   360  of    607.    Elapsed: 0:02:21. Loss: 0.60419\n",
            "  Batch   365  of    607.    Elapsed: 0:02:23. Loss: 0.60223\n",
            "  Batch   370  of    607.    Elapsed: 0:02:25. Loss: 0.60062\n",
            "  Batch   375  of    607.    Elapsed: 0:02:27. Loss: 0.60037\n",
            "  Batch   380  of    607.    Elapsed: 0:02:29. Loss: 0.59919\n",
            "  Batch   385  of    607.    Elapsed: 0:02:31. Loss: 0.59871\n",
            "  Batch   390  of    607.    Elapsed: 0:02:33. Loss: 0.59560\n",
            "  Batch   395  of    607.    Elapsed: 0:02:35. Loss: 0.59364\n",
            "  Batch   400  of    607.    Elapsed: 0:02:37. Loss: 0.59220\n",
            "  Batch   405  of    607.    Elapsed: 0:02:39. Loss: 0.59264\n",
            "  Batch   410  of    607.    Elapsed: 0:02:41. Loss: 0.59517\n",
            "  Batch   415  of    607.    Elapsed: 0:02:43. Loss: 0.59368\n",
            "  Batch   420  of    607.    Elapsed: 0:02:45. Loss: 0.59431\n",
            "  Batch   425  of    607.    Elapsed: 0:02:47. Loss: 0.59438\n",
            "  Batch   430  of    607.    Elapsed: 0:02:49. Loss: 0.59355\n",
            "  Batch   435  of    607.    Elapsed: 0:02:51. Loss: 0.59404\n",
            "  Batch   440  of    607.    Elapsed: 0:02:53. Loss: 0.59410\n",
            "  Batch   445  of    607.    Elapsed: 0:02:55. Loss: 0.59420\n",
            "  Batch   450  of    607.    Elapsed: 0:02:57. Loss: 0.59252\n",
            "  Batch   455  of    607.    Elapsed: 0:02:59. Loss: 0.59080\n",
            "  Batch   460  of    607.    Elapsed: 0:03:01. Loss: 0.58948\n",
            "  Batch   465  of    607.    Elapsed: 0:03:03. Loss: 0.58900\n",
            "  Batch   470  of    607.    Elapsed: 0:03:05. Loss: 0.58840\n",
            "  Batch   475  of    607.    Elapsed: 0:03:07. Loss: 0.58784\n",
            "  Batch   480  of    607.    Elapsed: 0:03:09. Loss: 0.58663\n",
            "  Batch   485  of    607.    Elapsed: 0:03:11. Loss: 0.58495\n",
            "  Batch   490  of    607.    Elapsed: 0:03:13. Loss: 0.58350\n",
            "  Batch   495  of    607.    Elapsed: 0:03:16. Loss: 0.58143\n",
            "  Batch   500  of    607.    Elapsed: 0:03:18. Loss: 0.58237\n",
            "  Batch   505  of    607.    Elapsed: 0:03:20. Loss: 0.58332\n",
            "  Batch   510  of    607.    Elapsed: 0:03:22. Loss: 0.58355\n",
            "  Batch   515  of    607.    Elapsed: 0:03:24. Loss: 0.58230\n",
            "  Batch   520  of    607.    Elapsed: 0:03:26. Loss: 0.58000\n",
            "  Batch   525  of    607.    Elapsed: 0:03:28. Loss: 0.57914\n",
            "  Batch   530  of    607.    Elapsed: 0:03:30. Loss: 0.57718\n",
            "  Batch   535  of    607.    Elapsed: 0:03:32. Loss: 0.57605\n",
            "  Batch   540  of    607.    Elapsed: 0:03:34. Loss: 0.57557\n",
            "  Batch   545  of    607.    Elapsed: 0:03:36. Loss: 0.57404\n",
            "  Batch   550  of    607.    Elapsed: 0:03:38. Loss: 0.57350\n",
            "  Batch   555  of    607.    Elapsed: 0:03:40. Loss: 0.57257\n",
            "  Batch   560  of    607.    Elapsed: 0:03:42. Loss: 0.57116\n",
            "  Batch   565  of    607.    Elapsed: 0:03:44. Loss: 0.57055\n",
            "  Batch   570  of    607.    Elapsed: 0:03:46. Loss: 0.57116\n",
            "  Batch   575  of    607.    Elapsed: 0:03:48. Loss: 0.56932\n",
            "  Batch   580  of    607.    Elapsed: 0:03:51. Loss: 0.56831\n",
            "  Batch   585  of    607.    Elapsed: 0:03:53. Loss: 0.56770\n",
            "  Batch   590  of    607.    Elapsed: 0:03:55. Loss: 0.56714\n",
            "  Batch   595  of    607.    Elapsed: 0:03:57. Loss: 0.56606\n",
            "  Batch   600  of    607.    Elapsed: 0:03:59. Loss: 0.56433\n",
            "  Batch   605  of    607.    Elapsed: 0:04:01. Loss: 0.56302\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:04:02\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:23\n",
            "  Accuracy: 0.75\n",
            "  Accuracy: 0.79\n",
            "  Macro F1-score: 0.75\n",
            "  Macro F1-score: 0.79\n",
            "  Weighted F1-score: 0.75\n",
            "  Weighted F1-score: 0.79\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.62      0.72       631\n",
            "         1.0       0.68      0.89      0.77       582\n",
            "\n",
            "    accuracy                           0.75      1213\n",
            "   macro avg       0.77      0.75      0.75      1213\n",
            "weighted avg       0.77      0.75      0.75      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.65      0.75       582\n",
            "         1.0       0.74      0.93      0.82       631\n",
            "\n",
            "    accuracy                           0.79      1213\n",
            "   macro avg       0.82      0.79      0.79      1213\n",
            "weighted avg       0.81      0.79      0.79      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[391 240]\n",
            " [ 64 518]]\n",
            "[[376 206]\n",
            " [ 44 587]]\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.42855\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.47260\n",
            "  Batch    15  of    607.    Elapsed: 0:00:06. Loss: 0.48053\n",
            "  Batch    20  of    607.    Elapsed: 0:00:08. Loss: 0.47963\n",
            "  Batch    25  of    607.    Elapsed: 0:00:10. Loss: 0.47988\n",
            "  Batch    30  of    607.    Elapsed: 0:00:12. Loss: 0.48768\n",
            "  Batch    35  of    607.    Elapsed: 0:00:14. Loss: 0.46969\n",
            "  Batch    40  of    607.    Elapsed: 0:00:16. Loss: 0.46393\n",
            "  Batch    45  of    607.    Elapsed: 0:00:18. Loss: 0.44413\n",
            "  Batch    50  of    607.    Elapsed: 0:00:20. Loss: 0.44334\n",
            "  Batch    55  of    607.    Elapsed: 0:00:23. Loss: 0.44944\n",
            "  Batch    60  of    607.    Elapsed: 0:00:25. Loss: 0.46238\n",
            "  Batch    65  of    607.    Elapsed: 0:00:27. Loss: 0.45455\n",
            "  Batch    70  of    607.    Elapsed: 0:00:29. Loss: 0.45299\n",
            "  Batch    75  of    607.    Elapsed: 0:00:31. Loss: 0.44354\n",
            "  Batch    80  of    607.    Elapsed: 0:00:33. Loss: 0.44577\n",
            "  Batch    85  of    607.    Elapsed: 0:00:35. Loss: 0.43886\n",
            "  Batch    90  of    607.    Elapsed: 0:00:37. Loss: 0.43989\n",
            "  Batch    95  of    607.    Elapsed: 0:00:39. Loss: 0.44202\n",
            "  Batch   100  of    607.    Elapsed: 0:00:41. Loss: 0.44087\n",
            "  Batch   105  of    607.    Elapsed: 0:00:43. Loss: 0.44141\n",
            "  Batch   110  of    607.    Elapsed: 0:00:45. Loss: 0.43760\n",
            "  Batch   115  of    607.    Elapsed: 0:00:47. Loss: 0.43866\n",
            "  Batch   120  of    607.    Elapsed: 0:00:49. Loss: 0.43627\n",
            "  Batch   125  of    607.    Elapsed: 0:00:51. Loss: 0.43513\n",
            "  Batch   130  of    607.    Elapsed: 0:00:53. Loss: 0.43643\n",
            "  Batch   135  of    607.    Elapsed: 0:00:56. Loss: 0.43755\n",
            "  Batch   140  of    607.    Elapsed: 0:00:58. Loss: 0.43427\n",
            "  Batch   145  of    607.    Elapsed: 0:01:00. Loss: 0.43119\n",
            "  Batch   150  of    607.    Elapsed: 0:01:02. Loss: 0.42887\n",
            "  Batch   155  of    607.    Elapsed: 0:01:04. Loss: 0.42670\n",
            "  Batch   160  of    607.    Elapsed: 0:01:06. Loss: 0.42975\n",
            "  Batch   165  of    607.    Elapsed: 0:01:08. Loss: 0.43427\n",
            "  Batch   170  of    607.    Elapsed: 0:01:10. Loss: 0.42868\n",
            "  Batch   175  of    607.    Elapsed: 0:01:12. Loss: 0.43044\n",
            "  Batch   180  of    607.    Elapsed: 0:01:14. Loss: 0.42431\n",
            "  Batch   185  of    607.    Elapsed: 0:01:16. Loss: 0.42792\n",
            "  Batch   190  of    607.    Elapsed: 0:01:18. Loss: 0.42837\n",
            "  Batch   195  of    607.    Elapsed: 0:01:21. Loss: 0.42612\n",
            "  Batch   200  of    607.    Elapsed: 0:01:23. Loss: 0.43150\n",
            "  Batch   205  of    607.    Elapsed: 0:01:25. Loss: 0.43225\n",
            "  Batch   210  of    607.    Elapsed: 0:01:27. Loss: 0.42986\n",
            "  Batch   215  of    607.    Elapsed: 0:01:29. Loss: 0.42776\n",
            "  Batch   220  of    607.    Elapsed: 0:01:31. Loss: 0.42741\n",
            "  Batch   225  of    607.    Elapsed: 0:01:33. Loss: 0.42947\n",
            "  Batch   230  of    607.    Elapsed: 0:01:35. Loss: 0.43349\n",
            "  Batch   235  of    607.    Elapsed: 0:01:37. Loss: 0.43777\n",
            "  Batch   240  of    607.    Elapsed: 0:01:39. Loss: 0.43758\n",
            "  Batch   245  of    607.    Elapsed: 0:01:41. Loss: 0.43478\n",
            "  Batch   250  of    607.    Elapsed: 0:01:43. Loss: 0.43365\n",
            "  Batch   255  of    607.    Elapsed: 0:01:45. Loss: 0.43463\n",
            "  Batch   260  of    607.    Elapsed: 0:01:48. Loss: 0.43527\n",
            "  Batch   265  of    607.    Elapsed: 0:01:50. Loss: 0.43615\n",
            "  Batch   270  of    607.    Elapsed: 0:01:52. Loss: 0.43544\n",
            "  Batch   275  of    607.    Elapsed: 0:01:54. Loss: 0.43647\n",
            "  Batch   280  of    607.    Elapsed: 0:01:56. Loss: 0.43695\n",
            "  Batch   285  of    607.    Elapsed: 0:01:58. Loss: 0.43704\n",
            "  Batch   290  of    607.    Elapsed: 0:02:00. Loss: 0.43482\n",
            "  Batch   295  of    607.    Elapsed: 0:02:02. Loss: 0.43405\n",
            "  Batch   300  of    607.    Elapsed: 0:02:04. Loss: 0.43270\n",
            "  Batch   305  of    607.    Elapsed: 0:02:06. Loss: 0.43316\n",
            "  Batch   310  of    607.    Elapsed: 0:02:08. Loss: 0.43135\n",
            "  Batch   315  of    607.    Elapsed: 0:02:10. Loss: 0.43007\n",
            "  Batch   320  of    607.    Elapsed: 0:02:12. Loss: 0.42826\n",
            "  Batch   325  of    607.    Elapsed: 0:02:14. Loss: 0.42945\n",
            "  Batch   330  of    607.    Elapsed: 0:02:17. Loss: 0.42946\n",
            "  Batch   335  of    607.    Elapsed: 0:02:19. Loss: 0.43088\n",
            "  Batch   340  of    607.    Elapsed: 0:02:21. Loss: 0.42993\n",
            "  Batch   345  of    607.    Elapsed: 0:02:23. Loss: 0.43081\n",
            "  Batch   350  of    607.    Elapsed: 0:02:25. Loss: 0.43112\n",
            "  Batch   355  of    607.    Elapsed: 0:02:27. Loss: 0.43024\n",
            "  Batch   360  of    607.    Elapsed: 0:02:29. Loss: 0.42976\n",
            "  Batch   365  of    607.    Elapsed: 0:02:31. Loss: 0.42935\n",
            "  Batch   370  of    607.    Elapsed: 0:02:33. Loss: 0.42870\n",
            "  Batch   375  of    607.    Elapsed: 0:02:35. Loss: 0.42643\n",
            "  Batch   380  of    607.    Elapsed: 0:02:37. Loss: 0.42785\n",
            "  Batch   385  of    607.    Elapsed: 0:02:39. Loss: 0.42981\n",
            "  Batch   390  of    607.    Elapsed: 0:02:42. Loss: 0.42720\n",
            "  Batch   395  of    607.    Elapsed: 0:02:44. Loss: 0.42754\n",
            "  Batch   400  of    607.    Elapsed: 0:02:46. Loss: 0.42824\n",
            "  Batch   405  of    607.    Elapsed: 0:02:48. Loss: 0.42835\n",
            "  Batch   410  of    607.    Elapsed: 0:02:50. Loss: 0.42759\n",
            "  Batch   415  of    607.    Elapsed: 0:02:52. Loss: 0.42715\n",
            "  Batch   420  of    607.    Elapsed: 0:02:54. Loss: 0.42642\n",
            "  Batch   425  of    607.    Elapsed: 0:02:56. Loss: 0.42640\n",
            "  Batch   430  of    607.    Elapsed: 0:02:58. Loss: 0.42764\n",
            "  Batch   435  of    607.    Elapsed: 0:03:00. Loss: 0.42945\n",
            "  Batch   440  of    607.    Elapsed: 0:03:02. Loss: 0.43056\n",
            "  Batch   445  of    607.    Elapsed: 0:03:05. Loss: 0.43165\n",
            "  Batch   450  of    607.    Elapsed: 0:03:07. Loss: 0.43272\n",
            "  Batch   455  of    607.    Elapsed: 0:03:09. Loss: 0.43147\n",
            "  Batch   460  of    607.    Elapsed: 0:03:11. Loss: 0.43011\n",
            "  Batch   465  of    607.    Elapsed: 0:03:13. Loss: 0.42895\n",
            "  Batch   470  of    607.    Elapsed: 0:03:15. Loss: 0.43079\n",
            "  Batch   475  of    607.    Elapsed: 0:03:17. Loss: 0.42967\n",
            "  Batch   480  of    607.    Elapsed: 0:03:19. Loss: 0.42955\n",
            "  Batch   485  of    607.    Elapsed: 0:03:21. Loss: 0.42905\n",
            "  Batch   490  of    607.    Elapsed: 0:03:23. Loss: 0.42997\n",
            "  Batch   495  of    607.    Elapsed: 0:03:26. Loss: 0.42892\n",
            "  Batch   500  of    607.    Elapsed: 0:03:28. Loss: 0.42985\n",
            "  Batch   505  of    607.    Elapsed: 0:03:30. Loss: 0.43023\n",
            "  Batch   510  of    607.    Elapsed: 0:03:32. Loss: 0.43015\n",
            "  Batch   515  of    607.    Elapsed: 0:03:34. Loss: 0.43017\n",
            "  Batch   520  of    607.    Elapsed: 0:03:36. Loss: 0.42966\n",
            "  Batch   525  of    607.    Elapsed: 0:03:38. Loss: 0.43043\n",
            "  Batch   530  of    607.    Elapsed: 0:03:40. Loss: 0.42979\n",
            "  Batch   535  of    607.    Elapsed: 0:03:42. Loss: 0.42970\n",
            "  Batch   540  of    607.    Elapsed: 0:03:45. Loss: 0.42907\n",
            "  Batch   545  of    607.    Elapsed: 0:03:47. Loss: 0.42895\n",
            "  Batch   550  of    607.    Elapsed: 0:03:49. Loss: 0.42945\n",
            "  Batch   555  of    607.    Elapsed: 0:03:51. Loss: 0.43052\n",
            "  Batch   560  of    607.    Elapsed: 0:03:53. Loss: 0.42913\n",
            "  Batch   565  of    607.    Elapsed: 0:03:55. Loss: 0.42922\n",
            "  Batch   570  of    607.    Elapsed: 0:03:57. Loss: 0.42942\n",
            "  Batch   575  of    607.    Elapsed: 0:03:59. Loss: 0.42940\n",
            "  Batch   580  of    607.    Elapsed: 0:04:01. Loss: 0.42836\n",
            "  Batch   585  of    607.    Elapsed: 0:04:04. Loss: 0.42713\n",
            "  Batch   590  of    607.    Elapsed: 0:04:06. Loss: 0.42719\n",
            "  Batch   595  of    607.    Elapsed: 0:04:08. Loss: 0.42707\n",
            "  Batch   600  of    607.    Elapsed: 0:04:10. Loss: 0.42730\n",
            "  Batch   605  of    607.    Elapsed: 0:04:12. Loss: 0.42766\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:04:13\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.43\n",
            "  Validation took: 0:00:23\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.81\n",
            "  Macro F1-score: 0.81\n",
            "  Macro F1-score: 0.80\n",
            "  Weighted F1-score: 0.81\n",
            "  Weighted F1-score: 0.80\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.82      0.82       631\n",
            "         1.0       0.80      0.80      0.80       582\n",
            "\n",
            "    accuracy                           0.81      1213\n",
            "   macro avg       0.81      0.81      0.81      1213\n",
            "weighted avg       0.81      0.81      0.81      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.65      0.77       582\n",
            "         1.0       0.75      0.95      0.84       631\n",
            "\n",
            "    accuracy                           0.81      1213\n",
            "   macro avg       0.84      0.80      0.80      1213\n",
            "weighted avg       0.83      0.81      0.80      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[518 113]\n",
            " [118 464]]\n",
            "[[380 202]\n",
            " [ 31 600]]\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of    607.    Elapsed: 0:00:02. Loss: 0.24779\n",
            "  Batch    10  of    607.    Elapsed: 0:00:04. Loss: 0.26405\n",
            "  Batch    15  of    607.    Elapsed: 0:00:06. Loss: 0.27720\n",
            "  Batch    20  of    607.    Elapsed: 0:00:08. Loss: 0.24743\n",
            "  Batch    25  of    607.    Elapsed: 0:00:10. Loss: 0.27060\n",
            "  Batch    30  of    607.    Elapsed: 0:00:13. Loss: 0.32295\n",
            "  Batch    35  of    607.    Elapsed: 0:00:15. Loss: 0.32483\n",
            "  Batch    40  of    607.    Elapsed: 0:00:17. Loss: 0.33005\n",
            "  Batch    45  of    607.    Elapsed: 0:00:19. Loss: 0.33713\n",
            "  Batch    50  of    607.    Elapsed: 0:00:21. Loss: 0.34923\n",
            "  Batch    55  of    607.    Elapsed: 0:00:23. Loss: 0.35622\n",
            "  Batch    60  of    607.    Elapsed: 0:00:25. Loss: 0.34343\n",
            "  Batch    65  of    607.    Elapsed: 0:00:27. Loss: 0.33984\n",
            "  Batch    70  of    607.    Elapsed: 0:00:29. Loss: 0.33665\n",
            "  Batch    75  of    607.    Elapsed: 0:00:31. Loss: 0.33058\n",
            "  Batch    80  of    607.    Elapsed: 0:00:34. Loss: 0.32688\n",
            "  Batch    85  of    607.    Elapsed: 0:00:36. Loss: 0.32384\n",
            "  Batch    90  of    607.    Elapsed: 0:00:38. Loss: 0.32274\n",
            "  Batch    95  of    607.    Elapsed: 0:00:40. Loss: 0.32419\n",
            "  Batch   100  of    607.    Elapsed: 0:00:42. Loss: 0.33074\n",
            "  Batch   105  of    607.    Elapsed: 0:00:44. Loss: 0.32996\n",
            "  Batch   110  of    607.    Elapsed: 0:00:46. Loss: 0.32409\n",
            "  Batch   115  of    607.    Elapsed: 0:00:48. Loss: 0.32552\n",
            "  Batch   120  of    607.    Elapsed: 0:00:50. Loss: 0.31923\n",
            "  Batch   125  of    607.    Elapsed: 0:00:52. Loss: 0.31688\n",
            "  Batch   130  of    607.    Elapsed: 0:00:54. Loss: 0.32387\n",
            "  Batch   135  of    607.    Elapsed: 0:00:57. Loss: 0.32205\n",
            "  Batch   140  of    607.    Elapsed: 0:00:59. Loss: 0.32340\n",
            "  Batch   145  of    607.    Elapsed: 0:01:01. Loss: 0.32463\n",
            "  Batch   150  of    607.    Elapsed: 0:01:03. Loss: 0.32645\n",
            "  Batch   155  of    607.    Elapsed: 0:01:05. Loss: 0.32818\n",
            "  Batch   160  of    607.    Elapsed: 0:01:07. Loss: 0.33141\n",
            "  Batch   165  of    607.    Elapsed: 0:01:09. Loss: 0.32995\n",
            "  Batch   170  of    607.    Elapsed: 0:01:11. Loss: 0.33454\n",
            "  Batch   175  of    607.    Elapsed: 0:01:13. Loss: 0.33638\n",
            "  Batch   180  of    607.    Elapsed: 0:01:15. Loss: 0.33355\n",
            "  Batch   185  of    607.    Elapsed: 0:01:18. Loss: 0.33738\n",
            "  Batch   190  of    607.    Elapsed: 0:01:20. Loss: 0.34017\n",
            "  Batch   195  of    607.    Elapsed: 0:01:22. Loss: 0.34068\n",
            "  Batch   200  of    607.    Elapsed: 0:01:24. Loss: 0.33612\n",
            "  Batch   205  of    607.    Elapsed: 0:01:26. Loss: 0.33571\n",
            "  Batch   210  of    607.    Elapsed: 0:01:28. Loss: 0.33752\n",
            "  Batch   215  of    607.    Elapsed: 0:01:30. Loss: 0.33209\n",
            "  Batch   220  of    607.    Elapsed: 0:01:32. Loss: 0.33036\n",
            "  Batch   225  of    607.    Elapsed: 0:01:34. Loss: 0.32758\n",
            "  Batch   230  of    607.    Elapsed: 0:01:36. Loss: 0.32962\n",
            "  Batch   235  of    607.    Elapsed: 0:01:38. Loss: 0.33505\n",
            "  Batch   240  of    607.    Elapsed: 0:01:41. Loss: 0.33586\n",
            "  Batch   245  of    607.    Elapsed: 0:01:43. Loss: 0.33641\n",
            "  Batch   250  of    607.    Elapsed: 0:01:45. Loss: 0.33654\n",
            "  Batch   255  of    607.    Elapsed: 0:01:47. Loss: 0.33842\n",
            "  Batch   260  of    607.    Elapsed: 0:01:49. Loss: 0.34071\n",
            "  Batch   265  of    607.    Elapsed: 0:01:51. Loss: 0.34129\n",
            "  Batch   270  of    607.    Elapsed: 0:01:53. Loss: 0.34608\n",
            "  Batch   275  of    607.    Elapsed: 0:01:55. Loss: 0.35080\n",
            "  Batch   280  of    607.    Elapsed: 0:01:57. Loss: 0.34993\n",
            "  Batch   285  of    607.    Elapsed: 0:01:59. Loss: 0.34852\n",
            "  Batch   290  of    607.    Elapsed: 0:02:01. Loss: 0.34902\n",
            "  Batch   295  of    607.    Elapsed: 0:02:04. Loss: 0.35159\n",
            "  Batch   300  of    607.    Elapsed: 0:02:06. Loss: 0.35317\n",
            "  Batch   305  of    607.    Elapsed: 0:02:08. Loss: 0.35111\n",
            "  Batch   310  of    607.    Elapsed: 0:02:10. Loss: 0.35100\n",
            "  Batch   315  of    607.    Elapsed: 0:02:12. Loss: 0.35205\n",
            "  Batch   320  of    607.    Elapsed: 0:02:14. Loss: 0.35281\n",
            "  Batch   325  of    607.    Elapsed: 0:02:16. Loss: 0.35604\n",
            "  Batch   330  of    607.    Elapsed: 0:02:18. Loss: 0.35681\n",
            "  Batch   335  of    607.    Elapsed: 0:02:20. Loss: 0.35751\n",
            "  Batch   340  of    607.    Elapsed: 0:02:22. Loss: 0.35663\n",
            "  Batch   345  of    607.    Elapsed: 0:02:24. Loss: 0.35320\n",
            "  Batch   350  of    607.    Elapsed: 0:02:27. Loss: 0.35252\n",
            "  Batch   355  of    607.    Elapsed: 0:02:29. Loss: 0.35126\n",
            "  Batch   360  of    607.    Elapsed: 0:02:31. Loss: 0.35167\n",
            "  Batch   365  of    607.    Elapsed: 0:02:33. Loss: 0.35233\n",
            "  Batch   370  of    607.    Elapsed: 0:02:35. Loss: 0.35318\n",
            "  Batch   375  of    607.    Elapsed: 0:02:37. Loss: 0.35162\n",
            "  Batch   380  of    607.    Elapsed: 0:02:39. Loss: 0.35055\n",
            "  Batch   385  of    607.    Elapsed: 0:02:41. Loss: 0.35165\n",
            "  Batch   390  of    607.    Elapsed: 0:02:43. Loss: 0.35129\n",
            "  Batch   395  of    607.    Elapsed: 0:02:45. Loss: 0.35054\n",
            "  Batch   400  of    607.    Elapsed: 0:02:47. Loss: 0.35054\n",
            "  Batch   405  of    607.    Elapsed: 0:02:50. Loss: 0.35144\n",
            "  Batch   410  of    607.    Elapsed: 0:02:52. Loss: 0.35232\n",
            "  Batch   415  of    607.    Elapsed: 0:02:54. Loss: 0.35316\n",
            "  Batch   420  of    607.    Elapsed: 0:02:56. Loss: 0.35093\n",
            "  Batch   425  of    607.    Elapsed: 0:02:58. Loss: 0.35039\n",
            "  Batch   430  of    607.    Elapsed: 0:03:00. Loss: 0.34955\n",
            "  Batch   435  of    607.    Elapsed: 0:03:02. Loss: 0.34761\n",
            "  Batch   440  of    607.    Elapsed: 0:03:04. Loss: 0.34709\n",
            "  Batch   445  of    607.    Elapsed: 0:03:06. Loss: 0.34681\n",
            "  Batch   450  of    607.    Elapsed: 0:03:08. Loss: 0.34783\n",
            "  Batch   455  of    607.    Elapsed: 0:03:10. Loss: 0.34617\n",
            "  Batch   460  of    607.    Elapsed: 0:03:13. Loss: 0.34717\n",
            "  Batch   465  of    607.    Elapsed: 0:03:15. Loss: 0.34840\n",
            "  Batch   470  of    607.    Elapsed: 0:03:17. Loss: 0.34719\n",
            "  Batch   475  of    607.    Elapsed: 0:03:19. Loss: 0.34666\n",
            "  Batch   480  of    607.    Elapsed: 0:03:21. Loss: 0.34491\n",
            "  Batch   485  of    607.    Elapsed: 0:03:23. Loss: 0.34390\n",
            "  Batch   490  of    607.    Elapsed: 0:03:25. Loss: 0.34333\n",
            "  Batch   495  of    607.    Elapsed: 0:03:27. Loss: 0.34278\n",
            "  Batch   500  of    607.    Elapsed: 0:03:29. Loss: 0.34204\n",
            "  Batch   505  of    607.    Elapsed: 0:03:32. Loss: 0.34249\n",
            "  Batch   510  of    607.    Elapsed: 0:03:34. Loss: 0.34301\n",
            "  Batch   515  of    607.    Elapsed: 0:03:36. Loss: 0.34415\n",
            "  Batch   520  of    607.    Elapsed: 0:03:38. Loss: 0.34377\n",
            "  Batch   525  of    607.    Elapsed: 0:03:40. Loss: 0.34548\n",
            "  Batch   530  of    607.    Elapsed: 0:03:42. Loss: 0.34541\n",
            "  Batch   535  of    607.    Elapsed: 0:03:44. Loss: 0.34571\n",
            "  Batch   540  of    607.    Elapsed: 0:03:46. Loss: 0.34580\n",
            "  Batch   545  of    607.    Elapsed: 0:03:49. Loss: 0.34628\n",
            "  Batch   550  of    607.    Elapsed: 0:03:51. Loss: 0.34653\n",
            "  Batch   555  of    607.    Elapsed: 0:03:53. Loss: 0.34653\n",
            "  Batch   560  of    607.    Elapsed: 0:03:55. Loss: 0.34589\n",
            "  Batch   565  of    607.    Elapsed: 0:03:57. Loss: 0.34664\n",
            "  Batch   570  of    607.    Elapsed: 0:03:59. Loss: 0.34587\n",
            "  Batch   575  of    607.    Elapsed: 0:04:01. Loss: 0.34569\n",
            "  Batch   580  of    607.    Elapsed: 0:04:03. Loss: 0.34543\n",
            "  Batch   585  of    607.    Elapsed: 0:04:05. Loss: 0.34396\n",
            "  Batch   590  of    607.    Elapsed: 0:04:08. Loss: 0.34590\n",
            "  Batch   595  of    607.    Elapsed: 0:04:10. Loss: 0.34616\n",
            "  Batch   600  of    607.    Elapsed: 0:04:12. Loss: 0.34636\n",
            "  Batch   605  of    607.    Elapsed: 0:04:14. Loss: 0.34652\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:04:15\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:23\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.82\n",
            "  Macro F1-score: 0.81\n",
            "  Macro F1-score: 0.82\n",
            "  Weighted F1-score: 0.81\n",
            "  Weighted F1-score: 0.82\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.85      0.82       631\n",
            "         1.0       0.82      0.76      0.79       582\n",
            "\n",
            "    accuracy                           0.81      1213\n",
            "   macro avg       0.81      0.81      0.81      1213\n",
            "weighted avg       0.81      0.81      0.81      1213\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.73      0.79       582\n",
            "         1.0       0.78      0.90      0.84       631\n",
            "\n",
            "    accuracy                           0.82      1213\n",
            "   macro avg       0.83      0.82      0.82      1213\n",
            "weighted avg       0.83      0.82      0.82      1213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[536  95]\n",
            " [137 445]]\n",
            "[[424 158]\n",
            " [ 61 570]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-9qbgDHEMW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}